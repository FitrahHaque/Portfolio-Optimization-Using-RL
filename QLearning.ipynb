{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 122,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading data from file...\n",
      "                 AAPL       AMZN      GOOGL       MSFT       TSLA\n",
      "Date                                                             \n",
      "2019-01-02  37.667183  76.956497  52.483086  95.119820  20.674667\n",
      "2019-01-03  33.915260  75.014000  51.029533  91.620537  20.024000\n",
      "2019-01-04  35.363064  78.769501  53.647011  95.881767  21.179333\n",
      "2019-01-07  35.284367  81.475502  53.540031  96.004059  22.330667\n",
      "2019-01-08  35.956989  82.829002  54.010281  96.700127  22.356667\n",
      "Training days: 1008, Testing days: 250\n"
     ]
    }
   ],
   "source": [
    "import math\n",
    "import os\n",
    "import random\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import yfinance as yf\n",
    "\n",
    "# File name to store the data locally\n",
    "data_filename = \"prices.csv\"\n",
    "\n",
    "# Define the 5 assets (tickers) to include in the portfolio\n",
    "tickers = [\"AAPL\", \"MSFT\", \"GOOGL\", \"AMZN\", \"TSLA\"]\n",
    "# Check if the data file exists\n",
    "if os.path.exists(data_filename):\n",
    "    print(\"Loading data from file...\")\n",
    "    prices_df = pd.read_csv(data_filename, index_col=0, parse_dates=True)\n",
    "else:\n",
    "    print(\"File not found. Downloading data from yfinance...\")\n",
    "    # Download data from yfinance\n",
    "    df = yf.download(tickers, start=\"2019-01-01\", end=\"2023-12-31\", interval=\"1d\")\n",
    "    # Extract the 'Close' prices from the MultiIndex DataFrame\n",
    "    prices_df = df.xs('Close', axis=1, level='Price')\n",
    "    # Drop rows with missing data and save to file for future use\n",
    "    prices_df.dropna(inplace=True)\n",
    "    prices_df.to_csv(data_filename)\n",
    "    print(\"Data downloaded and saved to\", data_filename)\n",
    "\n",
    "print(prices_df.head())\n",
    "\n",
    "# Split into training (first 4 years) and testing (last 1 year)\n",
    "train_df = prices_df[prices_df.index < \"2023-01-01\"]\n",
    "test_df  = prices_df[prices_df.index >= \"2023-01-01\"]\n",
    "\n",
    "# Convert price DataFrames to numpy arrays for faster calculations\n",
    "train_prices = train_df.values  # shape: [train_days, 5]\n",
    "test_prices  = test_df.values   # shape: [test_days, 5]\n",
    "num_assets = train_prices.shape[1]\n",
    "print(f\"Training days: {train_prices.shape[0]}, Testing days: {test_prices.shape[0]}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 123,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Initial state: EEEEE\n",
      "Number of valid actions from initial state: 21\n"
     ]
    }
   ],
   "source": [
    "# Mapping functions between allocation percentages and letter codes\n",
    "def encode_state(increments):\n",
    "    \"\"\"\n",
    "    Convert a tuple of 5 integer increments (each 0-20) to a 5-letter state string.\n",
    "    Each increment represents 5% allocation units.\n",
    "    \"\"\"\n",
    "    return \"\".join(chr(ord('A') + inc) for inc in increments)\n",
    "\n",
    "def decode_state(state_str):\n",
    "    \"\"\"Convert a 5-letter state string back to a tuple of 5 increment values (0-20 each).\"\"\"\n",
    "    return tuple(ord(ch) - ord('A') for ch in state_str)\n",
    "\n",
    "# Define all possible actions (including no-action)\n",
    "actions = [(i, j) for i in range(num_assets) for j in range(num_assets) if i != j]\n",
    "actions.append((None, None))  # no rebalance action\n",
    "\n",
    "def get_valid_actions(state):\n",
    "    \"\"\"\n",
    "    Given a state (5-letter string or tuple of increments), return a list of valid actions.\n",
    "    An action (i,j) is valid if asset i has at least 5% to give (increment >=1) \n",
    "    and asset j has at most 95% (increment <=19) to receive.\n",
    "    The no-action (None,None) is always valid.\n",
    "    \"\"\"\n",
    "    # If state is given as a string, decode to increments\n",
    "    increments = decode_state(state) if isinstance(state, str) else state\n",
    "    valid = []\n",
    "    for (i, j) in actions:\n",
    "        if i is None and j is None:\n",
    "            # No-action is always allowed\n",
    "            valid.append((None, None))\n",
    "            continue\n",
    "        if i is None or j is None:\n",
    "            # Ignore half-specified actions (only (None,None) is used for no-action)\n",
    "            continue\n",
    "        if increments[i] >= 1 and increments[j] <= 19:\n",
    "            valid.append((i, j))\n",
    "    return valid\n",
    "\n",
    "# Example: initial equal-weight state and its valid actions count\n",
    "initial_state = (4, 4, 4, 4, 4)  # 4*5% = 20% each, corresponds to 'EEEEE'\n",
    "print(\"Initial state:\", encode_state(initial_state))\n",
    "print(\"Number of valid actions from initial state:\", len(get_valid_actions(initial_state)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 124,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_reward(weight_frac, price_today, price_next):\n",
    "    \"\"\"\n",
    "    Compute the log return of the portfolio given weight fractions and asset prices.\n",
    "    weight_frac: list of 5 weight fractions (sums to 1) at day t after rebalancing.\n",
    "    price_today: prices of the 5 assets at day t.\n",
    "    price_next: prices of the 5 assets at day t+1.\n",
    "    Returns: log(portfolio_return) from t to t+1.\n",
    "    \"\"\"\n",
    "    # Portfolio value growth factor = sum_k w_k * (price_next_k / price_today_k)\n",
    "    growth_factor = 0.0\n",
    "    for k in range(num_assets):\n",
    "        growth_factor += weight_frac[k] * (price_next[k] / price_today[k])\n",
    "    # Reward is log of the growth factor\n",
    "    return math.log(growth_factor)\n",
    "\n",
    "def apply_action(state, action):\n",
    "    \"\"\"\n",
    "    Apply a rebalancing action to a state (5% transfer from one asset to another).\n",
    "    state: current state as a tuple of increments (summing to 20).\n",
    "    action: tuple (i,j) meaning transfer 5% from asset i to asset j, or (None,None) for no action.\n",
    "    Returns the new state (tuple of increments) after the action.\n",
    "    \"\"\"\n",
    "    increments = list(state)\n",
    "    if action is None or action == (None, None):\n",
    "        # No change in allocation\n",
    "        return tuple(increments)\n",
    "    i, j = action\n",
    "    # Reduce 5% (one increment) from asset i and add 5% to asset j\n",
    "    if increments[i] >= 1 and increments[j] <= 19:\n",
    "        increments[i] -= 1\n",
    "        increments[j] += 1\n",
    "    return tuple(increments)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 125,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize Q-table as an empty dictionary\n",
    "Q = {}\n",
    "\n",
    "def select_action(state, Q, epsilon):\n",
    "    \"\"\"\n",
    "    Select an action for the given state using epsilon-greedy policy.\n",
    "    \"\"\"\n",
    "    valid_actions = get_valid_actions(state)\n",
    "    # Exploration: choose a random valid action with probability epsilon\n",
    "    if random.random() < epsilon:\n",
    "        return random.choice(valid_actions)\n",
    "    # Exploitation: choose the action with highest Q-value (ties broken arbitrarily)\n",
    "    state_key = encode_state(state)\n",
    "    # Initialize Q entries for this state if not seen before\n",
    "    if state_key not in Q:\n",
    "        Q[state_key] = {a: 0.0 for a in valid_actions}\n",
    "    # Select the action with max Q-value\n",
    "    best_action = max(Q[state_key], key=Q[state_key].get)\n",
    "    return best_action\n",
    "\n",
    "def update_Q(Q, state, action, reward, next_state, alpha, gamma):\n",
    "    \"\"\"\n",
    "    Update the Q-table for state-action pair (state, action) using the Q-learning update rule.\n",
    "    \"\"\"\n",
    "    state_key = encode_state(state)\n",
    "    next_state_key = encode_state(next_state)\n",
    "    \n",
    "    # Initialize Q entries for current state if not already present\n",
    "    if state_key not in Q:\n",
    "        Q[state_key] = {a: 0.0 for a in get_valid_actions(state)}\n",
    "    \n",
    "    # Initialize Q entries for next_state if not already\n",
    "    if next_state_key not in Q:\n",
    "        Q[next_state_key] = {a: 0.0 for a in get_valid_actions(next_state)}\n",
    "    # Current Q-value for this state-action\n",
    "    q_current = Q[state_key].get(action, 0.0)\n",
    "    # Best possible Q-value for next state (future reward estimate)\n",
    "    q_next_max = max(Q[next_state_key].values()) if Q[next_state_key] else 0.0\n",
    "\n",
    "    # Q-learning update\n",
    "    Q[state_key][action] = q_current + alpha * (reward + gamma * q_next_max - q_current)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 126,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training completed.\n"
     ]
    }
   ],
   "source": [
    "# Hyperparameters for Q-learning\n",
    "alpha = 0.1        # learning rate\n",
    "gamma = 0.99       # discount factor (close to 1 for long-term rewards)\n",
    "epsilon = 1.0      # initial exploration rate\n",
    "epsilon_min = 0.1  # minimum exploration rate\n",
    "decay_rate = 0.99  # multiplicative decay for epsilon per episode\n",
    "\n",
    "episodes = 100  # number of training episodes (iterations over the 4-year training period)\n",
    "\n",
    "# Initial state is equal-weight (20% each) for each episode\n",
    "initial_state = (4, 4, 4, 4, 4)  # corresponds to 'EEEEE'\n",
    "\n",
    "train_days = train_prices.shape[0]\n",
    "\n",
    "for ep in range(episodes):\n",
    "    state = initial_state\n",
    "    # Iterate over each day in the training period (except the last day, since we look ahead one day for reward)\n",
    "    for t in range(train_days - 1):\n",
    "        # Choose an action (ε-greedy policy)\n",
    "        action = select_action(state, Q, epsilon)\n",
    "        # Apply the action to get new portfolio distribution\n",
    "        new_state = apply_action(state, action)\n",
    "        # Calculate reward from day t to t+1\n",
    "        weights_new = [inc/20.0 for inc in new_state]  # convert increments to fractions\n",
    "        reward = compute_reward(weights_new, train_prices[t], train_prices[t+1])\n",
    "        # Update Q-table based on the action and received reward\n",
    "        update_Q(Q, state, action, reward, new_state, alpha, gamma)\n",
    "        # Move to next state\n",
    "        state = new_state\n",
    "    # Decay exploration rate after each episode (to gradually reduce random exploration)\n",
    "    epsilon = max(epsilon * decay_rate, epsilon_min)\n",
    "\n",
    "print(\"Training completed.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 127,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Baseline final portfolio value: 1.7536  (Return: 75.36%)\n",
      "Agent final portfolio value:    1.7740  (Return: 77.40%)\n"
     ]
    }
   ],
   "source": [
    "# Evaluation on the test period (last year)\n",
    "test_days = test_prices.shape[0]\n",
    "\n",
    "# Initialize portfolio values and states for agent and baseline\n",
    "agent_value = 1.0\n",
    "baseline_value = 1.0\n",
    "state = initial_state  # agent's state (start equal-weight)\n",
    "# Baseline: determine initial shares for each asset with equal weights\n",
    "baseline_weights = [0.2] * num_assets  # 20% in each asset\n",
    "# If initial portfolio value = 1.0, money allocated to each asset = 0.2\n",
    "# Number of shares of each asset the baseline holds initially:\n",
    "baseline_shares = [baseline_weights[i] * baseline_value / test_prices[0][i] for i in range(num_assets)]\n",
    "\n",
    "# Simulate day by day\n",
    "for t in range(test_days - 1):\n",
    "    # Agent chooses the best action (no exploration in test)\n",
    "    state_key = encode_state(state)\n",
    "    if state_key in Q:\n",
    "        # Choose action with highest Q-value for current state\n",
    "        action = max(Q[state_key], key=Q[state_key].get)\n",
    "    else:\n",
    "        action = (None, None)  # if state not seen in training, do nothing\n",
    "    # Rebalance according to the chosen action\n",
    "    state = apply_action(state, action)\n",
    "    # Compute portfolio growth factor for agent from day t to t+1\n",
    "    weights = [inc/20.0 for inc in state]\n",
    "    growth_factor = 0.0\n",
    "    for k in range(num_assets):\n",
    "        growth_factor += weights[k] * (test_prices[t+1][k] / test_prices[t][k])\n",
    "    agent_value *= growth_factor\n",
    "    # Update baseline portfolio value from day t to t+1 (using held shares)\n",
    "    baseline_value = 0.0\n",
    "    for k in range(num_assets):\n",
    "        baseline_value += baseline_shares[k] * test_prices[t+1][k]\n",
    "\n",
    "# Calculate total returns over the test period\n",
    "agent_return_pct = (agent_value - 1.0) * 100\n",
    "baseline_return_pct = (baseline_value - 1.0) * 100\n",
    "\n",
    "print(f\"Baseline final portfolio value: {baseline_value:.4f}  (Return: {baseline_return_pct:.2f}%)\")\n",
    "print(f\"Agent final portfolio value:    {agent_value:.4f}  (Return: {agent_return_pct:.2f}%)\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
