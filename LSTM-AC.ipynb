{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 143,
   "metadata": {},
   "outputs": [],
   "source": [
    "import math\n",
    "import random\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import yfinance as yf\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.distributions import Dirichlet\n",
    "import torch.optim as optim\n",
    "import torch.nn.functional as F\n",
    "import matplotlib.pyplot as plt\n",
    "from collections import deque"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Hyperparameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 144,
   "metadata": {},
   "outputs": [],
   "source": [
    "T_WINDOW      = 10       # lookback window for LSTM\n",
    "ASSET_COUNT   = 5\n",
    "ACTION_DIM    = ASSET_COUNT + 1  # +cash\n",
    "STATE_DIM     = ACTION_DIM * T_WINDOW + ASSET_COUNT * 2  # allocations time series + indicators\n",
    "HIDDEN_SIZE   = 128\n",
    "LSTM_LAYERS   = 1\n",
    "\n",
    "LR_INIT       = 3e-4\n",
    "WEIGHT_DECAY  = 1e-4\n",
    "TOTAL_UPDATES = 300\n",
    "BATCH_SIZE    = 64\n",
    "PPO_EPOCHS    = 5\n",
    "CLIP_EPS      = 0.2\n",
    "GAMMA         = 0.99\n",
    "LAMBDA_GAE    = 0.95\n",
    "ENTROPY_COEF  = 0.01\n",
    "VALUE_COEF    = 0.5\n",
    "MAX_GRAD_NORM = 0.5\n",
    "\n",
    "N_STEP        = 3\n",
    "VAL_INTERVAL  = 20\n",
    "MAX_PATIENCE  = 5"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Data Loading & Normalization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 145,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded data from prices.csv\n",
      "Training days: 1323, Testing days: 250\n"
     ]
    }
   ],
   "source": [
    "# Define the 10 assets (tickers) for the portfolio\n",
    "# tickers = [\"AAPL\", \"MSFT\", \"GOOGL\", \"AMZN\", \"TSLA\", \"BA\", \"NFLX\", \"NVDA\", \"META\", \"SBUX\"]\n",
    "\n",
    "# tickers = [\"AAPL\", \"MSFT\", \"GOOGL\", \"SBUX\", \"TSLA\"]\n",
    "tickers = [\"GME\", \"AMC\", \"SPCE\", \"NVAX\", \"NOK\"]\n",
    "# tickers = [\"GME\", \"AMC\", \"BB\", \"NVAX\", \"NOK\"]\n",
    "# tickers = [\"GME\", \"AMC\", \"HMC\", \"NVAX\", \"NOK\"]\n",
    "# Date range for historical data\n",
    "start_date = \"2017-01-01\"\n",
    "end_date   = \"2023-12-31\"\n",
    "\n",
    "# Try to load price data from a local CSV, otherwise download using yfinance\n",
    "data_file = \"prices.csv\"\n",
    "try:\n",
    "    prices_df = pd.read_csv(data_file, index_col=0, parse_dates=True)\n",
    "    print(\"Loaded data from\", data_file)\n",
    "except FileNotFoundError:\n",
    "    print(\"Downloading price data for tickers:\", tickers)\n",
    "    df = yf.download(tickers, start=start_date, end=end_date, interval=\"1d\")\n",
    "    # Extract the 'Close' prices from the MultiIndex DataFrame\n",
    "    prices_df = df.xs('Close', axis=1, level='Price')\n",
    "    prices_df.dropna(inplace=True)\n",
    "    prices_df.to_csv(data_file)\n",
    "    print(\"Data downloaded and saved to\", data_file)\n",
    "\n",
    "# Split data into training (first 4 years) and testing (last year)\n",
    "full_train_df = prices_df[prices_df.index < \"2023-01-01\"]\n",
    "test_df  = prices_df[prices_df.index >= \"2023-01-01\"]\n",
    "full_train_prices = full_train_df.values  # shape: [train_days, 5]\n",
    "test_prices  = test_df.values   # shape: [test_days, 5]\n",
    "num_assets = full_train_prices.shape[1]\n",
    "print(f\"Training days: {full_train_prices.shape[0]}, Testing days: {test_prices.shape[0]}\")\n",
    "\n",
    "# Further split full training into training and validation (80/20)\n",
    "split_index = int(0.8 * full_train_prices.shape[0])\n",
    "train_prices = full_train_prices[:split_index]\n",
    "val_prices   = full_train_prices[split_index:]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### observation normalization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 146,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "class ObsNormalizer:\n",
    "    def __init__(self, size, eps=1e-4):\n",
    "        self.eps = eps\n",
    "        self.mean = np.zeros(size)\n",
    "        self.var  = np.ones(size)\n",
    "        self.count = eps\n",
    "\n",
    "    def update(self, x):\n",
    "        batch_mean = x.mean(0)\n",
    "        batch_var  = x.var(0)\n",
    "        batch_count = x.shape[0]\n",
    "        # Welford update\n",
    "        delta = batch_mean - self.mean\n",
    "        tot_count = self.count + batch_count\n",
    "        new_mean = self.mean + delta * batch_count / tot_count\n",
    "        m_a = self.var * self.count\n",
    "        m_b = batch_var * batch_count\n",
    "        M2 = m_a + m_b + delta**2 * self.count * batch_count / tot_count\n",
    "        new_var = M2 / tot_count\n",
    "        self.mean, self.var, self.count = new_mean, new_var, tot_count\n",
    "\n",
    "    def normalize(self, x):\n",
    "        return (x - self.mean) / np.sqrt(self.var + self.eps)\n",
    "\n",
    "obs_norm = ObsNormalizer(STATE_DIM)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Reward Shaper"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 147,
   "metadata": {},
   "outputs": [],
   "source": [
    "class RewardShaper:\n",
    "    def __init__(self, window=30):\n",
    "        self.window = window\n",
    "        self.rews = deque(maxlen=window)\n",
    "        self.peak = 0.0\n",
    "\n",
    "    def reset(self):\n",
    "        self.rews.clear()\n",
    "        self.peak = 0.0\n",
    "\n",
    "    def shape(self, raw, portval, turnover):\n",
    "        # rolling Sharpe\n",
    "        self.rews.append(raw)\n",
    "        r = np.array(self.rews)\n",
    "        sharpe = r.mean() / (r.std()+1e-6)\n",
    "        # drawdown\n",
    "        self.peak = max(self.peak, portval)\n",
    "        dd = (self.peak - portval) / (self.peak+1e-6)\n",
    "        # volatility penalty\n",
    "        vol_pen = r.std()\n",
    "        # turnover penalty\n",
    "        to_pen = turnover\n",
    "        return sharpe - 0.5*dd - 0.1*vol_pen - 0.1*to_pen\n",
    "\n",
    "shaper = RewardShaper(window=30)\n",
    "\n",
    "def compute_n_step_log_return(price_array, t, n_step, weights):\n",
    "    \"\"\"\n",
    "    Compute log return from day t to t+n_step using given portfolio weights.\n",
    "    - price_array: prices matrix\n",
    "    - t: current day index\n",
    "    - n_step: horizon (if beyond data end, use last available day)\n",
    "    - weights: allocation fractions for assets and cash (length N+1, sum=1)\n",
    "    Returns: log(growth_factor)\n",
    "    \"\"\"\n",
    "    last_idx = min(t + n_step, price_array.shape[0] - 1)\n",
    "    growth_factor = 0.0\n",
    "    # weights includes cash as last element\n",
    "    for i in range(len(weights)):\n",
    "        if i == num_assets:  # cash index\n",
    "            ratio = 1.0\n",
    "        else:\n",
    "            # price ratio from t to last_idx\n",
    "            ratio = price_array[last_idx, i] / (price_array[t, i] + 1e-6)\n",
    "        growth_factor += weights[i] * ratio\n",
    "    return math.log(growth_factor + 1e-6)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "State Builder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 148,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_market_indicators(prices, t, window=10):\n",
    "    feats = []\n",
    "    for i in range(ASSET_COUNT):\n",
    "        if t<window:\n",
    "            feats+= [1.0,0.0]\n",
    "        else:\n",
    "            win = prices[t-window+1:t+1,i]\n",
    "            ma = win.mean()\n",
    "            feats += [prices[t,i]/(ma+1e-6), np.std(np.diff(win)/(win[:-1]+1e-6))]\n",
    "    return np.array(feats, dtype=np.float32)\n",
    "\n",
    "def get_state(prices, t, alloc_history):\n",
    "    # alloc_history: deque of last T_WINDOW allocation vectors\n",
    "    alloc_seq = np.array(alloc_history).flatten()\n",
    "    ind = get_market_indicators(prices, t)\n",
    "    return np.concatenate([alloc_seq, ind])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Actor-Critic Network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 149,
   "metadata": {},
   "outputs": [],
   "source": [
    "class PPOActorCritic(nn.Module):\n",
    "    def __init__(self, state_dim, action_dim):\n",
    "        super().__init__()\n",
    "        # LSTM to encode past allocations\n",
    "        self.lstm = nn.LSTM(ACTION_DIM, HIDDEN_SIZE, \n",
    "                            num_layers=LSTM_LAYERS, batch_first=True)\n",
    "        self.ln1  = nn.LayerNorm(HIDDEN_SIZE)\n",
    "        # MLP after concat\n",
    "        self.fc1 = nn.Linear(HIDDEN_SIZE + 2*ASSET_COUNT, HIDDEN_SIZE)\n",
    "        self.ln2 = nn.LayerNorm(HIDDEN_SIZE)\n",
    "        # policy head: Dirichlet alphas\n",
    "        self.policy = nn.Linear(HIDDEN_SIZE, action_dim)\n",
    "        # value head\n",
    "        self.value  = nn.Linear(HIDDEN_SIZE, 1)\n",
    "\n",
    "        # orthogonal init\n",
    "        for m in self.modules():\n",
    "            if isinstance(m, nn.Linear):\n",
    "                nn.init.orthogonal_(m.weight, gain=np.sqrt(2))\n",
    "                nn.init.constant_(m.bias, 0.0)\n",
    "\n",
    "    def forward(self, alloc_seq, ind_feats, hx=None):\n",
    "        # alloc_seq: [B, T_WINDOW, ACTION_DIM]\n",
    "        out, hx = self.lstm(alloc_seq, hx)  # out: [B, T_WINDOW, H]\n",
    "        h = out[:,-1]                       # take last step\n",
    "        h = self.ln1(h)\n",
    "        x = torch.cat([h, ind_feats], dim=-1)\n",
    "        x = F.relu(self.ln2(self.fc1(x)))\n",
    "        alpha = F.softplus(self.policy(x)) + 1e-3\n",
    "        v     = self.value(x)\n",
    "        return alpha, v, hx"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### PPO Agent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 150,
   "metadata": {},
   "outputs": [],
   "source": [
    "class PPOAgent:\n",
    "    def __init__(self, state_dim, action_dim):\n",
    "        self.net = PPOActorCritic(state_dim, action_dim).float()\n",
    "        self.optim = optim.AdamW(self.net.parameters(), \n",
    "                                 lr=LR_INIT, weight_decay=WEIGHT_DECAY)\n",
    "        self.scheduler = optim.lr_scheduler.CosineAnnealingLR(\n",
    "            self.optim, TOTAL_UPDATES, eta_min=1e-6)\n",
    "        self.buffer = []  # collect trajectories\n",
    "\n",
    "    def select_action(self, state, hx=None):\n",
    "        seq, ind = state\n",
    "        seq_t = torch.from_numpy(seq[None]).float()\n",
    "        ind_t = torch.from_numpy(ind[None]).float()\n",
    "        alpha, v, hx = self.net(seq_t, ind_t, hx)\n",
    "        dist = Dirichlet(alpha.squeeze(0))\n",
    "        a = dist.sample()\n",
    "        return a.detach().cpu().numpy(), dist.log_prob(a).item(), v.item(), hx\n",
    "\n",
    "    def store(self, transition):\n",
    "        self.buffer.append(transition)\n",
    "\n",
    "    def update(self):\n",
    "        # extract batch\n",
    "        seqs, inds, acts, oldlps, rews, vals, dones = zip(*self.buffer)\n",
    "        T = len(rews)\n",
    "        # to tensors\n",
    "        seq_t = torch.tensor(seqs, dtype=torch.float32)\n",
    "        ind_t = torch.tensor(inds, dtype=torch.float32)\n",
    "        act_t = torch.tensor(acts, dtype=torch.float32)\n",
    "        lp_t  = torch.tensor(oldlps, dtype=torch.float32)\n",
    "        rew   = np.array(rews, dtype=np.float32)\n",
    "        val   = np.array(vals, dtype=np.float32)\n",
    "        done  = np.array(dones, dtype=np.bool_)\n",
    "\n",
    "        # compute GAE & returns\n",
    "        adv = np.zeros(T, np.float32)\n",
    "        ret = np.zeros(T, np.float32)\n",
    "        next_val = 0.0\n",
    "        gae = 0.0\n",
    "        for i in reversed(range(T)):\n",
    "            mask = 0.0 if done[i] else 1.0\n",
    "            delta = rew[i] + GAMMA*next_val*mask - val[i]\n",
    "            gae   = delta + GAMMA*LAMBDA_GAE*mask*gae\n",
    "            adv[i] = gae\n",
    "            next_val = val[i]\n",
    "        ret = adv + val\n",
    "        # normalize adv\n",
    "        adv = (adv - adv.mean())/(adv.std()+1e-6)\n",
    "\n",
    "        # convert\n",
    "        adv_t = torch.tensor(adv, dtype=torch.float32)\n",
    "        ret_t = torch.tensor(ret, dtype=torch.float32)\n",
    "\n",
    "        # PPO updates\n",
    "        for _ in range(PPO_EPOCHS):\n",
    "            # single large batch (can minibatch if desired)\n",
    "            alpha, v, _ = self.net(seq_t, ind_t)\n",
    "            dist = Dirichlet(alpha)\n",
    "            lp_new = dist.log_prob(act_t)\n",
    "            ratio = torch.exp(lp_new - lp_t)\n",
    "            s1 = ratio * adv_t\n",
    "            s2 = torch.clamp(ratio, 1-CLIP_EPS, 1+CLIP_EPS) * adv_t\n",
    "            loss_a = -torch.min(s1, s2).mean()\n",
    "            loss_c = F.mse_loss(v.squeeze(1), ret_t)\n",
    "            loss_e = -ENTROPY_COEF * dist.entropy().mean()\n",
    "            loss = loss_a + VALUE_COEF*loss_c + loss_e\n",
    "\n",
    "            self.optim.zero_grad()\n",
    "            loss.backward()\n",
    "            nn.utils.clip_grad_norm_(self.net.parameters(), MAX_GRAD_NORM)\n",
    "            self.optim.step()\n",
    "        self.scheduler.step()\n",
    "        self.buffer.clear()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Validation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 151,
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate_policy_ppo(net, price_array, initial_alloc, T_WINDOW, ASSET_COUNT, N_STEP):\n",
    "    \"\"\"\n",
    "    Deterministic evaluation of the trained PPO policy:\n",
    "      - net: the PPOActorCritic network\n",
    "      - price_array: np.array of shape [days, ASSET_COUNT]\n",
    "      - initial_alloc: length-(ASSET_COUNT+1) starting allocation (fractions sum to 1)\n",
    "      - T_WINDOW: how many past allocations the LSTM expects\n",
    "      - N_STEP: unused here (we use 1-step returns for eval)\n",
    "    Returns: list of daily portfolio values\n",
    "    \"\"\"\n",
    "    net.eval()\n",
    "    ACTION_DIM = ASSET_COUNT + 1\n",
    "    days = price_array.shape[0]\n",
    "\n",
    "    # initialize allocation history with the initial_alloc repeated\n",
    "    alloc_hist = deque([initial_alloc.copy()] * T_WINDOW, maxlen=T_WINDOW)\n",
    "    port_vals = [1.0]\n",
    "    alloc = initial_alloc.copy()\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for t in range(days - 1):\n",
    "            # build the two inputs\n",
    "            seq_np = np.stack(alloc_hist, axis=0)           # shape [T_WINDOW, ACTION_DIM]\n",
    "            ind_np = get_market_indicators(price_array, t)  # shape [2*ASSET_COUNT]\n",
    "\n",
    "            seq_t = torch.from_numpy(seq_np[None]).float()  # [1, T_WINDOW, ACTION_DIM]\n",
    "            ind_t = torch.from_numpy(ind_np[None]).float()  # [1, 2*ASSET_COUNT]\n",
    "\n",
    "            # forward pass\n",
    "            alpha_t, _, _ = net(seq_t, ind_t)\n",
    "            alpha = alpha_t.squeeze(0).cpu().numpy()        # [ACTION_DIM]\n",
    "\n",
    "            # deterministic allocation = mean of Dirichlet = alpha / sum(alpha)\n",
    "            if alpha.sum() <= 0:\n",
    "                alloc = np.ones_like(alpha) / ACTION_DIM\n",
    "            else:\n",
    "                alloc = alpha / alpha.sum()\n",
    "\n",
    "            # compute 1‑step portfolio growth\n",
    "            growth = 0.0\n",
    "            for i in range(ACTION_DIM):\n",
    "                if i == ASSET_COUNT:  # cash\n",
    "                    ratio = 1.0\n",
    "                else:\n",
    "                    ratio = price_array[t+1, i] / (price_array[t, i] + 1e-6)\n",
    "                growth += alloc[i] * ratio\n",
    "\n",
    "            port_vals.append(port_vals[-1] * growth)\n",
    "\n",
    "            # update history\n",
    "            alloc_hist.append(alloc)\n",
    "\n",
    "    return port_vals\n",
    "\n",
    "def evaluate_val_perf(net, price_array, initial_alloc):\n",
    "    # run the deterministic eval\n",
    "    vals = evaluate_policy_ppo(\n",
    "        net,\n",
    "        price_array,\n",
    "        initial_alloc,\n",
    "        T_WINDOW  = T_WINDOW,\n",
    "        ASSET_COUNT = ASSET_COUNT,\n",
    "        N_STEP = N_STEP\n",
    "    )\n",
    "    # return log of final portfolio value\n",
    "    return math.log(vals[-1] + 1e-6)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Training Loop with Early Stopping"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 152,
   "metadata": {},
   "outputs": [],
   "source": [
    "def run_training():\n",
    "    agent = PPOAgent(STATE_DIM, ACTION_DIM)\n",
    "    best_val = -float('inf'); patience = 0\n",
    "    initial_alloc = np.zeros(ACTION_DIM, np.float32); initial_alloc[-1] = 1.0\n",
    "\n",
    "    for update in range(1, TOTAL_UPDATES+1):\n",
    "        # collect one rollout\n",
    "        shaper.reset()\n",
    "        alloc_hist = deque([initial_alloc.copy() for _ in range(T_WINDOW)], maxlen=T_WINDOW)\n",
    "        portval = 1.0\n",
    "        hx = None\n",
    "\n",
    "        for t in range(train_prices.shape[0]-N_STEP-1):\n",
    "            state = (np.array(alloc_hist), \n",
    "                     get_market_indicators(train_prices, t))\n",
    "            # normalize obs\n",
    "            flat = state[0].flatten(); obs = np.concatenate([flat, state[1]])\n",
    "            obs_norm.update(obs[None])\n",
    "            norm_obs = (obs - obs_norm.mean)/np.sqrt(obs_norm.var+1e-4)\n",
    "            # re-split\n",
    "            seq = norm_obs[:ACTION_DIM*T_WINDOW].reshape(T_WINDOW, ACTION_DIM)\n",
    "            ind = norm_obs[ACTION_DIM*T_WINDOW:]\n",
    "            \n",
    "            # action & logprob\n",
    "            a, lp, v, hx = agent.select_action((seq, ind), hx)\n",
    "            # compute turnover\n",
    "            turnover = np.sum(np.abs(a - alloc_hist[-1]))\n",
    "            alloc_hist.append(a)\n",
    "            # reward\n",
    "            raw = compute_n_step_log_return(train_prices, t, N_STEP, a)\n",
    "            portval *= math.exp(raw)\n",
    "            r = shaper.shape(raw, portval, turnover)\n",
    "            done = False\n",
    "            # store transition\n",
    "            agent.store((seq, ind, a, lp, r, v, done))\n",
    "\n",
    "        agent.update()\n",
    "\n",
    "        # # validation & early stop\n",
    "        # if update % VAL_INTERVAL == 0:\n",
    "        #     # evaluate deterministically on val set...\n",
    "        #     val_perf = evaluate_val_perf(\n",
    "        #             agent.net,\n",
    "        #             val_prices,      # your validation prices np.array\n",
    "        #             initial_alloc       # same initial alloc used in training\n",
    "        #         )\n",
    "        #     print(f\"Update {update}: val={val_perf:.4f}  best={best_val:.4f}\")\n",
    "        #     if val_perf > best_val:\n",
    "        #         best_val = val_perf\n",
    "        #         patience = 0\n",
    "        #         torch.save(agent.net.state_dict(), 'best_ppo.pt')\n",
    "        #         print(\"  ↳ new best! checkpointing model\")\n",
    "        #     else:\n",
    "        #         patience += 1\n",
    "        #         print(f\"  ↳ no improvement ({patience}/{MAX_PATIENCE})\")\n",
    "        #         if patience >= MAX_PATIENCE:\n",
    "        #             print(\"Early stopping.\")\n",
    "        #             break\n",
    "        if update % 10 == 0:\n",
    "            print(f\"Update {update}:  portfolio value={portval:.4f}\")\n",
    "    # restore best\n",
    "    agent.net.load_state_dict(torch.load('best_ppo.pt'))\n",
    "    return agent.net"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 153,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Update 10:  portfolio value=91.1247\n",
      "Update 20:  portfolio value=1171.4208\n",
      "Update 30:  portfolio value=183.7978\n",
      "Update 40:  portfolio value=77.9434\n",
      "Update 50:  portfolio value=415.2174\n",
      "Update 60:  portfolio value=397.4070\n",
      "Update 70:  portfolio value=348.6648\n",
      "Update 80:  portfolio value=2401.6916\n",
      "Update 90:  portfolio value=994.4806\n",
      "Update 100:  portfolio value=635.4881\n",
      "Update 110:  portfolio value=169.8166\n",
      "Update 120:  portfolio value=199.0055\n",
      "Update 130:  portfolio value=2174.3429\n",
      "Update 140:  portfolio value=764.2259\n",
      "Update 150:  portfolio value=4449.1161\n",
      "Update 160:  portfolio value=2804.5081\n",
      "Update 170:  portfolio value=225.4980\n",
      "Update 180:  portfolio value=5038.8210\n",
      "Update 190:  portfolio value=1225.9553\n",
      "Update 200:  portfolio value=1151.2056\n",
      "Update 210:  portfolio value=1488.9899\n",
      "Update 220:  portfolio value=9772.9722\n",
      "Update 230:  portfolio value=396.8083\n",
      "Update 240:  portfolio value=3233.9323\n",
      "Update 250:  portfolio value=378.7924\n",
      "Update 260:  portfolio value=1630.1881\n",
      "Update 270:  portfolio value=197.0629\n",
      "Update 280:  portfolio value=1293.2618\n",
      "Update 290:  portfolio value=289.3329\n",
      "Update 300:  portfolio value=258.4105\n",
      "Test portfolio value: 0.7137304993850421\n",
      "Update 10:  portfolio value=179.0097\n",
      "Update 20:  portfolio value=95.4351\n",
      "Update 30:  portfolio value=381.5210\n",
      "Update 40:  portfolio value=186.8915\n",
      "Update 50:  portfolio value=1079.0991\n",
      "Update 60:  portfolio value=720.5684\n",
      "Update 70:  portfolio value=256.2348\n",
      "Update 80:  portfolio value=197.6404\n",
      "Update 90:  portfolio value=4560.5565\n",
      "Update 100:  portfolio value=674.8926\n",
      "Update 110:  portfolio value=1665.8136\n",
      "Update 120:  portfolio value=1337.8833\n",
      "Update 130:  portfolio value=2825.5060\n",
      "Update 140:  portfolio value=732.6837\n",
      "Update 150:  portfolio value=154.2127\n",
      "Update 160:  portfolio value=307.4429\n",
      "Update 170:  portfolio value=1915.8579\n",
      "Update 180:  portfolio value=792.2028\n",
      "Update 190:  portfolio value=689.6626\n",
      "Update 200:  portfolio value=479.4526\n",
      "Update 210:  portfolio value=88.9703\n",
      "Update 220:  portfolio value=97.1311\n",
      "Update 230:  portfolio value=209.3679\n",
      "Update 240:  portfolio value=1174.6093\n",
      "Update 250:  portfolio value=78.9283\n",
      "Update 260:  portfolio value=3307.6625\n",
      "Update 270:  portfolio value=759.1414\n",
      "Update 280:  portfolio value=707.2418\n",
      "Update 290:  portfolio value=471.7134\n",
      "Update 300:  portfolio value=989.1849\n",
      "Test portfolio value: 0.7137304993850421\n",
      "Update 10:  portfolio value=630.3156\n",
      "Update 20:  portfolio value=236.9022\n",
      "Update 30:  portfolio value=8679.8794\n",
      "Update 40:  portfolio value=1675.8979\n",
      "Update 50:  portfolio value=261.7600\n",
      "Update 60:  portfolio value=1723.6889\n",
      "Update 70:  portfolio value=575.0565\n",
      "Update 80:  portfolio value=11133.7253\n",
      "Update 90:  portfolio value=151.1454\n",
      "Update 100:  portfolio value=1678.9549\n",
      "Update 110:  portfolio value=510.5983\n",
      "Update 120:  portfolio value=199.1237\n",
      "Update 130:  portfolio value=1437.2340\n",
      "Update 140:  portfolio value=20583.7157\n",
      "Update 150:  portfolio value=801.9640\n",
      "Update 160:  portfolio value=893.2134\n",
      "Update 170:  portfolio value=1047.3788\n",
      "Update 180:  portfolio value=54.0636\n",
      "Update 190:  portfolio value=328.7260\n",
      "Update 200:  portfolio value=313.5640\n",
      "Update 210:  portfolio value=1352.9865\n",
      "Update 220:  portfolio value=910.3056\n",
      "Update 230:  portfolio value=437.3813\n",
      "Update 240:  portfolio value=360.2362\n",
      "Update 250:  portfolio value=7374.7029\n",
      "Update 260:  portfolio value=245.4188\n",
      "Update 270:  portfolio value=1176.2636\n",
      "Update 280:  portfolio value=1431.6157\n",
      "Update 290:  portfolio value=724.1599\n",
      "Update 300:  portfolio value=54.2994\n",
      "Test portfolio value: 0.7137304993850421\n",
      "Update 10:  portfolio value=1393.5037\n",
      "Update 20:  portfolio value=4136.4352\n",
      "Update 30:  portfolio value=175.9544\n",
      "Update 40:  portfolio value=3440.2278\n",
      "Update 50:  portfolio value=238.7315\n",
      "Update 60:  portfolio value=1038.7758\n",
      "Update 70:  portfolio value=336.0321\n",
      "Update 80:  portfolio value=949.5958\n",
      "Update 90:  portfolio value=1335.1798\n",
      "Update 100:  portfolio value=1899.8653\n",
      "Update 110:  portfolio value=2022.2194\n",
      "Update 120:  portfolio value=1417.2354\n",
      "Update 130:  portfolio value=221.5392\n",
      "Update 140:  portfolio value=1222.8652\n",
      "Update 150:  portfolio value=188.8747\n",
      "Update 160:  portfolio value=6455.3676\n",
      "Update 170:  portfolio value=923.5665\n",
      "Update 180:  portfolio value=565.6567\n",
      "Update 190:  portfolio value=1478.1676\n",
      "Update 200:  portfolio value=414.5419\n",
      "Update 210:  portfolio value=66.6201\n",
      "Update 220:  portfolio value=559.9131\n",
      "Update 230:  portfolio value=392.0290\n",
      "Update 240:  portfolio value=658.0697\n",
      "Update 250:  portfolio value=211.9273\n",
      "Update 260:  portfolio value=411.3760\n",
      "Update 270:  portfolio value=2223.2466\n",
      "Update 280:  portfolio value=169.5241\n",
      "Update 290:  portfolio value=509.9529\n",
      "Update 300:  portfolio value=495.2535\n",
      "Test portfolio value: 0.7137304993850421\n",
      "Update 10:  portfolio value=488.8649\n",
      "Update 20:  portfolio value=2801.0734\n",
      "Update 30:  portfolio value=37.5752\n",
      "Update 40:  portfolio value=76.6990\n",
      "Update 50:  portfolio value=404.5539\n",
      "Update 60:  portfolio value=806.2556\n",
      "Update 70:  portfolio value=189.8559\n",
      "Update 80:  portfolio value=1092.9373\n",
      "Update 90:  portfolio value=411.7782\n",
      "Update 100:  portfolio value=623.5631\n",
      "Update 110:  portfolio value=580.3640\n",
      "Update 120:  portfolio value=2002.1609\n",
      "Update 130:  portfolio value=1228.9250\n",
      "Update 140:  portfolio value=864.1383\n",
      "Update 150:  portfolio value=686.9575\n",
      "Update 160:  portfolio value=1077.7624\n",
      "Update 170:  portfolio value=217.6494\n",
      "Update 180:  portfolio value=560.3422\n",
      "Update 190:  portfolio value=1128.7598\n",
      "Update 200:  portfolio value=97.5601\n",
      "Update 210:  portfolio value=1411.7761\n",
      "Update 220:  portfolio value=474.3090\n",
      "Update 230:  portfolio value=131.0347\n",
      "Update 240:  portfolio value=955.7075\n",
      "Update 250:  portfolio value=508.0966\n",
      "Update 260:  portfolio value=677.7822\n",
      "Update 270:  portfolio value=251.0126\n",
      "Update 280:  portfolio value=383.6649\n",
      "Update 290:  portfolio value=99.9621\n",
      "Update 300:  portfolio value=60.7128\n",
      "Test portfolio value: 0.7137304993850421\n",
      "Update 10:  portfolio value=521.5468\n",
      "Update 20:  portfolio value=4675.6548\n",
      "Update 30:  portfolio value=3882.5144\n",
      "Update 40:  portfolio value=1501.3479\n",
      "Update 50:  portfolio value=5294.8268\n",
      "Update 60:  portfolio value=87.5476\n",
      "Update 70:  portfolio value=186.9667\n",
      "Update 80:  portfolio value=116.9866\n",
      "Update 90:  portfolio value=1048.2834\n",
      "Update 100:  portfolio value=1138.2864\n",
      "Update 110:  portfolio value=601.6563\n",
      "Update 120:  portfolio value=4931.4563\n",
      "Update 130:  portfolio value=309.5498\n",
      "Update 140:  portfolio value=165.7475\n",
      "Update 150:  portfolio value=3877.3300\n",
      "Update 160:  portfolio value=1138.0038\n",
      "Update 170:  portfolio value=2157.9254\n",
      "Update 180:  portfolio value=484.3047\n",
      "Update 190:  portfolio value=3517.4072\n",
      "Update 200:  portfolio value=635.1353\n",
      "Update 210:  portfolio value=4743.0088\n",
      "Update 220:  portfolio value=10504.3257\n",
      "Update 230:  portfolio value=637.8604\n",
      "Update 240:  portfolio value=9093.5349\n",
      "Update 250:  portfolio value=1135.4275\n",
      "Update 260:  portfolio value=945.6596\n",
      "Update 270:  portfolio value=848.7316\n",
      "Update 280:  portfolio value=948.9792\n",
      "Update 290:  portfolio value=1421.3269\n",
      "Update 300:  portfolio value=1713.8280\n",
      "Test portfolio value: 0.7137304993850421\n",
      "Update 10:  portfolio value=209.5268\n",
      "Update 20:  portfolio value=463.6230\n",
      "Update 30:  portfolio value=155.6774\n",
      "Update 40:  portfolio value=149.8558\n",
      "Update 50:  portfolio value=529.2575\n",
      "Update 60:  portfolio value=17.9002\n",
      "Update 70:  portfolio value=242.6831\n",
      "Update 80:  portfolio value=85.4906\n",
      "Update 90:  portfolio value=281.5960\n",
      "Update 100:  portfolio value=1841.0615\n",
      "Update 110:  portfolio value=170.1714\n",
      "Update 120:  portfolio value=130.3849\n",
      "Update 130:  portfolio value=130.3134\n",
      "Update 140:  portfolio value=297.0586\n",
      "Update 150:  portfolio value=43.0734\n",
      "Update 160:  portfolio value=2361.9621\n",
      "Update 170:  portfolio value=650.7999\n",
      "Update 180:  portfolio value=238.9674\n",
      "Update 190:  portfolio value=130.0519\n",
      "Update 200:  portfolio value=548.7797\n",
      "Update 210:  portfolio value=1142.5146\n",
      "Update 220:  portfolio value=9308.6514\n",
      "Update 230:  portfolio value=4843.7479\n",
      "Update 240:  portfolio value=1288.9771\n",
      "Update 250:  portfolio value=184.3890\n",
      "Update 260:  portfolio value=1348.1422\n",
      "Update 270:  portfolio value=525.9059\n",
      "Update 280:  portfolio value=18506.9406\n",
      "Update 290:  portfolio value=1241.4588\n",
      "Update 300:  portfolio value=158.9792\n",
      "Test portfolio value: 0.7137304993850421\n",
      "Update 10:  portfolio value=982.6533\n",
      "Update 20:  portfolio value=5724.7963\n",
      "Update 30:  portfolio value=344.1943\n",
      "Update 40:  portfolio value=2297.4427\n",
      "Update 50:  portfolio value=259.4807\n",
      "Update 60:  portfolio value=3791.3958\n",
      "Update 70:  portfolio value=838.9425\n",
      "Update 80:  portfolio value=2055.6634\n",
      "Update 90:  portfolio value=48.7352\n",
      "Update 100:  portfolio value=533.4983\n",
      "Update 110:  portfolio value=571.5465\n",
      "Update 120:  portfolio value=1502.9540\n",
      "Update 130:  portfolio value=135.5052\n",
      "Update 140:  portfolio value=410.0088\n",
      "Update 150:  portfolio value=547.3986\n",
      "Update 160:  portfolio value=166.2989\n",
      "Update 170:  portfolio value=4154.2082\n",
      "Update 180:  portfolio value=144.0894\n",
      "Update 190:  portfolio value=121.0080\n",
      "Update 200:  portfolio value=863.2986\n",
      "Update 210:  portfolio value=58.8775\n",
      "Update 220:  portfolio value=70.7243\n",
      "Update 230:  portfolio value=1451.0266\n",
      "Update 240:  portfolio value=511.5431\n",
      "Update 250:  portfolio value=992.0835\n",
      "Update 260:  portfolio value=182.1637\n",
      "Update 270:  portfolio value=671.2708\n",
      "Update 280:  portfolio value=430.7058\n",
      "Update 290:  portfolio value=777.3181\n",
      "Update 300:  portfolio value=857.7472\n",
      "Test portfolio value: 0.7137304993850421\n",
      "Update 10:  portfolio value=2374.0631\n",
      "Update 20:  portfolio value=970.4714\n",
      "Update 30:  portfolio value=1256.9840\n",
      "Update 40:  portfolio value=238.6505\n",
      "Update 50:  portfolio value=753.5454\n",
      "Update 60:  portfolio value=4133.9330\n",
      "Update 70:  portfolio value=1483.3306\n",
      "Update 80:  portfolio value=16583.8206\n",
      "Update 90:  portfolio value=779.7461\n",
      "Update 100:  portfolio value=1990.9223\n",
      "Update 110:  portfolio value=6109.7492\n",
      "Update 120:  portfolio value=1483.1221\n",
      "Update 130:  portfolio value=3500.5460\n",
      "Update 140:  portfolio value=483.4805\n",
      "Update 150:  portfolio value=1250.6957\n",
      "Update 160:  portfolio value=1864.4307\n",
      "Update 170:  portfolio value=1424.2448\n",
      "Update 180:  portfolio value=7948.9758\n",
      "Update 190:  portfolio value=316.5537\n",
      "Update 200:  portfolio value=303.7968\n",
      "Update 210:  portfolio value=92.7710\n",
      "Update 220:  portfolio value=6820.9437\n",
      "Update 230:  portfolio value=3162.5972\n",
      "Update 240:  portfolio value=378.4996\n",
      "Update 250:  portfolio value=328.5875\n",
      "Update 260:  portfolio value=1055.5752\n",
      "Update 270:  portfolio value=595.0179\n",
      "Update 280:  portfolio value=37.2576\n",
      "Update 290:  portfolio value=421.1482\n",
      "Update 300:  portfolio value=137.2665\n",
      "Test portfolio value: 0.7137304993850421\n",
      "Update 10:  portfolio value=11752.4372\n",
      "Update 20:  portfolio value=133.7972\n",
      "Update 30:  portfolio value=7254.1448\n",
      "Update 40:  portfolio value=802.2512\n",
      "Update 50:  portfolio value=4162.9682\n",
      "Update 60:  portfolio value=180.9582\n",
      "Update 70:  portfolio value=1986.2682\n",
      "Update 80:  portfolio value=2903.6740\n",
      "Update 90:  portfolio value=1193.7707\n",
      "Update 100:  portfolio value=312.4151\n",
      "Update 110:  portfolio value=685.4968\n",
      "Update 120:  portfolio value=4296.3111\n",
      "Update 130:  portfolio value=411.6652\n",
      "Update 140:  portfolio value=660.6878\n",
      "Update 150:  portfolio value=293.0263\n",
      "Update 160:  portfolio value=536.8397\n",
      "Update 170:  portfolio value=133.8457\n",
      "Update 180:  portfolio value=193.3702\n",
      "Update 190:  portfolio value=283.0141\n",
      "Update 200:  portfolio value=2167.7691\n",
      "Update 210:  portfolio value=703.6074\n",
      "Update 220:  portfolio value=343.5274\n",
      "Update 230:  portfolio value=174.6417\n",
      "Update 240:  portfolio value=401.7178\n",
      "Update 250:  portfolio value=230.6836\n",
      "Update 260:  portfolio value=46.6224\n",
      "Update 270:  portfolio value=802.0977\n",
      "Update 280:  portfolio value=584.1725\n",
      "Update 290:  portfolio value=230.0269\n",
      "Update 300:  portfolio value=332.9021\n",
      "Test portfolio value: 0.7137304993850421\n",
      "Test final return: -28.626950061495794 %\n"
     ]
    }
   ],
   "source": [
    "initial_alloc = np.zeros(ACTION_DIM, np.float32); initial_alloc[-1] = 1.0\n",
    "models = []\n",
    "for i in range(10):\n",
    "    net = run_training()\n",
    "    best_value = -float('inf')\n",
    "    port_values = evaluate_policy_ppo(\n",
    "        net,\n",
    "        test_prices,\n",
    "        initial_alloc,\n",
    "        T_WINDOW=10,\n",
    "        ASSET_COUNT=5,\n",
    "        N_STEP=3\n",
    "    )\n",
    "    if best_value < port_values[-1]:\n",
    "        best_value = port_values[-1]\n",
    "        # best_model = model\n",
    "    print(\"Test portfolio value:\", port_values[-1])    \n",
    "    models.append(net)\n",
    "\n",
    "print(\"Test final return:\", (best_value-1)*100, \"%\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
