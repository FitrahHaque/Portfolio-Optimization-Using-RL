{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 34,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "_Y77BeglBh0B",
        "outputId": "e96b675b-fabe-4d82-947e-bccea7072beb"
      },
      "outputs": [],
      "source": [
        "# from google.colab import drive\n",
        "# drive.mount('/content/drive')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 35,
      "metadata": {
        "id": "ygKC_nPqrODL"
      },
      "outputs": [],
      "source": [
        "import math\n",
        "import random\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "import torch.optim as optim\n",
        "import yfinance as yf\n",
        "from collections import deque\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "WaT-xj4XrdOK"
      },
      "source": [
        "### Data Preparation"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 36,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "9ILxQXy4rY-R",
        "outputId": "77615a20-66fc-412f-dfc7-bed4f770d3c6"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Loaded data from prices.csv\n",
            "Training days: 1323, Testing days: 250\n"
          ]
        }
      ],
      "source": [
        "# Define the 10 assets (tickers) for the portfolio\n",
        "# tickers = [\"AAPL\", \"MSFT\", \"GOOGL\", \"AMZN\", \"TSLA\", \"BA\", \"NFLX\", \"NVDA\", \"META\", \"SBUX\"]\n",
        "# tickers = [\"AAPL\", \"MSFT\", \"GOOGL\", \"SBUX\", \"TSLA\"]\n",
        "tickers = [\"GME\", \"AMC\", \"SPCE\", \"NVAX\", \"NOK\"]\n",
        "\n",
        "# Date range for historical data\n",
        "start_date = \"2015-01-01\"\n",
        "end_date   = \"2023-12-31\"\n",
        "\n",
        "# Try to load price data from a local CSV, otherwise download using yfinance\n",
        "data_file = \"prices.csv\"\n",
        "try:\n",
        "    prices_df = pd.read_csv(data_file, index_col=0, parse_dates=True)\n",
        "    print(\"Loaded data from\", data_file)\n",
        "except FileNotFoundError:\n",
        "    print(\"Downloading price data for tickers:\", tickers)\n",
        "    df = yf.download(tickers, start=start_date, end=end_date, interval=\"1d\")\n",
        "    # Extract the 'Close' prices from the MultiIndex DataFrame\n",
        "    prices_df = df.xs('Close', axis=1, level='Price')\n",
        "    prices_df.dropna(inplace=True)\n",
        "    prices_df.to_csv(data_file)\n",
        "    print(\"Data downloaded and saved to\", data_file)\n",
        "\n",
        "# Split data into training (first 4 years) and testing (last year)\n",
        "train_df = prices_df[prices_df.index < \"2023-01-01\"]\n",
        "test_df  = prices_df[prices_df.index >= \"2023-01-01\"]\n",
        "train_prices = train_df.values  # shape: [train_days, 5]\n",
        "test_prices  = test_df.values   # shape: [test_days, 5]\n",
        "num_assets = train_prices.shape[1]\n",
        "print(f\"Training days: {train_prices.shape[0]}, Testing days: {test_prices.shape[0]}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "uEJnt93hrlGz"
      },
      "source": [
        "### State Encoding/Decoding and Actions"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 37,
      "metadata": {
        "id": "0191obe4rd9I"
      },
      "outputs": [],
      "source": [
        "def encode_state(increments):\n",
        "    \"\"\"\n",
        "    Encode a state (tuple of 5 integer increments) as a 5-letter string.\n",
        "    Each integer (0-20) represents allocation in 5% units.\n",
        "    'A' corresponds to 0 (0%), 'B' to 1 (5%), ..., 'U' to 20 (100%).\n",
        "    \"\"\"\n",
        "    return \"\".join(chr(ord('A') + inc) for inc in increments)\n",
        "\n",
        "def decode_state(state_str):\n",
        "    \"\"\"\n",
        "    Decode a 5-letter state string back into a tuple of 5 increments (0-20 each).\n",
        "    \"\"\"\n",
        "    return tuple(ord(ch) - ord('A') for ch in state_str)\n",
        "\n",
        "# Define all possible actions (including no-action) as tuples\n",
        "all_actions = [(i, j) for i in range(num_assets) for j in range(num_assets) if i != j]\n",
        "all_actions.append((None, None))  # (None,None) denotes no rebalance action\n",
        "action_count = len(all_actions)  # should be 21 (for 5 assets)\n",
        "# Create a mapping from action tuple to index in all_actions list\n",
        "action_to_index = {act: idx for idx, act in enumerate(all_actions)}\n",
        "\n",
        "def get_valid_actions(state):\n",
        "    \"\"\"\n",
        "    Given a state (tuple of 5 increments or state string), return a list of valid action tuples.\n",
        "    A transfer action (i,j) is valid if the current state has at least 5% in asset i (increment >= 1)\n",
        "    and at most 95% in asset j (increment <= 19). The no-action (None,None) is always valid.\n",
        "    \"\"\"\n",
        "    # If state is given as a string, decode it to a tuple of increments\n",
        "    increments = decode_state(state) if isinstance(state, str) else tuple(state)\n",
        "    valid_actions = []\n",
        "    for act in all_actions:\n",
        "        if act == (None, None):\n",
        "            valid_actions.append(act)\n",
        "        else:\n",
        "            i, j = act\n",
        "            if increments[i] >= 1 and increments[j] <= 19:\n",
        "                valid_actions.append(act)\n",
        "    return valid_actions\n",
        "\n",
        "def apply_action(state, action):\n",
        "    \"\"\"\n",
        "    Apply a rebalancing action to a state.\n",
        "    - state: current state as a tuple of 5 increments (sums to 20).\n",
        "    - action: a tuple (i, j) meaning transfer 5% (one increment) from asset i to asset j.\n",
        "              (None, None) means no rebalancing.\n",
        "    Returns the new state (tuple of 5 increments) after applying the action.\n",
        "    \"\"\"\n",
        "    increments = list(state)\n",
        "    if action is None or action == (None, None):\n",
        "        # No rebalance\n",
        "        return tuple(increments)\n",
        "    i, j = action\n",
        "    # Transfer one increment from i to j (if possible)\n",
        "    if increments[i] >= 1 and increments[j] <= 19:\n",
        "        increments[i] -= 1\n",
        "        increments[j] += 1\n",
        "    return tuple(increments)\n",
        "\n",
        "def compute_reward(weights_frac, price_today, price_next):\n",
        "    \"\"\"\n",
        "    Compute the log return of the portfolio for one time step.\n",
        "    - weights_frac: list of 5 asset weight fractions after rebalancing on day t (sum=1).\n",
        "    - price_today: prices of the 5 assets at day t.\n",
        "    - price_next: prices of the 5 assets at day t+1.\n",
        "    Returns: log(portfolio return) from day t to t+1.\n",
        "    \"\"\"\n",
        "    # Portfolio value growth factor = sum_k w_k * (price_next_k / price_today_k)\n",
        "    growth_factor = 0.0\n",
        "    for k in range(num_assets):\n",
        "        growth_factor += weights_frac[k] * (price_next[k] / price_today[k])\n",
        "    # Reward is the log of the growth factor\n",
        "    return math.log(growth_factor)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qMipXOQPiNWW"
      },
      "source": [
        "### Reward Shaping using a Rolling Sharpe Ratio"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 38,
      "metadata": {
        "id": "7WYWx-jViOO0"
      },
      "outputs": [],
      "source": [
        "class SharpeRewardShaper:\n",
        "    def __init__(self, window=30, epsilon=1e-6):\n",
        "        self.window = window\n",
        "        self.rewards_history = []\n",
        "        self.epsilon = epsilon\n",
        "\n",
        "    def shape(self, raw_reward):\n",
        "        self.rewards_history.append(raw_reward)\n",
        "        if len(self.rewards_history) > self.window:\n",
        "            self.rewards_history.pop(0)\n",
        "        avg_reward = np.mean(self.rewards_history)\n",
        "        std_reward = np.std(self.rewards_history) + self.epsilon\n",
        "        sharpe = avg_reward / std_reward\n",
        "        return sharpe\n",
        "\n",
        "reward_shaper = SharpeRewardShaper(window=30)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Uqa6xKTSrqzp"
      },
      "source": [
        "### Replay Buffer for Experience Replay"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 39,
      "metadata": {
        "id": "88rVfnT-rnrp"
      },
      "outputs": [],
      "source": [
        "class ReplayBuffer:\n",
        "    def __init__(self, capacity):\n",
        "        \"\"\"\n",
        "        Replay buffer to store past transitions for experience replay.\n",
        "        Stores tuples of (state, action_index, reward, next_state, done).\n",
        "        \"\"\"\n",
        "        self.capacity = capacity\n",
        "        self.buffer = deque(maxlen=capacity)\n",
        "\n",
        "    def push(self, state, action_idx, reward, next_state, done):\n",
        "        \"\"\"Save a transition to the buffer.\"\"\"\n",
        "        self.buffer.append((state, action_idx, reward, next_state, done))\n",
        "\n",
        "    def sample(self, batch_size):\n",
        "        \"\"\"\n",
        "        Sample a random batch of transitions from the buffer.\n",
        "        Returns: tuples (states, action_idxs, rewards, next_states, dones) for the batch.\n",
        "        \"\"\"\n",
        "        batch = random.sample(self.buffer, batch_size)\n",
        "        # Extract each component into separate lists\n",
        "        states, action_idxs, rewards, next_states, dones = zip(*batch)\n",
        "        return list(states), list(action_idxs), list(rewards), list(next_states), list(dones)\n",
        "\n",
        "    def __len__(self):\n",
        "        \"\"\"Current size of the buffer.\"\"\"\n",
        "        return len(self.buffer)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-z0AwXHTrwmz"
      },
      "source": [
        "### Neural Network for Q-value Function"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 40,
      "metadata": {
        "id": "j6RJgJrfrtDJ"
      },
      "outputs": [],
      "source": [
        "class DQNNetwork(nn.Module):\n",
        "    def __init__(self, state_dim, action_dim):\n",
        "        \"\"\"\n",
        "        Neural network that approximates Q(s,a) for all actions a given state s.\n",
        "        state_dim: dimensionality of state input (e.g. 5)\n",
        "        action_dim: number of possible actions (e.g. 21)\n",
        "        \"\"\"\n",
        "        super(DQNNetwork, self).__init__()\n",
        "        self.fc1 = nn.Linear(state_dim, 128)\n",
        "        self.bn1 = nn.BatchNorm1d(128)\n",
        "        self.dropout1 = nn.Dropout(0.3)\n",
        "        self.fc2 = nn.Linear(128, 256)\n",
        "        self.bn2 = nn.BatchNorm1d(256)\n",
        "        self.dropout2 = nn.Dropout(0.3)\n",
        "        self.fc3 = nn.Linear(256, 128)\n",
        "        self.fc4 = nn.Linear(128, action_dim)\n",
        "\n",
        "        # hidden1 = 128\n",
        "        # hidden2 = 128\n",
        "        # self.fc1 = nn.Linear(state_dim, hidden1)\n",
        "        # self.fc2 = nn.Linear(hidden1, hidden2)\n",
        "        # self.fc3 = nn.Linear(hidden2, action_dim)\n",
        "\n",
        "\n",
        "    def forward(self, x):\n",
        "        # x is a tensor of shape [batch_size, state_dim]\n",
        "\n",
        "        x = F.relu(self.bn1(self.fc1(x)))\n",
        "        x = self.dropout1(x)\n",
        "        x = F.relu(self.bn2(self.fc2(x)))\n",
        "        x = self.dropout2(x)\n",
        "        x = F.relu(self.fc3(x))\n",
        "        q_vals = self.fc4(x)\n",
        "        return q_vals\n",
        "\n",
        "        # x = F.relu(self.fc1(x))\n",
        "        # x = F.relu(self.fc2(x))\n",
        "        # q_values = self.fc3(x)  # outputs Q-values for each action\n",
        "        # return q_values"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8DkUKJ6zr0X7"
      },
      "source": [
        "### DQN Agent Training Setup"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 41,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "4FEtbjn1r0v7",
        "outputId": "aeb4afb6-650e-4ba8-903b-54dec9397ab2"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Episode 10/150 completed, epsilon=0.951\n",
            "Episode 20/150 completed, epsilon=0.905\n",
            "Episode 30/150 completed, epsilon=0.860\n",
            "Episode 40/150 completed, epsilon=0.818\n",
            "Episode 50/150 completed, epsilon=0.778\n",
            "Episode 60/150 completed, epsilon=0.740\n",
            "Episode 70/150 completed, epsilon=0.704\n",
            "Episode 80/150 completed, epsilon=0.670\n",
            "Episode 90/150 completed, epsilon=0.637\n",
            "Episode 100/150 completed, epsilon=0.606\n",
            "Episode 110/150 completed, epsilon=0.576\n",
            "Episode 120/150 completed, epsilon=0.548\n",
            "Episode 130/150 completed, epsilon=0.521\n",
            "Episode 140/150 completed, epsilon=0.496\n",
            "Episode 150/150 completed, epsilon=0.471\n",
            "Training completed.\n"
          ]
        }
      ],
      "source": [
        "# Hyperparameters\n",
        "gamma = 0.99            # discount factor for future rewards\n",
        "learning_rate = 5e-4  # learning rate for optimizer\n",
        "epsilon_start = 1.0     # initial exploration rate\n",
        "epsilon_min   = 0.2     # minimum exploration rate\n",
        "epsilon_decay = 0.995    # multiplicative decay factor per episode\n",
        "episodes = 150          # number of training episodes\n",
        "batch_size = 64         # mini-batch size for replay updates\n",
        "target_update_freq = 5 # how often (in episodes) to update the target network\n",
        "replay_capacity = 10000 # capacity of the replay buffer\n",
        "use_double_dqn = True  # use Double DQN for      \n",
        "state_dim = 5\n",
        "action_dim = action_count  # 21\n",
        "ensemble_size = 2\n",
        "\n",
        "# Initialize replay memory, policy network, target network, optimizer\n",
        "replay_buffer = ReplayBuffer(replay_capacity)\n",
        "# prioritized_replay_buffer = PrioritizedReplayBuffer(replay_capacity)\n",
        "\n",
        "# policy_net = DQNNetwork(state_dim, action_dim)\n",
        "# target_net = DQNNetwork(state_dim, action_dim)\n",
        "# Copy initial weights to target network\n",
        "# target_net.load_state_dict(policy_net.state_dict())\n",
        "# target_net.eval()  # target network in evaluation mode (not strictly necessary)\n",
        "# optimizer = optim.Adam(policy_net.parameters(), lr=learning_rate)\n",
        "\n",
        "\n",
        "# Create ensemble of networks and their corresponding target networks\n",
        "ensemble_nets = [DQNNetwork(state_dim, action_dim) for _ in range(ensemble_size)]\n",
        "ensemble_targets = [DQNNetwork(state_dim, action_dim) for _ in range(ensemble_size)]\n",
        "for net, target in zip(ensemble_nets, ensemble_targets):\n",
        "    target.load_state_dict(net.state_dict())\n",
        "    target.eval()\n",
        "# Combine parameters of all ensemble networks in one optimizer\n",
        "ensemble_optimizer = optim.Adam([p for net in ensemble_nets for p in net.parameters()], lr=learning_rate)\n",
        "\n",
        "# For action selection and training, we take the average Q-values across ensemble members.\n",
        "def ensemble_q_values(state_input):\n",
        "    # Temporarily store the training state of each network.\n",
        "    original_modes = [net.training for net in ensemble_nets]\n",
        "\n",
        "    # Switch networks to eval mode for inference (to avoid BN issues with batch size 1)\n",
        "    for net in ensemble_nets:\n",
        "        net.eval()\n",
        "\n",
        "    # Compute Q-values from each network and average them\n",
        "    q_vals_list = [net(state_input) for net in ensemble_nets]  # shape: [ensemble_size, batch, action_dim]\n",
        "    avg_q_vals = torch.stack(q_vals_list, dim=0).mean(dim=0)\n",
        "\n",
        "    # Restore the original training mode of each network\n",
        "    for net, mode in zip(ensemble_nets, original_modes):\n",
        "        if mode:\n",
        "            net.train()\n",
        "        else:\n",
        "            net.eval()\n",
        "\n",
        "    return avg_q_vals\n",
        "\n",
        "# Helper function: select action using epsilon-greedy policy\n",
        "def select_action(state, epsilon):\n",
        "    \"\"\"\n",
        "    Choose an action index for the given state using an epsilon-greedy strategy.\n",
        "    - state: current state as a tuple of increments.\n",
        "    - epsilon: current exploration rate.\n",
        "    Returns: (action_idx, action_tuple)\n",
        "    \"\"\"\n",
        "    valid_actions = get_valid_actions(state)\n",
        "    if random.random() < epsilon:\n",
        "        # Exploration: random valid action\n",
        "        action = random.choice(valid_actions)\n",
        "    else:\n",
        "        # Exploitation: choose best action according to Q-network\n",
        "        # Prepare state as a tensor (1x5) with normalized allocations\n",
        "        state_input = torch.FloatTensor([inc/20.0 for inc in state]).unsqueeze(0)\n",
        "        with torch.no_grad():\n",
        "          q_values = ensemble_q_values(state_input)  # shape [1, action_dim]\n",
        "          q_values = q_values.numpy().squeeze()  # shape [action_dim]\n",
        "        # Mask out invalid actions by setting their Q-value very low\n",
        "        # (So they won't be chosen as max)\n",
        "        invalid_actions = set(all_actions) - set(valid_actions)\n",
        "        for act in invalid_actions:\n",
        "            q_values[action_to_index[act]] = -1e9  # large negative to disable\n",
        "        best_idx = int(np.argmax(q_values))\n",
        "        action = all_actions[best_idx]\n",
        "\n",
        "    # Return both the index and the tuple representation\n",
        "    return action_to_index[action], action\n",
        "\n",
        "# Training loop\n",
        "initial_state = (4, 4, 4, 4, 4)  # start each episode with equal weights (\"EEEEE\")\n",
        "epsilon = epsilon_start\n",
        "train_days = train_prices.shape[0]\n",
        "for ep in range(1, episodes+1):\n",
        "    state = initial_state\n",
        "    # Iterate over each day in training data (except last, as we look ahead one day for reward)\n",
        "    for t in range(train_days - 1):\n",
        "        # Choose action (epsilon-greedy)\n",
        "        action_idx, action = select_action(state, epsilon)\n",
        "        # Apply action to get new state\n",
        "        new_state = apply_action(state, action)\n",
        "        # Compute reward from day t to t+1\n",
        "        weights_new = [inc/20.0 for inc in new_state]  # convert increments to fractions\n",
        "        reward = compute_reward(weights_new, train_prices[t], train_prices[t+1])\n",
        "        reward = reward_shaper.shape(reward)\n",
        "\n",
        "        # Check if we've reached the end of an episode (done flag)\n",
        "        done = (t == train_days - 2)  # True if next_state will be the last state of episode\n",
        "        # Store the transition in replay memory\n",
        "        replay_buffer.push(state, action_idx, reward, new_state, done)\n",
        "        # prioritized_replay_buffer.add((state, action_idx, reward, new_state, done), priority=1.0)\n",
        "\n",
        "        # Update state\n",
        "        state = new_state\n",
        "        # # Perform a learning step if we have enough samples\n",
        "        if len(replay_buffer) >= batch_size:\n",
        "            # Sample a batch of transitions\n",
        "            states_batch, actions_batch, rewards_batch, next_states_batch, dones_batch = replay_buffer.sample(batch_size)\n",
        "\n",
        "            # Convert to tensors\n",
        "            # State and next state inputs as batch_size x 5 tensors (normalize allocations to [0,1])\n",
        "            state_tensor = torch.FloatTensor([ [inc/20.0 for inc in s] for s in states_batch ])\n",
        "            next_state_tensor = torch.FloatTensor([ [inc/20.0 for inc in s] for s in next_states_batch ])\n",
        "            action_tensor = torch.LongTensor(actions_batch)\n",
        "            reward_tensor = torch.FloatTensor(rewards_batch)\n",
        "            done_tensor   = torch.BoolTensor(dones_batch)\n",
        "\n",
        "            # Compute current Q values for each state-action in the batch\n",
        "            # policy_net(state_tensor) has shape [batch, action_dim]; gather along actions\n",
        "            q_values = ensemble_q_values(state_tensor)  # [batch, action_count]\n",
        "            state_action_values = q_values.gather(1, action_tensor.unsqueeze(1)).squeeze(1)\n",
        "            # Compute target Q values using target network\n",
        "            with torch.no_grad():\n",
        "                # next_q_values = target_net(next_state_tensor)  # [batch, action_count]\n",
        "                if use_double_dqn:\n",
        "                    # Step 1: For each next_state, select the best action using the online network\n",
        "                    # Online ensemble selects best action:\n",
        "                    # online_next_q = policy_net(next_state_tensor)  # shape: [batch, action_dim]\n",
        "                    online_next_q = ensemble_q_values(next_state_tensor)  # avg Q from ensemble networks\n",
        "                    best_actions = online_next_q.argmax(dim=1, keepdim=True)  # best action indices from online net\n",
        "                    # Step 2: Evaluate these actions using the target network\n",
        "                    # For evaluation, take average target Q from target ensemble\n",
        "                    q_vals_targets_list = []\n",
        "                    for target_net in ensemble_targets:\n",
        "                        q_vals_targets_list.append(target_net(next_state_tensor))\n",
        "                    # target_next_q = target_net(next_state_tensor)\n",
        "                    target_next_q = torch.stack(q_vals_targets_list).mean(dim=0)\n",
        "                    selected_q = target_next_q.gather(1, best_actions).squeeze(1)\n",
        "                else:\n",
        "                    # Standard DQN target: use the max Q-value from the target network directly\n",
        "                    q_vals_targets_list = []\n",
        "                    for target_net in ensemble_targets:\n",
        "                        q_vals_targets_list.append(target_net(next_state_tensor))\n",
        "                    target_next_q = torch.stack(q_vals_targets_list).mean(dim=0)\n",
        "                    selected_q = target_next_q.max(dim=1)[0]\n",
        "\n",
        "            selected_q = selected_q * (1 - done_tensor.float())\n",
        "\n",
        "            # target_values = []\n",
        "            # for i in range(batch_size):\n",
        "            #     if done_tensor[i]:\n",
        "            #         # If episode ended at this transition, target is just the immediate reward\n",
        "            #         target_values.append(reward_tensor[i])\n",
        "            #     else:\n",
        "            #         # Not done: use Bellman update with next state's max Q\n",
        "            #         # Mask invalid actions for next state i\n",
        "            #         next_state = next_states_batch[i]\n",
        "            #         valid_next_actions = get_valid_actions(next_state)\n",
        "            #         # Find indices of valid actions for next state\n",
        "            #         valid_idxs = [action_to_index[a] for a in valid_next_actions]\n",
        "            #         # Max Q-value among valid next actions\n",
        "            #         max_next_Q = torch.max(next_q_values[i, valid_idxs])\n",
        "            #         target = rewards_batch[i] + gamma * max_next_Q.item()\n",
        "            #         target_values.append(target)\n",
        "            target_values = reward_tensor + gamma * selected_q\n",
        "\n",
        "            # target_tensor = torch.FloatTensor(target_values)\n",
        "            losses = F.smooth_l1_loss(state_action_values, target_values, reduction='none')\n",
        "            # loss = (losses * torch.FloatTensor(weights)).mean() if 'weights' in locals() else losses.mean()\n",
        "            loss = losses.mean()\n",
        "            # Optimize the model: MSE loss between state_action_values and target_values\n",
        "            # loss = F.mse_loss(state_action_values, target_values)\n",
        "\n",
        "            ensemble_optimizer.zero_grad()\n",
        "            loss.backward()\n",
        "            ensemble_optimizer.step()\n",
        "\n",
        "\n",
        "    # Decay epsilon after each episode\n",
        "    if epsilon > epsilon_min:\n",
        "        epsilon *= epsilon_decay\n",
        "        epsilon = max(epsilon, epsilon_min)\n",
        "    # Update target network periodically\n",
        "    if ep % target_update_freq == 0:\n",
        "        for net, target in zip(ensemble_nets, ensemble_targets):\n",
        "            target.load_state_dict(net.state_dict())\n",
        "    if ep % 10 == 0 or ep == episodes:\n",
        "        print(f\"Episode {ep}/{episodes} completed, epsilon={epsilon:.3f}\")\n",
        "print(\"Training completed.\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vzosJhCGr4NQ"
      },
      "source": [
        "### Policy Evaluation on Test Data"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 42,
      "metadata": {
        "id": "j8hnwCBjr15n"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Test period: Agent final portfolio value = 0.8050 (Return = -19.50%)\n",
            "Test period: Baseline final value = 0.6263 (Return = -37.37%)\n"
          ]
        }
      ],
      "source": [
        "def evaluate_policy(price_array, model, initial_state):\n",
        "    \"\"\"\n",
        "    Simulate the portfolio value over time on given price data using the provided model (greedy policy).\n",
        "    Returns a list of portfolio values for each day in the price data.\n",
        "    \"\"\"\n",
        "    days = price_array.shape[0]\n",
        "    state = initial_state\n",
        "    portfolio_value = 1.0  # start with $1.0\n",
        "    values = [portfolio_value]\n",
        "    # Baseline (buy-and-hold equal weights) for comparison\n",
        "    baseline_value = 1.0\n",
        "    # Compute initial shares for baseline (with equal weights)\n",
        "    baseline_weights = [0.2] * num_assets  # 20% each\n",
        "    baseline_shares = [baseline_weights[i] * baseline_value / price_array[0][i] for i in range(num_assets)]\n",
        "    for t in range(days - 1):\n",
        "        # Agent action: choose greedy (highest Q) action for current state\n",
        "        state_input = torch.FloatTensor([inc/20.0 for inc in state]).unsqueeze(0)\n",
        "        with torch.no_grad():\n",
        "            ensemble_qs = model(state_input)\n",
        "            q_vals = ensemble_qs.numpy().squeeze()\n",
        "        # Mask invalid actions for current state\n",
        "        valid_acts = get_valid_actions(state)\n",
        "        invalid_acts = set(all_actions) - set(valid_acts)\n",
        "        for act in invalid_acts:\n",
        "            q_vals[action_to_index[act]] = -1e9\n",
        "        best_act_idx = int(np.argmax(q_vals))\n",
        "        best_action = all_actions[best_act_idx]\n",
        "        # Rebalance portfolio according to best action\n",
        "        state = apply_action(state, best_action)\n",
        "        # Compute portfolio growth factor from day t to t+1 for agent\n",
        "        weights = [inc/20.0 for inc in state]\n",
        "        # growth_factor = 0.0\n",
        "        # for k in range(num_assets):\n",
        "        #     growth_factor += weights[k] * (price_array[t+1][k] / price_array[t][k])\n",
        "        growth_factor = sum([weights[k] * (price_array[t+1][k] / price_array[t][k]) for k in range(num_assets)])\n",
        "        portfolio_value *= growth_factor\n",
        "        values.append(portfolio_value)\n",
        "        # Update baseline value (its shares just appreciate with market, no rebalance)\n",
        "        baseline_portfolio_val = 0.0\n",
        "        for k in range(num_assets):\n",
        "            baseline_portfolio_val += baseline_shares[k] * price_array[t+1][k]\n",
        "        baseline_value = baseline_portfolio_val\n",
        "    # After iterating, 'values' list contains portfolio value from start to end of period\n",
        "    final_return = (portfolio_value - 1.0) / 1.0 * 100  # in %\n",
        "    baseline_return = (baseline_value - 1.0) / 1.0 * 100\n",
        "    print(f\"Test period: Agent final portfolio value = {portfolio_value:.4f} (Return = {final_return:.2f}%)\")\n",
        "    print(f\"Test period: Baseline final value = {baseline_value:.4f} (Return = {baseline_return:.2f}%)\")\n",
        "    return values, baseline_weights\n",
        "\n",
        "# Evaluate the trained model on test data\n",
        "agent_values = evaluate_policy(test_prices, ensemble_q_values, initial_state)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "a3zdHpEGGpHd"
      },
      "outputs": [],
      "source": []
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "T4",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.9.6"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
