{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 133,
   "metadata": {},
   "outputs": [],
   "source": [
    "import math\n",
    "import random\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "from collections import deque\n",
    "# %pip install yfinance\n",
    "import yfinance as yf\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Data Preparation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Downloading price data for tickers: ['AAPL', 'MSFT', 'GOOGL', 'SBUX', 'TSLA']\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[*********************100%***********************]  5 of 5 completed"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Data downloaded and saved to prices.csv\n",
      "Training days: 2014, Testing days: 250\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "# Define the 10 assets (tickers) for the portfolio\n",
    "# tickers = [\"AAPL\", \"MSFT\", \"GOOGL\", \"AMZN\", \"TSLA\", \"BA\", \"NFLX\", \"NVDA\", \"META\", \"SBUX\"]\n",
    "# tickers = [\"AAPL\", \"MSFT\", \"GOOGL\", \"SBUX\", \"TSLA\"]\n",
    "tickers = [\"GME\", \"AMC\", \"SPCE\", \"NVAX\", \"NOK\"]\n",
    "# Date range for historical data\n",
    "start_date = \"2015-01-01\"\n",
    "end_date   = \"2023-12-31\"\n",
    "\n",
    "# Try to load price data from a local CSV, otherwise download using yfinance\n",
    "data_file = \"prices.csv\"\n",
    "try:\n",
    "    prices_df = pd.read_csv(data_file, index_col=0, parse_dates=True)\n",
    "    print(\"Loaded data from\", data_file)\n",
    "except FileNotFoundError:\n",
    "    print(\"Downloading price data for tickers:\", tickers)\n",
    "    df = yf.download(tickers, start=start_date, end=end_date, interval=\"1d\")\n",
    "    # Extract the 'Close' prices from the MultiIndex DataFrame\n",
    "    prices_df = df.xs('Close', axis=1, level='Price')\n",
    "    prices_df.dropna(inplace=True)\n",
    "    prices_df.to_csv(data_file)\n",
    "    print(\"Data downloaded and saved to\", data_file)\n",
    "\n",
    "# Split data into training (first 4 years) and testing (last year)\n",
    "train_df = prices_df[prices_df.index < \"2023-01-01\"]\n",
    "test_df  = prices_df[prices_df.index >= \"2023-01-01\"]\n",
    "train_prices = train_df.values  # shape: [train_days, 5]\n",
    "test_prices  = test_df.values   # shape: [test_days, 5]\n",
    "num_assets = train_prices.shape[1]\n",
    "print(f\"Training days: {train_prices.shape[0]}, Testing days: {test_prices.shape[0]}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### State Encoding/Decoding and Actions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 135,
   "metadata": {},
   "outputs": [],
   "source": [
    "def encode_state(increments):\n",
    "    \"\"\"\n",
    "    Encode a state (tuple of 5 integer increments) as a 5-letter string.\n",
    "    Each integer (0-20) represents allocation in 5% units.\n",
    "    'A' corresponds to 0 (0%), 'B' to 1 (5%), ..., 'U' to 20 (100%).\n",
    "    \"\"\"\n",
    "    return \"\".join(chr(ord('A') + inc) for inc in increments)\n",
    "\n",
    "def decode_state(state_str):\n",
    "    \"\"\"\n",
    "    Decode a 5-letter state string back into a tuple of 5 increments (0-20 each).\n",
    "    \"\"\"\n",
    "    return tuple(ord(ch) - ord('A') for ch in state_str)\n",
    "\n",
    "# Define all possible actions (including no-action) as tuples\n",
    "all_actions = [(i, j) for i in range(num_assets) for j in range(num_assets) if i != j]\n",
    "all_actions.append((None, None))  # (None,None) denotes no rebalance action\n",
    "action_count = len(all_actions)  # should be 21 (for 5 assets)\n",
    "# Create a mapping from action tuple to index in all_actions list\n",
    "action_to_index = {act: idx for idx, act in enumerate(all_actions)}\n",
    "\n",
    "def get_valid_actions(state):\n",
    "    \"\"\"\n",
    "    Given a state (tuple of 5 increments or state string), return a list of valid action tuples.\n",
    "    A transfer action (i,j) is valid if the current state has at least 5% in asset i (increment >= 1)\n",
    "    and at most 95% in asset j (increment <= 19). The no-action (None,None) is always valid.\n",
    "    \"\"\"\n",
    "    # If state is given as a string, decode it to a tuple of increments\n",
    "    increments = decode_state(state) if isinstance(state, str) else tuple(state)\n",
    "    valid_actions = []\n",
    "    for act in all_actions:\n",
    "        if act == (None, None):\n",
    "            valid_actions.append(act)\n",
    "        else:\n",
    "            i, j = act\n",
    "            if increments[i] >= 1 and increments[j] <= 19:\n",
    "                valid_actions.append(act)\n",
    "    return valid_actions\n",
    "\n",
    "def apply_action(state, action):\n",
    "    \"\"\"\n",
    "    Apply a rebalancing action to a state.\n",
    "    - state: current state as a tuple of 5 increments (sums to 20).\n",
    "    - action: a tuple (i, j) meaning transfer 5% (one increment) from asset i to asset j.\n",
    "              (None, None) means no rebalancing.\n",
    "    Returns the new state (tuple of 5 increments) after applying the action.\n",
    "    \"\"\"\n",
    "    increments = list(state)\n",
    "    if action is None or action == (None, None):\n",
    "        # No rebalance\n",
    "        return tuple(increments)\n",
    "    i, j = action\n",
    "    # Transfer one increment from i to j (if possible)\n",
    "    if increments[i] >= 1 and increments[j] <= 19:\n",
    "        increments[i] -= 1\n",
    "        increments[j] += 1\n",
    "    return tuple(increments)\n",
    "\n",
    "def compute_reward(weights_frac, price_today, price_next):\n",
    "    \"\"\"\n",
    "    Compute the log return of the portfolio for one time step.\n",
    "    - weights_frac: list of 5 asset weight fractions after rebalancing on day t (sum=1).\n",
    "    - price_today: prices of the 5 assets at day t.\n",
    "    - price_next: prices of the 5 assets at day t+1.\n",
    "    Returns: log(portfolio return) from day t to t+1.\n",
    "    \"\"\"\n",
    "    # Portfolio value growth factor = sum_k w_k * (price_next_k / price_today_k)\n",
    "    growth_factor = 0.0\n",
    "    for k in range(num_assets):\n",
    "        growth_factor += weights_frac[k] * (price_next[k] / price_today[k])\n",
    "    # Reward is the log of the growth factor\n",
    "    return math.log(growth_factor)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Sharpe Ratio"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 136,
   "metadata": {},
   "outputs": [],
   "source": [
    "class SharpeRewardShaper:\n",
    "    def __init__(self, window=30, epsilon=1e-6):\n",
    "        self.window = window\n",
    "        self.rewards_history = []\n",
    "        self.epsilon = epsilon\n",
    "\n",
    "    def shape(self, raw_reward):\n",
    "        self.rewards_history.append(raw_reward)\n",
    "        if len(self.rewards_history) > self.window:\n",
    "            self.rewards_history.pop(0)\n",
    "        avg_reward = np.mean(self.rewards_history)\n",
    "        std_reward = np.std(self.rewards_history) + self.epsilon\n",
    "        sharpe = avg_reward / std_reward\n",
    "        return sharpe\n",
    "\n",
    "reward_shaper = SharpeRewardShaper(window=30)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Replay Buffer for Experience Replay"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 137,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ReplayBuffer:\n",
    "    def __init__(self, capacity):\n",
    "        \"\"\"\n",
    "        Replay buffer to store past transitions for experience replay.\n",
    "        Stores tuples of (state, action_index, reward, next_state, done).\n",
    "        \"\"\"\n",
    "        self.capacity = capacity\n",
    "        self.buffer = deque(maxlen=capacity)\n",
    "    \n",
    "    def push(self, state, action_idx, reward, next_state, done):\n",
    "        \"\"\"Save a transition to the buffer.\"\"\"\n",
    "        self.buffer.append((state, action_idx, reward, next_state, done))\n",
    "    \n",
    "    def sample(self, batch_size):\n",
    "        \"\"\"\n",
    "        Sample a random batch of transitions from the buffer.\n",
    "        Returns: tuples (states, action_idxs, rewards, next_states, dones) for the batch.\n",
    "        \"\"\"\n",
    "        batch = random.sample(self.buffer, batch_size)\n",
    "        # Extract each component into separate lists\n",
    "        states, action_idxs, rewards, next_states, dones = zip(*batch)\n",
    "        return list(states), list(action_idxs), list(rewards), list(next_states), list(dones)\n",
    "    \n",
    "    def __len__(self):\n",
    "        \"\"\"Current size of the buffer.\"\"\"\n",
    "        return len(self.buffer)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Prioritized Replay Buffer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 138,
   "metadata": {},
   "outputs": [],
   "source": [
    "# class PrioritizedReplayBuffer:\n",
    "#     def __init__(self, capacity, alpha=0.6):\n",
    "#         \"\"\"\n",
    "#         Prioritized replay buffer with given capacity.\n",
    "#         alpha: how much prioritization to use (0 = no prioritization, 1 = full prioritization).\n",
    "#         \"\"\"\n",
    "#         self.capacity = capacity\n",
    "#         self.alpha = alpha\n",
    "#         self.buffer = []      # to store transitions (s, a, r, s', done)\n",
    "#         self.priorities = np.zeros(capacity, dtype=np.float32)  # priority values for each entry\n",
    "#         self.next_idx = 0\n",
    "#         self.size = 0\n",
    "\n",
    "#     def add(self, transition, priority=1.0):\n",
    "#         \"\"\"Add a new experience with given priority (use high priority for new transitions to ensure sampling).\"\"\"\n",
    "#         max_prio = self.priorities.max() if self.size > 0 else 1.0\n",
    "#         # Use max priority for new transitions to guarantee it will be sampled at least once\n",
    "#         if priority is None:\n",
    "#             priority = max_prio\n",
    "#         # If buffer not yet full, append; otherwise overwrite oldest\n",
    "#         if self.size < self.capacity:\n",
    "#             self.buffer.append(transition)\n",
    "#         else:\n",
    "#             self.buffer[self.next_idx] = transition\n",
    "#         # Set priority (raised to alpha)\n",
    "#         self.priorities[self.next_idx] = (priority ** self.alpha)\n",
    "#         # Move index forward and wrap around\n",
    "#         self.next_idx = (self.next_idx + 1) % self.capacity\n",
    "#         # Track size up to capacity\n",
    "#         self.size = min(self.size + 1, self.capacity)\n",
    "\n",
    "#     def sample(self, batch_size, beta=0.4):\n",
    "#         \"\"\"\n",
    "#         Sample a batch of experiences with probabilities proportional to priorities.\n",
    "#         beta: compensation factor for importance sampling (0 = no correction, 1 = full correction).\n",
    "#         Returns (batch, indices, weights) for the sampled experiences.\n",
    "#         \"\"\"\n",
    "#         assert self.size > 0, \"Replay buffer is empty!\"\n",
    "#         # Compute probabilities for each entry\n",
    "#         prios = self.priorities[:self.size]\n",
    "#         probs = prios / prios.sum()\n",
    "#         # Sample indices according to probability distribution\n",
    "#         indices = np.random.choice(self.size, batch_size, p=probs, replace=False)\n",
    "#         # Collect transitions for these indices\n",
    "#         batch = [self.buffer[idx] for idx in indices]\n",
    "#         # Compute importance-sampling weights to correct bias\n",
    "#         total = self.size\n",
    "#         weights = (total * probs[indices]) ** (-beta)   # smaller probability -> larger weight\n",
    "#         weights = weights / weights.max()              # normalize weights (max weight = 1)\n",
    "#         weights = np.array(weights, dtype=np.float32)\n",
    "#         return batch, indices, weights\n",
    "\n",
    "#     def update_priorities(self, indices, errors, epsilon=1e-3):\n",
    "#         \"\"\"\n",
    "#         Update priorities of sampled transitions based on new TD-errors.\n",
    "#         Adding a small epsilon to avoid zero priority.\n",
    "#         \"\"\"\n",
    "#         for idx, error in zip(indices, errors):\n",
    "#             new_priority = abs(error) + epsilon\n",
    "#             self.priorities[idx] = (new_priority ** self.alpha)\n",
    "            \n",
    "#     def __len__(self):\n",
    "#         \"\"\"Current size of the buffer.\"\"\"\n",
    "#         return len(self.buffer)\n",
    "  # if len(prioritized_replay_buffer) >= batch_size:\n",
    "        #     # Sample a batch of transitions\n",
    "        #     batch, idxs, weights = prioritized_replay_buffer.sample(batch_size)\n",
    "        #     states_batch, actions_batch, rewards_batch, next_states_batch, dones_batch = zip(*batch)\n",
    "\n",
    "        #     # Convert to tensors\n",
    "        #     # State and next state inputs as batch_size x 5 tensors (normalize allocations to [0,1])\n",
    "        #     state_tensor = torch.FloatTensor([ [inc/20.0 for inc in s] for s in states_batch ])\n",
    "        #     next_state_tensor = torch.FloatTensor([ [inc/20.0 for inc in s] for s in next_states_batch ])\n",
    "        #     action_tensor = torch.LongTensor(actions_batch)\n",
    "        #     reward_tensor = torch.FloatTensor(rewards_batch)\n",
    "        #     done_tensor   = torch.BoolTensor(dones_batch)\n",
    "\n",
    "        #     # Compute current Q values for each state-action in the batch\n",
    "        #     # policy_net(state_tensor) has shape [batch, action_dim]; gather along actions\n",
    "        #     q_values = policy_net(state_tensor)  # [batch, action_count]\n",
    "        #     state_action_values = q_values.gather(1, action_tensor.unsqueeze(1)).squeeze(1)\n",
    "        #     # Compute target Q values using target network\n",
    "        #     with torch.no_grad():\n",
    "        #         next_q_values = target_net(next_state_tensor)  # [batch, action_count]\n",
    "        #     target_values = []\n",
    "        #     td_errors = []\n",
    "        #     for i in range(batch_size):\n",
    "        #         if done_tensor[i]:\n",
    "        #             # If episode ended at this transition, target is just the immediate reward\n",
    "        #             target = reward_tensor[i] \n",
    "        #         else:\n",
    "        #             # Not done: use Bellman update with next state's max Q\n",
    "        #             # Mask invalid actions for next state i\n",
    "        #             next_state = next_states_batch[i]\n",
    "        #             valid_next_actions = get_valid_actions(next_state)\n",
    "        #             # Find indices of valid actions for next state\n",
    "        #             valid_idxs = [action_to_index[a] for a in valid_next_actions]\n",
    "        #             # Max Q-value among valid next actions\n",
    "        #             max_next_Q = torch.max(next_q_values[i, valid_idxs])\n",
    "        #             target = rewards_batch[i] + gamma * max_next_Q.item()\n",
    "                \n",
    "        #         target_values.append(target)\n",
    "        #         td_error = (target - state_action_values[i].item())\n",
    "        #         td_errors.append(td_error)\n",
    "        #     target_tensor = torch.FloatTensor(target_values)\n",
    "        #     # Optimize the model: MSE loss between state_action_values and target_values\n",
    "        #     loss = F.mse_loss(state_action_values, target_tensor)\n",
    "        #     optimizer.zero_grad()\n",
    "        #     loss.backward()\n",
    "        #     optimizer.step()\n",
    "        #     # Now update the replay buffer priorities with the computed TD errors:\n",
    "        #     prioritized_replay_buffer.update_priorities(idxs, td_errors)\n",
    "        # if len(prioritized_replay_buffer) >= batch_size:\n",
    "        #     # Sample a batch of transitions from the prioritized replay buffer\n",
    "        #     batch, idxs, weights = prioritized_replay_buffer.sample(batch_size)\n",
    "        #     states_batch, actions_batch, rewards_batch, next_states_batch, dones_batch = zip(*batch)\n",
    "\n",
    "        #     # Convert to tensors:\n",
    "        #     state_tensor = torch.FloatTensor([[inc / 20.0 for inc in s] for s in states_batch])\n",
    "        #     next_state_tensor = torch.FloatTensor([[inc / 20.0 for inc in s] for s in next_states_batch])\n",
    "        #     action_tensor = torch.LongTensor(actions_batch)\n",
    "        #     reward_tensor = torch.FloatTensor(rewards_batch)\n",
    "        #     done_tensor = torch.BoolTensor(dones_batch)\n",
    "\n",
    "        #     # Compute current Q values for each state-action pair in the batch using the online network\n",
    "        #     q_values = policy_net(state_tensor)  # shape: [batch, action_dim]\n",
    "        #     state_action_values = q_values.gather(1, action_tensor.unsqueeze(1)).squeeze(1)\n",
    "            \n",
    "        #     # Compute target Q values using Double DQN logic:\n",
    "        #     with torch.no_grad():\n",
    "        #         if use_double_dqn:\n",
    "        #             # Step 1: For each next_state, select the best action using the online network\n",
    "        #             online_next_q = policy_net(next_state_tensor)  # shape: [batch, action_dim]\n",
    "        #             best_actions = online_next_q.argmax(dim=1, keepdim=True)  # best action indices from online net\n",
    "        #             # Step 2: Evaluate these actions using the target network\n",
    "        #             target_next_q = target_net(next_state_tensor)\n",
    "        #             selected_q = target_next_q.gather(1, best_actions).squeeze(1)\n",
    "        #         else:\n",
    "        #             # Standard DQN target: use the max Q-value from the target network directly\n",
    "        #             target_next_q = target_net(next_state_tensor)\n",
    "        #             selected_q = target_next_q.max(dim=1)[0]\n",
    "                \n",
    "        #     # If done flag is true for a transition, we do not add future reward.\n",
    "        #     selected_q = selected_q * (1 - done_tensor.float())\n",
    "            \n",
    "        #     # Compute target values for the batch: \n",
    "        #     target_values = rewards_tensor = reward_tensor + gamma * selected_q\n",
    "\n",
    "        #     # Now compute TD errors and per-sample losses:\n",
    "        #     # Optionally, if using PER, we can compute element-wise loss:\n",
    "        #     losses = torch.nn.functional.smooth_l1_loss(state_action_values, target_values, reduction='none')\n",
    "        #     loss = (losses * torch.FloatTensor(weights)).mean() if 'weights' in locals() else losses.mean()\n",
    "\n",
    "        #     optimizer.zero_grad()\n",
    "        #     loss.backward()\n",
    "        #     optimizer.step()\n",
    "\n",
    "        #     # Compute individual TD errors for updating priorities:\n",
    "        #     td_errors = []\n",
    "        #     for i in range(batch_size):\n",
    "        #         td_error = target_values[i].item() - state_action_values[i].item()\n",
    "        #         td_errors.append(td_error)\n",
    "        #     prioritized_replay_buffer.update_priorities(idxs, td_errors)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Neural Network for Q-value Function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 139,
   "metadata": {},
   "outputs": [],
   "source": [
    "class DQNNetwork(nn.Module):\n",
    "    def __init__(self, state_dim, action_dim):\n",
    "        \"\"\"\n",
    "        Neural network that approximates Q(s,a) for all actions a given state s.\n",
    "        state_dim: dimensionality of state input (e.g. 5)\n",
    "        action_dim: number of possible actions (e.g. 21)\n",
    "        \"\"\"\n",
    "        super(DQNNetwork, self).__init__()\n",
    "        self.fc1 = nn.Linear(state_dim, 128)\n",
    "        # self.bn1 = nn.BatchNorm1d(128)\n",
    "        # self.dropout1 = nn.Dropout(0.3)\n",
    "        self.fc2 = nn.Linear(128, 256)\n",
    "        # self.bn2 = nn.BatchNorm1d(256)\n",
    "        # self.dropout2 = nn.Dropout(0.3)\n",
    "        self.fc3 = nn.Linear(256, 128)\n",
    "        self.fc4 = nn.Linear(128, action_dim)\n",
    "\n",
    "        # hidden1 = 128\n",
    "        # hidden2 = 128\n",
    "        # self.fc1 = nn.Linear(state_dim, hidden1)\n",
    "        # self.fc2 = nn.Linear(hidden1, hidden2)\n",
    "        # self.fc3 = nn.Linear(hidden2, action_dim)\n",
    "\n",
    "\n",
    "    def forward(self, x):\n",
    "        # x is a tensor of shape [batch_size, state_dim]\n",
    "\n",
    "        x = F.relu((self.fc1(x)))\n",
    "        # x = self.dropout1(x)\n",
    "        x = F.relu((self.fc2(x)))\n",
    "        # x = self.dropout2(x)\n",
    "        x = F.relu(self.fc3(x))\n",
    "        q_vals = self.fc4(x)\n",
    "        return q_vals\n",
    "\n",
    "        # x = F.relu(self.fc1(x))\n",
    "        # x = F.relu(self.fc2(x))\n",
    "        # q_values = self.fc3(x)  # outputs Q-values for each action\n",
    "        # return q_values"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### DQN Agent Training Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 140,
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[140], line 140\u001b[0m\n\u001b[1;32m    138\u001b[0m         optimizer\u001b[38;5;241m.\u001b[39mzero_grad()\n\u001b[1;32m    139\u001b[0m         loss\u001b[38;5;241m.\u001b[39mbackward()\n\u001b[0;32m--> 140\u001b[0m         \u001b[43moptimizer\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mstep\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    143\u001b[0m \u001b[38;5;66;03m# Decay epsilon after each episode\u001b[39;00m\n\u001b[1;32m    144\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m epsilon \u001b[38;5;241m>\u001b[39m epsilon_min:\n",
      "File \u001b[0;32m~/Library/Python/3.9/lib/python/site-packages/torch/optim/optimizer.py:373\u001b[0m, in \u001b[0;36mOptimizer.profile_hook_step.<locals>.wrapper\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    368\u001b[0m         \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    369\u001b[0m             \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mRuntimeError\u001b[39;00m(\n\u001b[1;32m    370\u001b[0m                 \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mfunc\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m must return None or a tuple of (new_args, new_kwargs), but got \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mresult\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    371\u001b[0m             )\n\u001b[0;32m--> 373\u001b[0m out \u001b[38;5;241m=\u001b[39m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    374\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_optimizer_step_code()\n\u001b[1;32m    376\u001b[0m \u001b[38;5;66;03m# call optimizer step post hooks\u001b[39;00m\n",
      "File \u001b[0;32m~/Library/Python/3.9/lib/python/site-packages/torch/optim/optimizer.py:79\u001b[0m, in \u001b[0;36m_use_grad_for_differentiable.<locals>._use_grad\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m     77\u001b[0m \u001b[38;5;28;01mfinally\u001b[39;00m:\n\u001b[1;32m     78\u001b[0m     torch\u001b[38;5;241m.\u001b[39m_dynamo\u001b[38;5;241m.\u001b[39mgraph_break()\n\u001b[0;32m---> 79\u001b[0m     \u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mset_grad_enabled\u001b[49m\u001b[43m(\u001b[49m\u001b[43mprev_grad\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     80\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m ret\n",
      "File \u001b[0;32m~/Library/Python/3.9/lib/python/site-packages/torch/autograd/grad_mode.py:184\u001b[0m, in \u001b[0;36mset_grad_enabled.__init__\u001b[0;34m(self, mode)\u001b[0m\n\u001b[1;32m    182\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m__init__\u001b[39m(\u001b[38;5;28mself\u001b[39m, mode: \u001b[38;5;28mbool\u001b[39m) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m    183\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mprev \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mis_grad_enabled()\n\u001b[0;32m--> 184\u001b[0m     \u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_C\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_set_grad_enabled\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmode\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    185\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmode \u001b[38;5;241m=\u001b[39m mode\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "# Hyperparameters\n",
    "gamma = 0.98           # discount factor for future rewards\n",
    "learning_rate = 5e-4  # learning rate for optimizer\n",
    "epsilon_start = 1.0     # initial exploration rate\n",
    "epsilon_min   = 0.1     # minimum exploration rate\n",
    "epsilon_decay = 0.995    # multiplicative decay factor per episode\n",
    "episodes = 150          # number of training episodes\n",
    "batch_size = 64         # mini-batch size for replay updates\n",
    "target_update_freq = 5 # how often (in episodes) to update the target network\n",
    "replay_capacity = 10000 # capacity of the replay buffer\n",
    "use_double_dqn = True  # use Double DQN for training\n",
    "state_dim = 5\n",
    "action_dim = action_count  # 21\n",
    "\n",
    "# Initialize replay memory, policy network, target network, optimizer\n",
    "replay_buffer = ReplayBuffer(replay_capacity)\n",
    "# prioritized_replay_buffer = PrioritizedReplayBuffer(replay_capacity)\n",
    "\n",
    "policy_net = DQNNetwork(state_dim, action_dim)\n",
    "target_net = DQNNetwork(state_dim, action_dim)\n",
    "# Copy initial weights to target network\n",
    "target_net.load_state_dict(policy_net.state_dict())\n",
    "target_net.eval()  # target network in evaluation mode (not strictly necessary)\n",
    "optimizer = optim.Adam(policy_net.parameters(), lr=learning_rate)\n",
    "\n",
    "# Helper function: select action using epsilon-greedy policy\n",
    "def select_action(state, epsilon):\n",
    "    \"\"\"\n",
    "    Choose an action index for the given state using an epsilon-greedy strategy.\n",
    "    - state: current state as a tuple of increments.\n",
    "    - epsilon: current exploration rate.\n",
    "    Returns: (action_idx, action_tuple)\n",
    "    \"\"\"\n",
    "    valid_actions = get_valid_actions(state)\n",
    "    if random.random() < epsilon:\n",
    "        # Exploration: random valid action\n",
    "        action = random.choice(valid_actions)\n",
    "    else:\n",
    "        # Exploitation: choose best action according to Q-network\n",
    "        # Prepare state as a tensor (1x5) with normalized allocations\n",
    "        state_input = torch.FloatTensor([inc/20.0 for inc in state]).unsqueeze(0)\n",
    "        with torch.no_grad():\n",
    "            q_values = policy_net(state_input)  # shape [1, action_dim]\n",
    "        q_values = q_values.numpy().squeeze()  # shape [action_dim]\n",
    "        # Mask out invalid actions by setting their Q-value very low\n",
    "        # (So they won't be chosen as max)\n",
    "        invalid_actions = set(all_actions) - set(valid_actions)\n",
    "        for act in invalid_actions:\n",
    "            idx = action_to_index[act]\n",
    "            q_values[idx] = -1e9  # large negative to disable\n",
    "        best_idx = int(np.argmax(q_values))\n",
    "        action = all_actions[best_idx]\n",
    "    # Return both the index and the tuple representation\n",
    "    return action_to_index[action], action\n",
    "\n",
    "# Training loop\n",
    "initial_state = (4, 4, 4, 4, 4)  # start each episode with equal weights (\"EEEEE\")\n",
    "epsilon = epsilon_start\n",
    "train_days = train_prices.shape[0]\n",
    "for ep in range(1, episodes+1):\n",
    "    state = initial_state\n",
    "    # Iterate over each day in training data (except last, as we look ahead one day for reward)\n",
    "    for t in range(train_days - 1):\n",
    "        # Choose action (epsilon-greedy)\n",
    "        action_idx, action = select_action(state, epsilon)\n",
    "        # Apply action to get new state\n",
    "        new_state = apply_action(state, action)\n",
    "        # Compute reward from day t to t+1\n",
    "        weights_new = [inc/20.0 for inc in new_state]  # convert increments to fractions\n",
    "        raw_reward = compute_reward(weights_new, train_prices[t], train_prices[t+1])\n",
    "        reward = reward_shaper.shape(raw_reward)  # shape the reward using Sharpe ratio\n",
    "        # Check if we've reached the end of an episode (done flag)\n",
    "        done = (t == train_days - 2)  # True if next_state will be the last state of episode\n",
    "        # Store the transition in replay memory\n",
    "        replay_buffer.push(state, action_idx, reward, new_state, done)\n",
    "        # prioritized_replay_buffer.add((state, action_idx, reward, new_state, done), priority=1.0)\n",
    "\n",
    "        # Update state\n",
    "        state = new_state\n",
    "        # # Perform a learning step if we have enough samples\n",
    "        if len(replay_buffer) >= batch_size:\n",
    "            # Sample a batch of transitions\n",
    "            states_batch, actions_batch, rewards_batch, next_states_batch, dones_batch = replay_buffer.sample(batch_size)\n",
    "\n",
    "            # Convert to tensors\n",
    "            # State and next state inputs as batch_size x 5 tensors (normalize allocations to [0,1])\n",
    "            state_tensor = torch.FloatTensor([ [inc/20.0 for inc in s] for s in states_batch ])\n",
    "            next_state_tensor = torch.FloatTensor([ [inc/20.0 for inc in s] for s in next_states_batch ])\n",
    "            action_tensor = torch.LongTensor(actions_batch)\n",
    "            reward_tensor = torch.FloatTensor(rewards_batch)\n",
    "            done_tensor   = torch.BoolTensor(dones_batch)\n",
    "\n",
    "            # Compute current Q values for each state-action in the batch\n",
    "            # policy_net(state_tensor) has shape [batch, action_dim]; gather along actions\n",
    "            q_values = policy_net(state_tensor)  # [batch, action_count]\n",
    "            state_action_values = q_values.gather(1, action_tensor.unsqueeze(1)).squeeze(1)\n",
    "            # Compute target Q values using target network\n",
    "            with torch.no_grad():\n",
    "                # next_q_values = target_net(next_state_tensor)  # [batch, action_count]\n",
    "                if use_double_dqn:\n",
    "                    # Step 1: For each next_state, select the best action using the online network\n",
    "                    online_next_q = policy_net(next_state_tensor)  # shape: [batch, action_dim]\n",
    "                    best_actions = online_next_q.argmax(dim=1, keepdim=True)  # best action indices from online net\n",
    "                    # Step 2: Evaluate these actions using the target network\n",
    "                    target_next_q = target_net(next_state_tensor)\n",
    "                    selected_q = target_next_q.gather(1, best_actions).squeeze(1)\n",
    "                else:\n",
    "                    # Standard DQN target: use the max Q-value from the target network directly\n",
    "                    target_next_q = target_net(next_state_tensor)\n",
    "                    selected_q = target_next_q.max(dim=1)[0]\n",
    "\n",
    "            selected_q = selected_q * (1 - done_tensor.float())\n",
    " \n",
    "            # target_values = []\n",
    "            # for i in range(batch_size):\n",
    "            #     if done_tensor[i]:\n",
    "            #         # If episode ended at this transition, target is just the immediate reward\n",
    "            #         target_values.append(reward_tensor[i])\n",
    "            #     else:\n",
    "            #         # Not done: use Bellman update with next state's max Q\n",
    "            #         # Mask invalid actions for next state i\n",
    "            #         next_state = next_states_batch[i]\n",
    "            #         valid_next_actions = get_valid_actions(next_state)\n",
    "            #         # Find indices of valid actions for next state\n",
    "            #         valid_idxs = [action_to_index[a] for a in valid_next_actions]\n",
    "            #         # Max Q-value among valid next actions\n",
    "            #         max_next_Q = torch.max(next_q_values[i, valid_idxs])\n",
    "            #         target = rewards_batch[i] + gamma * max_next_Q.item()\n",
    "            #         target_values.append(target)\n",
    "            target_values = reward_tensor + gamma * selected_q\n",
    "\n",
    "            # target_tensor = torch.FloatTensor(target_values)\n",
    "            losses = F.smooth_l1_loss(state_action_values, target_values, reduction='none')\n",
    "            # loss = (losses * torch.FloatTensor(weights)).mean() if 'weights' in locals() else losses.mean()\n",
    "            loss = losses.mean()\n",
    "            # Optimize the model: MSE loss between state_action_values and target_values\n",
    "            # loss = F.mse_loss(state_action_values, target_values)\n",
    "            optimizer.zero_grad()\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "\n",
    "      \n",
    "    # Decay epsilon after each episode\n",
    "    if epsilon > epsilon_min:\n",
    "        epsilon *= epsilon_decay\n",
    "        epsilon = max(epsilon, epsilon_min)\n",
    "    # Update target network periodically\n",
    "    if ep % target_update_freq == 0:\n",
    "        target_net.load_state_dict(policy_net.state_dict())\n",
    "    if ep % 10 == 0 or ep == episodes:\n",
    "        print(f\"Episode {ep}/{episodes} completed, epsilon={epsilon:.3f}\")\n",
    "print(\"Training completed.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Policy Evaluation on Test Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test period: Agent final portfolio value = 1.5215 (Return = 52.15%)\n",
      "Test period: Baseline final value = 2.1592 (Return = 115.92%)\n"
     ]
    }
   ],
   "source": [
    "def evaluate_policy(price_array, model, initial_state):\n",
    "    \"\"\"\n",
    "    Simulate the portfolio value over time on given price data using the provided model (greedy policy).\n",
    "    Returns a list of portfolio values for each day in the price data.\n",
    "    \"\"\"\n",
    "    days = price_array.shape[0]\n",
    "    state = initial_state\n",
    "    portfolio_value = 1.0  # start with $1.0\n",
    "    values = [portfolio_value]\n",
    "    # Baseline (buy-and-hold equal weights) for comparison\n",
    "    baseline_value = 1.0\n",
    "    # Compute initial shares for baseline (with equal weights)\n",
    "    baseline_weights = [0.2] * num_assets  # 20% each\n",
    "    baseline_shares = [baseline_weights[i] * baseline_value / price_array[0][i] for i in range(num_assets)]\n",
    "    for t in range(days - 1):\n",
    "        # Agent action: choose greedy (highest Q) action for current state\n",
    "        state_input = torch.FloatTensor([inc/20.0 for inc in state]).unsqueeze(0)\n",
    "        with torch.no_grad():\n",
    "            q_vals = model(state_input).numpy().squeeze()\n",
    "        # Mask invalid actions for current state\n",
    "        valid_acts = get_valid_actions(state)\n",
    "        invalid_acts = set(all_actions) - set(valid_acts)\n",
    "        for act in invalid_acts:\n",
    "            q_vals[action_to_index[act]] = -1e9\n",
    "        best_act_idx = int(np.argmax(q_vals))\n",
    "        best_action = all_actions[best_act_idx]\n",
    "        # Rebalance portfolio according to best action\n",
    "        state = apply_action(state, best_action)\n",
    "        # Compute portfolio growth factor from day t to t+1 for agent\n",
    "        weights = [inc/20.0 for inc in state]\n",
    "        growth_factor = 0.0\n",
    "        for k in range(num_assets):\n",
    "            growth_factor += weights[k] * (price_array[t+1][k] / price_array[t][k])\n",
    "        portfolio_value *= growth_factor\n",
    "        values.append(portfolio_value)\n",
    "        # Update baseline value (its shares just appreciate with market, no rebalance)\n",
    "        baseline_portfolio_val = 0.0\n",
    "        for k in range(num_assets):\n",
    "            baseline_portfolio_val += baseline_shares[k] * price_array[t+1][k]\n",
    "        baseline_value = baseline_portfolio_val\n",
    "    # After iterating, 'values' list contains portfolio value from start to end of period\n",
    "    final_return = (portfolio_value - 1.0) / 1.0 * 100  # in %\n",
    "    baseline_return = (baseline_value - 1.0) / 1.0 * 100\n",
    "    print(f\"Test period: Agent final portfolio value = {portfolio_value:.4f} (Return = {final_return:.2f}%)\")\n",
    "    print(f\"Test period: Baseline final value = {baseline_value:.4f} (Return = {baseline_return:.2f}%)\")\n",
    "    return values, baseline_weights\n",
    "\n",
    "# Evaluate the trained model on test data\n",
    "agent_values, baseline_weights = evaluate_policy(test_prices, policy_net, initial_state)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Dueling DQN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class DuelingDQN(nn.Module):\n",
    "    def __init__(self, state_dim, action_dim):\n",
    "        super(DuelingDQN, self).__init__()\n",
    "        # Shared feature extraction layers\n",
    "        self.features = nn.Sequential(\n",
    "            nn.Linear(state_dim, 128),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(128, 128),\n",
    "            nn.ReLU()\n",
    "        )\n",
    "        # Value stream: outputs one value per state\n",
    "        self.value_stream = nn.Sequential(\n",
    "            nn.Linear(128, 128),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(128, 1)\n",
    "        )\n",
    "        # Advantage stream: outputs advantage for each action\n",
    "        self.adv_stream = nn.Sequential(\n",
    "            nn.Linear(128, 128),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(128, action_dim)\n",
    "        )\n",
    "    \n",
    "    def forward(self, x):\n",
    "        x = self.features(x)\n",
    "        value = self.value_stream(x)         # shape: [batch, 1]\n",
    "        advantage = self.adv_stream(x)         # shape: [batch, action_dim]\n",
    "        # Combine streams: Q(s,a) = V(s) + (A(s,a) - mean(A(s,·)))\n",
    "        advantage_mean = advantage.mean(dim=1, keepdim=True)\n",
    "        q_vals = value + (advantage - advantage_mean)\n",
    "        return q_vals\n",
    "        "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Dueling DQN Training Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# policy_net = DuelingDQN(state_dim, action_dim)\n",
    "# target_net = DuelingDQN(state_dim, action_dim)\n",
    "# target_net.load_state_dict(policy_net.state_dict())\n",
    "# target_net.eval()\n",
    "# optimizer = optim.Adam(policy_net.parameters(), lr=learning_rate)\n",
    "# replay_buffer = ReplayBuffer(replay_capacity)\n",
    "# # Optionally, create a flag to indicate which version is being used, e.g.,\n",
    "# use_dueling_dqn = True\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# def select_action(state, epsilon):\n",
    "#     \"\"\"\n",
    "#     Choose an action index for the given state using an epsilon-greedy strategy.\n",
    "#     - state: current state as a tuple of increments.\n",
    "#     - epsilon: current exploration rate.\n",
    "#     Returns: (action_idx, action_tuple)\n",
    "#     \"\"\"\n",
    "#     valid_actions = get_valid_actions(state)\n",
    "#     if random.random() < epsilon:\n",
    "#         # Exploration: random valid action\n",
    "#         action = random.choice(valid_actions)\n",
    "#     else:\n",
    "#         # Exploitation: choose best action according to Q-network\n",
    "#         # Prepare state as a tensor (1x5) with normalized allocations\n",
    "#         state_input = torch.FloatTensor([inc/20.0 for inc in state]).unsqueeze(0)\n",
    "#         with torch.no_grad():\n",
    "#             q_values = policy_net(state_input)  # shape [1, action_dim]\n",
    "#         q_values = q_values.numpy().squeeze()  # shape [action_dim]\n",
    "#         # Mask out invalid actions by setting their Q-value very low\n",
    "#         # (So they won't be chosen as max)\n",
    "#         invalid_actions = set(all_actions) - set(valid_actions)\n",
    "#         for act in invalid_actions:\n",
    "#             idx = action_to_index[act]\n",
    "#             q_values[idx] = -1e9  # large negative to disable\n",
    "#         best_idx = int(np.argmax(q_values))\n",
    "#         action = all_actions[best_idx]\n",
    "#     # Return both the index and the tuple representation\n",
    "#     return action_to_index[action], action\n",
    "\n",
    "# # Training loop\n",
    "# initial_state = (4, 4, 4, 4, 4)  # start each episode with equal weights (\"EEEEE\")\n",
    "# epsilon = epsilon_start\n",
    "# train_days = train_prices.shape[0]\n",
    "# for ep in range(1, episodes+1):\n",
    "#     state = initial_state\n",
    "#     # Iterate over each day in training data (except last, as we look ahead one day for reward)\n",
    "#     for t in range(train_days - 1):\n",
    "#         # Choose action (epsilon-greedy)\n",
    "#         action_idx, action = select_action(state, epsilon)\n",
    "#         # Apply action to get new state\n",
    "#         new_state = apply_action(state, action)\n",
    "#         # Compute reward from day t to t+1\n",
    "#         weights_new = [inc/20.0 for inc in new_state]  # convert increments to fractions\n",
    "#         reward = compute_reward(weights_new, train_prices[t], train_prices[t+1])\n",
    "#         # Check if we've reached the end of an episode (done flag)\n",
    "#         done = (t == train_days - 2)  # True if next_state will be the last state of episode\n",
    "#         # Store the transition in replay memory\n",
    "#         replay_buffer.push(state, action_idx, reward, new_state, done)\n",
    "#         # prioritized_replay_buffer.add((state, action_idx, reward, new_state, done), priority=1.0)\n",
    "\n",
    "#         # Update state\n",
    "#         state = new_state\n",
    "#         # # Perform a learning step if we have enough samples\n",
    "#         if len(replay_buffer) >= batch_size:\n",
    "#             # Sample a batch of transitions\n",
    "#             states_batch, actions_batch, rewards_batch, next_states_batch, dones_batch = replay_buffer.sample(batch_size)\n",
    "\n",
    "#             # Convert to tensors\n",
    "#             # State and next state inputs as batch_size x 5 tensors (normalize allocations to [0,1])\n",
    "#             state_tensor = torch.FloatTensor([ [inc/20.0 for inc in s] for s in states_batch ])\n",
    "#             next_state_tensor = torch.FloatTensor([ [inc/20.0 for inc in s] for s in next_states_batch ])\n",
    "#             action_tensor = torch.LongTensor(actions_batch)\n",
    "#             reward_tensor = torch.FloatTensor(rewards_batch)\n",
    "#             done_tensor   = torch.BoolTensor(dones_batch)\n",
    "\n",
    "#             # Compute current Q values for each state-action in the batch\n",
    "#             # policy_net(state_tensor) has shape [batch, action_dim]; gather along actions\n",
    "#             q_values = policy_net(state_tensor)  # [batch, action_count]\n",
    "#             state_action_values = q_values.gather(1, action_tensor.unsqueeze(1)).squeeze(1)\n",
    "#             # Compute target Q values using target network\n",
    "#             with torch.no_grad():\n",
    "#                 # next_q_values = target_net(next_state_tensor)  # [batch, action_count]\n",
    "#                 if use_double_dqn:\n",
    "#                     # Step 1: For each next_state, select the best action using the online network\n",
    "#                     online_next_q = policy_net(next_state_tensor)  # shape: [batch, action_dim]\n",
    "#                     best_actions = online_next_q.argmax(dim=1, keepdim=True)  # best action indices from online net\n",
    "#                     # Step 2: Evaluate these actions using the target network\n",
    "#                     target_next_q = target_net(next_state_tensor)\n",
    "#                     selected_q = target_next_q.gather(1, best_actions).squeeze(1)\n",
    "#                 else:\n",
    "#                     # Standard DQN target: use the max Q-value from the target network directly\n",
    "#                     target_next_q = target_net(next_state_tensor)\n",
    "#                     selected_q = target_next_q.max(dim=1)[0]\n",
    "\n",
    "#             selected_q = selected_q * (1 - done_tensor.float())\n",
    " \n",
    "#             target_values = reward_tensor + gamma * selected_q\n",
    "\n",
    "            \n",
    "#             loss = F.mse_loss(state_action_values, target_values)\n",
    "#             optimizer.zero_grad()\n",
    "#             loss.backward()\n",
    "#             optimizer.step()\n",
    "\n",
    "#     if epsilon > epsilon_min:\n",
    "#         epsilon *= epsilon_decay\n",
    "#         epsilon = max(epsilon, epsilon_min)\n",
    "#     # Update target network periodically\n",
    "#     if ep % target_update_freq == 0:\n",
    "#         target_net.load_state_dict(policy_net.state_dict())\n",
    "#     if ep % 10 == 0 or ep == episodes:\n",
    "#         print(f\"Episode {ep}/{episodes} completed, epsilon={epsilon:.3f}\")\n",
    "# print(\"Training completed.\")\n",
    "# agent_values, baseline_weights = evaluate_policy(test_prices, policy_net, initial_state)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
