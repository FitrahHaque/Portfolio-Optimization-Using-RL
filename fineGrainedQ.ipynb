{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 212,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "_Y77BeglBh0B",
        "outputId": "e96b675b-fabe-4d82-947e-bccea7072beb"
      },
      "outputs": [],
      "source": [
        "# from google.colab import drive\n",
        "# drive.mount('/content/drive')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 213,
      "metadata": {
        "id": "ygKC_nPqrODL"
      },
      "outputs": [],
      "source": [
        "import math\n",
        "import random\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "import torch.optim as optim\n",
        "import yfinance as yf\n",
        "from collections import deque\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "WaT-xj4XrdOK"
      },
      "source": [
        "### Data Preparation"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 214,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "9ILxQXy4rY-R",
        "outputId": "77615a20-66fc-412f-dfc7-bed4f770d3c6"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Loaded data from prices.csv\n",
            "Training days: 1323, Testing days: 250\n"
          ]
        }
      ],
      "source": [
        "# Define the 10 assets (tickers) for the portfolio\n",
        "# tickers = [\"AAPL\", \"MSFT\", \"GOOGL\", \"AMZN\", \"TSLA\", \"BA\", \"NFLX\", \"NVDA\", \"META\", \"SBUX\"]\n",
        "\n",
        "# tickers = [\"AAPL\", \"MSFT\", \"GOOGL\", \"SBUX\", \"TSLA\"]\n",
        "tickers = [\"GME\", \"AMC\", \"SPCE\", \"NVAX\", \"NOK\"]\n",
        "# tickers = [\"GME\", \"AMC\", \"BB\", \"NVAX\", \"NOK\"]\n",
        "# tickers = [\"GME\", \"AMC\", \"HMC\", \"NVAX\", \"NOK\"]\n",
        "# Date range for historical data\n",
        "start_date = \"2017-01-01\"\n",
        "end_date   = \"2023-12-31\"\n",
        "\n",
        "# Try to load price data from a local CSV, otherwise download using yfinance\n",
        "data_file = \"prices.csv\"\n",
        "try:\n",
        "    prices_df = pd.read_csv(data_file, index_col=0, parse_dates=True)\n",
        "    print(\"Loaded data from\", data_file)\n",
        "except FileNotFoundError:\n",
        "    print(\"Downloading price data for tickers:\", tickers)\n",
        "    df = yf.download(tickers, start=start_date, end=end_date, interval=\"1d\")\n",
        "    # Extract the 'Close' prices from the MultiIndex DataFrame\n",
        "    prices_df = df.xs('Close', axis=1, level='Price')\n",
        "    prices_df.dropna(inplace=True)\n",
        "    prices_df.to_csv(data_file)\n",
        "    print(\"Data downloaded and saved to\", data_file)\n",
        "\n",
        "# Split data into training (first 4 years) and testing (last year)\n",
        "full_train_df = prices_df[prices_df.index < \"2023-01-01\"]\n",
        "test_df  = prices_df[prices_df.index >= \"2023-01-01\"]\n",
        "full_train_prices = full_train_df.values  # shape: [train_days, 5]\n",
        "test_prices  = test_df.values   # shape: [test_days, 5]\n",
        "num_assets = full_train_prices.shape[1]\n",
        "print(f\"Training days: {full_train_prices.shape[0]}, Testing days: {test_prices.shape[0]}\")\n",
        "\n",
        "# Further split full training into training and validation (80/20)\n",
        "split_index = int(0.8 * full_train_prices.shape[0])\n",
        "train_prices = full_train_prices[:split_index]\n",
        "val_prices   = full_train_prices[split_index:]"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "###  Technical Indicators & State Representation"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 215,
      "metadata": {},
      "outputs": [],
      "source": [
        "def get_market_indicators(price_array, t, window=10):\n",
        "    \"\"\"\n",
        "    Compute technical indicators for each asset at time t:\n",
        "      - Moving Average Ratio: current price / moving average over window\n",
        "      - Volatility: std dev of daily returns over window (normalized by current price)\n",
        "    If not enough history exists, use current price and low volatility.\n",
        "    Returns a list of 2 values per asset.\n",
        "    \"\"\"\n",
        "    indicators = []\n",
        "    for asset in range(num_assets):\n",
        "        if t < window:\n",
        "            # Not enough history: use current price and set volatility to 0\n",
        "            moving_avg_ratio = 1.0\n",
        "            vol = 0.0\n",
        "        else:\n",
        "            window_prices = price_array[t-window+1:t+1, asset]\n",
        "            moving_avg = np.mean(window_prices)\n",
        "            moving_avg_ratio = price_array[t, asset] / (moving_avg + 1e-6)\n",
        "            returns = np.diff(window_prices) / (window_prices[:-1] + 1e-6)\n",
        "            vol = np.std(returns) / (price_array[t, asset] + 1e-6)\n",
        "        indicators.extend([moving_avg_ratio, vol])\n",
        "    return indicators\n",
        "\n",
        "def get_state(t, alloc_state, price_array):\n",
        "    \"\"\"\n",
        "    Constructs the state vector at time t by combining:\n",
        "      - Portfolio allocations (6 numbers: 5 assets + cash) normalized to [0,1]\n",
        "      - Market indicators for assets (2 per asset)\n",
        "    Returns a numpy array of length 6 + 10 = 16.\n",
        "    \"\"\"\n",
        "    # Normalize allocation percentages (they sum to 100)\n",
        "    alloc_norm = [a / 100.0 for a in alloc_state]\n",
        "    indicators = get_market_indicators(price_array, t)\n",
        "    return np.array(alloc_norm + indicators)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "uEJnt93hrlGz"
      },
      "source": [
        "### State Encoding/Decoding and Actions"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "0191obe4rd9I"
      },
      "outputs": [],
      "source": [
        "# Construct the new action space.\n",
        "# Each action is a tuple (src, dst, amt) where:\n",
        "# - src and dst are indices in assets_plus_cash (0..5)\n",
        "# - amt is in [1, 2, 3, 4, 5] representing a 1%-5% transfer.\n",
        "# We allow transferring funds between any two different assets.\n",
        "\n",
        "# Define all possible actions (including no-action) as tuples\n",
        "all_actions = []\n",
        "cash_idx = num_assets            # index 5 is for cash\n",
        "assets_plus_cash = list(range(num_assets)) + [cash_idx]  # [0,1,2,3,4,5]\n",
        "percents = [1,2,3,4,5]\n",
        "for pct in percents:\n",
        "    for src in assets_plus_cash:\n",
        "        for dst in assets_plus_cash:\n",
        "            if src != dst:\n",
        "                all_actions.append((src, dst, pct))\n",
        "all_actions.append((None, None, 0))  # No action\n",
        "action_count = len(all_actions) # 151\n",
        "# print(\"Number of actions:\", action_count)\n",
        "# Create a mapping from action tuple to index in all_actions list\n",
        "action_to_index = {act: idx for idx, act in enumerate(all_actions)}\n",
        "\n",
        "def get_valid_actions(state):\n",
        "    \"\"\"\n",
        "    Given a state (tuple of 6 integers summing to 100 representing allocations for 5 stocks and cash),\n",
        "    return a list of valid actions (from all_actions_new).\n",
        "    \n",
        "    Rules:\n",
        "      - If src is not cash, state[src] must be >= amt (i.e. you must have enough allocation to sell).\n",
        "      - Optionally, we can restrict that if dst is not cash, the allocation after transfer should not exceed 100.\n",
        "        (Here we assume no single asset can have more than 100%).\n",
        "      - The no-action (None, None, 0) is always valid.\n",
        "    \"\"\"\n",
        "    valid = []\n",
        "    for act in all_actions:\n",
        "        if act == (None, None, 0):\n",
        "            valid.append(act)\n",
        "        else:\n",
        "            src, dst, amt = act\n",
        "            # Check: if src is not cash, then it must have at least 'amt'\n",
        "            if src != cash_idx and state[src] < amt:\n",
        "                continue\n",
        "            # Check: if dst is not cash, ensure it does not exceed 100.\n",
        "            if dst != cash_idx and state[dst] + amt > 100:\n",
        "                continue\n",
        "            valid.append(act)\n",
        "    return valid\n",
        "\n",
        "\n",
        "def apply_action(state, action, cost_rate=0.00): \n",
        "    \"\"\"\n",
        "    Apply an action (src, dst, amt) to the state.\n",
        "    - state: tuple of 6 integers (allocations in % for 5 stocks and cash; sum to 100)\n",
        "    - action: (src, dst, amt). For example, (0,5,3) means transfer 3% from asset 0 to cash.\n",
        "      (None, None, 0) means no action.\n",
        "    Returns the new state as a tuple of 6 integers.\n",
        "    \"\"\"\n",
        "    transaction_cost = 0.0\n",
        "    state = list(state)\n",
        "    if action == (None, None, 0):\n",
        "        return tuple(state), transaction_cost\n",
        "    src, dst, amt = action\n",
        "    # compute cost = cost_rate × dollars moved (≈ amt% of portfolio)\n",
        "    transaction_cost = cost_rate * (amt / 100.0)\n",
        "    # If src is not cash, reduce its allocation by amt (clip at 0)\n",
        "    if src is not None and state[src] >= amt:\n",
        "        state[src] = state[src] - amt\n",
        "    # If dst is not cash, increase its allocation by amt (clip at 100)\n",
        "    if dst is not None and dst != cash_idx and state[dst] + amt <= 100:\n",
        "        state[dst] = state[dst] + amt\n",
        "    # If dst is cash, add amt to cash.\n",
        "    if dst == cash_idx:\n",
        "        state[cash_idx] = state[cash_idx] + amt\n",
        "        # renormalize so sum == 100 exactly\n",
        "    total = sum(state)\n",
        "    state = [100 * x / total for x in state]\n",
        "\n",
        "    return tuple(state), transaction_cost"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qMipXOQPiNWW"
      },
      "source": [
        "### Reward Shaping using a Rolling Sharpe Ratio"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 217,
      "metadata": {
        "id": "7WYWx-jViOO0"
      },
      "outputs": [],
      "source": [
        "class SharpeRewardShaper:\n",
        "    def __init__(self, window=30, epsilon=1e-6, drawdown_penalty=0.5):\n",
        "        self.window = window\n",
        "        self.rewards_history = deque(maxlen=window)\n",
        "        self.epsilon = epsilon\n",
        "        self.portfolio_max = -np.inf\n",
        "        self.drawdown_penalty = drawdown_penalty\n",
        "\n",
        "    def shape(self, raw_reward, current_portfolio):\n",
        "        \"\"\"\n",
        "        Enhances reward by:\n",
        "          - Computing rolling Sharpe ratio,\n",
        "          - Penalizing drawdowns (if current_portfolio is lower than past peak),\n",
        "          - Penalizing high volatility.\n",
        "        \"\"\"\n",
        "        self.rewards_history.append(raw_reward)\n",
        "        if len(self.rewards_history) > self.window:\n",
        "            self.rewards_history.pop(0)\n",
        "        avg_reward = np.mean(self.rewards_history)\n",
        "        std_reward = np.std(self.rewards_history) + self.epsilon\n",
        "        sharpe = avg_reward / std_reward\n",
        "\n",
        "        # Update running maximum portfolio value\n",
        "        self.portfolio_max = max(self.portfolio_max, current_portfolio)\n",
        "        drawdown = (self.portfolio_max - current_portfolio) / (self.portfolio_max + self.epsilon)\n",
        "        drawdown_penalty = drawdown * self.drawdown_penalty # factor can be tuned\n",
        "\n",
        "        # Volatility penalty: higher volatility (std reward) reduces reward\n",
        "        vol_penalty = std_reward * 0.5  # factor can be tuned\n",
        "\n",
        "        # Final shaped reward: encourage long term gain via n-step (raw_reward already multi-step) + penalize risks\n",
        "        shaped_reward = sharpe *0.5 + raw_reward * 0.5 - drawdown_penalty - vol_penalty\n",
        "\n",
        "        return shaped_reward\n",
        "\n",
        "def compute_reward(weights_frac, price_array, t, n_step=3, transaction_cost=0.000):\n",
        "    \"\"\"\n",
        "    Compute the reward using n-step return from time t to t+n_step.\n",
        "    Growth is computed by taking the weighted sum of asset returns over n_step days.\n",
        "    If t+n_step is beyond available data, use last available day.\n",
        "    \"\"\"\n",
        "    last_index = min(t + n_step, price_array.shape[0] - 1)\n",
        "    growth_factor = 0.0\n",
        "    for k in range(len(assets_plus_cash)):\n",
        "        # For cash, return is 1.0; for assets use price ratio over n steps.\n",
        "        if k == cash_idx:\n",
        "            ratio = 1.0\n",
        "        else:\n",
        "            ratio = price_array[last_index, k] / price_array[t, k]\n",
        "        growth_factor += weights_frac[k] * ratio\n",
        "        raw = math.log(growth_factor + 1e-6) - transaction_cost\n",
        "        return raw\n",
        "reward_shaper = SharpeRewardShaper(window=30)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Uqa6xKTSrqzp"
      },
      "source": [
        "### Replay Buffer for Experience Replay"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 218,
      "metadata": {
        "id": "88rVfnT-rnrp"
      },
      "outputs": [],
      "source": [
        "class ReplayBuffer:\n",
        "    def __init__(self, capacity):\n",
        "        \"\"\"\n",
        "        Replay buffer to store past transitions for experience replay.\n",
        "        Stores tuples of (state, action_index, reward, next_state, done).\n",
        "        \"\"\"\n",
        "        self.capacity = capacity\n",
        "        self.buffer = deque(maxlen=capacity)\n",
        "\n",
        "    def push(self, state, action_idx, reward, next_state, done):\n",
        "        \"\"\"Save a transition to the buffer.\"\"\"\n",
        "        self.buffer.append((state, action_idx, reward, next_state, done))\n",
        "\n",
        "    def sample(self, batch_size):\n",
        "        \"\"\"\n",
        "        Sample a random batch of transitions from the buffer.\n",
        "        Returns: tuples (states, action_idxs, rewards, next_states, dones) for the batch.\n",
        "        \"\"\"\n",
        "        batch = random.sample(self.buffer, batch_size)\n",
        "        # Extract each component into separate lists\n",
        "        states, action_idxs, rewards, next_states, dones = zip(*batch)\n",
        "        return list(states), list(action_idxs), list(rewards), list(next_states), list(dones)\n",
        "\n",
        "    def __len__(self):\n",
        "        \"\"\"Current size of the buffer.\"\"\"\n",
        "        return len(self.buffer)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-z0AwXHTrwmz"
      },
      "source": [
        "### Neural Network for Q-value Function"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 219,
      "metadata": {
        "id": "j6RJgJrfrtDJ"
      },
      "outputs": [],
      "source": [
        "class DQNNetwork(nn.Module):\n",
        "    def __init__(self, state_dim, action_dim):\n",
        "        \"\"\"\n",
        "        Neural network that approximates Q(s,a) for all actions a given state s.\n",
        "        state_dim: dimensionality of state input (e.g. 5)\n",
        "        action_dim: number of possible actions (e.g. 21)\n",
        "        \"\"\"\n",
        "        super(DQNNetwork, self).__init__()\n",
        "\n",
        "        # self.fc1 = nn.Linear(state_dim, 128)\n",
        "        # self.bn1 = nn.BatchNorm1d(128)\n",
        "        # self.dropout1 = nn.Dropout(0.3)\n",
        "        # self.fc2 = nn.Linear(128, 256)\n",
        "        # self.bn2 = nn.BatchNorm1d(256)\n",
        "        # self.dropout2 = nn.Dropout(0.3)\n",
        "        # self.fc3 = nn.Linear(256, 128)\n",
        "        # self.fc4 = nn.Linear(128, action_dim)\n",
        "\n",
        "        # hidden1 = 128\n",
        "        # hidden2 = 128\n",
        "        # self.fc1 = nn.Linear(state_dim, hidden1)\n",
        "        # self.fc2 = nn.Linear(hidden1, hidden2)\n",
        "        # self.fc3 = nn.Linear(hidden2, action_dim)\n",
        "\n",
        "        self.fc1 = nn.Linear(state_dim, 128)\n",
        "        self.ln1 = nn.LayerNorm(128)\n",
        "        self.dropout1 = nn.Dropout(0.3)\n",
        "        self.fc2 = nn.Linear(128, 256)\n",
        "        self.ln2 = nn.LayerNorm(256)\n",
        "        self.dropout2 = nn.Dropout(0.3)\n",
        "        self.fc3 = nn.Linear(256, 128)\n",
        "        self.fc4 = nn.Linear(128, action_dim)\n",
        "\n",
        "    def forward(self, x):\n",
        "        # x is a tensor of shape [batch_size, state_dim]\n",
        "\n",
        "        # x = F.relu(self.bn1(self.fc1(x)))\n",
        "        # x = self.dropout1(x)\n",
        "        # x = F.relu(self.bn2(self.fc2(x)))\n",
        "        # x = self.dropout2(x)\n",
        "        # x = F.relu(self.fc3(x))\n",
        "        # q_vals = self.fc4(x)\n",
        "\n",
        "        # x = F.relu(self.fc1(x))\n",
        "        # x = F.relu(self.fc2(x))\n",
        "        # q_values = self.fc3(x)  # outputs Q-values for each action\n",
        "\n",
        "        x = F.relu(self.ln1(self.fc1(x)))\n",
        "        x = self.dropout1(x)\n",
        "        x = F.relu(self.ln2(self.fc2(x)))\n",
        "        x = self.dropout2(x)\n",
        "        x = F.relu(self.fc3(x))\n",
        "        q_vals = self.fc4(x)\n",
        "\n",
        "        return q_vals"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8DkUKJ6zr0X7"
      },
      "source": [
        "### DQN Agent Training Setup"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "4FEtbjn1r0v7",
        "outputId": "aeb4afb6-650e-4ba8-903b-54dec9397ab2"
      },
      "outputs": [],
      "source": [
        "# Hyperparameters\n",
        "# gamma = 0.99            # discount factor for future rewards\n",
        "# learning_rate = 5e-4  # learning rate for optimizer\n",
        "# epsilon_start = 1.0     # initial exploration rate\n",
        "# epsilon_min   = 0.2     # minimum exploration rate\n",
        "# epsilon_decay = 0.995    # multiplicative decay factor per episode\n",
        "# episodes = 150          # number of training episodes\n",
        "# batch_size = 128         # mini-batch size for replay updates\n",
        "# target_update_freq = 5 # how often (in episodes) to update the target network\n",
        "# replay_capacity = 10000 # capacity of the replay buffer\n",
        "use_double_dqn = True  # use Double DQN for      \n",
        "state_dim = num_assets + 1 + num_assets * 2  # 6 (allocations) + 10 (2 indicators per each of the 5 assets) = 16\n",
        "# ensemble_size = 2\n",
        "# temperature = 1.0\n",
        "initial_state=(15, 15, 15, 15, 15, 25)\n",
        "# Initialize replay memory, policy network, target network, optimizer\n",
        "# prioritized_replay_buffer = PrioritizedReplayBuffer(replay_capacity)\n",
        "\n",
        "# policy_net = DQNNetwork(state_dim, action_dim)\n",
        "# target_net = DQNNetwork(state_dim, action_dim)\n",
        "# Copy initial weights to target network\n",
        "# target_net.load_state_dict(policy_net.state_dict())\n",
        "# target_net.eval()  # target network in evaluation mode (not strictly necessary)\n",
        "# optimizer = optim.Adam(policy_net.parameters(), lr=learning_rate)\n",
        "\n",
        "\n",
        "# # Create ensemble of networks and their corresponding target networks\n",
        "# ensemble_nets = [DQNNetwork(state_dim, action_count) for _ in range(ensemble_size)]\n",
        "# ensemble_targets = [DQNNetwork(state_dim, action_count) for _ in range(ensemble_size)]\n",
        "# for net, target in zip(ensemble_nets, ensemble_targets):\n",
        "#     target.load_state_dict(net.state_dict())\n",
        "#     target.eval()\n",
        "# # Combine parameters of all ensemble networks in one optimizer\n",
        "# ensemble_optimizer = optim.Adam([p for net in ensemble_nets for p in net.parameters()], lr=learning_rate)\n",
        "\n",
        "# For action selection and training, we take the average Q-values across ensemble members.\n",
        "def ensemble_q_values(state_input, nets):\n",
        "    # Temporarily store the training state of each network.\n",
        "    original_modes = [net.training for net in nets]\n",
        "\n",
        "    # Switch networks to eval mode for inference (to avoid BN issues with batch size 1)\n",
        "    for net in nets:\n",
        "        net.eval()\n",
        "\n",
        "    # Compute Q-values from each network and average them\n",
        "    q_vals_list = [net(state_input) for net in nets]  # shape: [ensemble_size, batch, action_dim]\n",
        "    avg_q_vals = torch.stack(q_vals_list, dim=0).mean(dim=0)\n",
        "\n",
        "    # Restore the original training mode of each network\n",
        "    for net, mode in zip(nets, original_modes):\n",
        "        if mode:\n",
        "            net.train()\n",
        "        else:\n",
        "            net.eval()\n",
        "\n",
        "    return avg_q_vals\n",
        "\n",
        "# Helper function: select action\n",
        "# def select_action(state, t, train_prices, epsilon, nets, temperature = 1.0):\n",
        "#     \"\"\"\n",
        "#     Hybrid exploration:\n",
        "#       - With probability epsilon, choose a random valid action.\n",
        "#       - Otherwise, use Boltzmann sampling (softmax over Q-values for valid actions).\n",
        "#     The state passed in is the portfolio allocation (tuple of 6 ints).\n",
        "#     We combine it with market indicators computed from price_array at time t.\n",
        "#     Returns: (action_idx, action_tuple)\n",
        "#     \"\"\"\n",
        "#     valid_actions = get_valid_actions(state)\n",
        "#     if random.random() < epsilon:\n",
        "#         # Exploration: random valid action\n",
        "#         action = random.choice(valid_actions)\n",
        "#     else:\n",
        "#         # Exploitation: choose best action according to Q-network\n",
        "#         # Normalize state: now state values are percentages out of 100\n",
        "#         state_vec = get_state(t, state, train_prices)\n",
        "#         state_tensor = torch.FloatTensor(state_vec).unsqueeze(0)\n",
        "#         with torch.no_grad():\n",
        "#           q_values = ensemble_q_values(state_tensor, nets)  # shape [1, action_count]\n",
        "#           q_values = q_values.numpy().squeeze()  # shape [action_count]\n",
        "#         # Mask out invalid actions by setting their Q-value very low\n",
        "#         # (So they won't be chosen as max)\n",
        "#         invalid_actions = set(all_actions) - set(valid_actions)\n",
        "#         for act in invalid_actions:\n",
        "#             # if act in action_to_index:\n",
        "#             q_values[action_to_index[act]] = -1e9  # large negative to disable\n",
        "#         # Boltzmann (softmax) sampling over valid Q-values\n",
        "#         # exp_q = np.exp(q_values / temperature)\n",
        "#         q_values = np.nan_to_num(q_values)  # replace NaN with 0\n",
        "#         q_values -= q_values.max()  # for numerical stability\n",
        "#         exp_q = np.exp(q_values / temperature)\n",
        "#         # Zero out probabilities for invalid actions\n",
        "#         for act in invalid_actions:\n",
        "#             exp_q[action_to_index[act]] = 0.0\n",
        "#         total = exp_q.sum()\n",
        "#         if total <= 0 or np.isnan(total):\n",
        "#             best_idx = int(np.nnanargmax(q_values))\n",
        "#             return best_idx, all_actions[best_idx]\n",
        "#         probs = exp_q / total\n",
        "#         action_idx = np.random.choice(len(probs), p=probs)\n",
        "#         action = all_actions[action_idx]\n",
        "\n",
        "#     # Return both the index and the tuple representation\n",
        "#     return action_to_index[action], action\n",
        "def select_action(state, t, train_prices, epsilon, nets):\n",
        "    \"\"\"\n",
        "    Epsilon-greedy exploration:\n",
        "      - With probability epsilon, choose a random valid action.\n",
        "      - Otherwise, pick the valid action with highest Q-value.\n",
        "    State is the 6-tuple allocation; we augment with indicators at time t.\n",
        "    \"\"\"\n",
        "    valid_actions = get_valid_actions(state)\n",
        "    # 1) Random exploration\n",
        "    if random.random() < epsilon:\n",
        "        action = random.choice(valid_actions)\n",
        "        return action_to_index[action], action\n",
        "\n",
        "    # 2) Greedy exploitation\n",
        "    state_vec   = get_state(t, state, train_prices)\n",
        "    state_tensor = torch.FloatTensor(state_vec).unsqueeze(0)\n",
        "    with torch.no_grad():\n",
        "        q_values = ensemble_q_values(state_tensor, nets).cpu().numpy().squeeze()\n",
        "\n",
        "    # Mask invalids\n",
        "    invalid = set(all_actions) - set(valid_actions)\n",
        "    for act in invalid:\n",
        "        q_values[action_to_index[act]] = -1e9\n",
        "\n",
        "    # Pick best\n",
        "    best_idx = int(np.argmax(q_values))\n",
        "    best_action = all_actions[best_idx]\n",
        "    return best_idx, best_action\n",
        "\n",
        "def evaluate_on_validation(val_prices, ensemble_nets, initial_state):\n",
        "    \"\"\"\n",
        "    Runs the greedy policy defined by ensemble_nets on val_prices,\n",
        "    returns the log of the final portfolio value (i.e. validation 'reward').\n",
        "    \"\"\"\n",
        "    val_portfolio = 1.0\n",
        "    state = initial_state\n",
        "\n",
        "    for t in range(val_prices.shape[0] - 1):\n",
        "        # build state + indicators\n",
        "        inp = get_state(t, state, val_prices)\n",
        "        inp_t = torch.FloatTensor(inp).unsqueeze(0)\n",
        "        with torch.no_grad():\n",
        "            q_vals = ensemble_q_values(inp_t, ensemble_nets).numpy().squeeze()\n",
        "\n",
        "        # mask invalid actions\n",
        "        valid = set(get_valid_actions(state))\n",
        "        for act in set(all_actions) - valid:\n",
        "            q_vals[action_to_index[act]] = -1e9\n",
        "\n",
        "        # take the greedy action\n",
        "        best_idx = int(np.argmax(q_vals))\n",
        "        state, transaction_cost = apply_action(state, all_actions[best_idx])\n",
        "\n",
        "        # update portfolio\n",
        "        w = [x/100 for x in state]\n",
        "        growth = sum(\n",
        "            w[k] * (val_prices[t+1][k] / val_prices[t][k])\n",
        "            if k < num_assets else w[cash_idx]\n",
        "            for k in range(len(assets_plus_cash))\n",
        "        )\n",
        "        val_portfolio = (val_portfolio - transaction_cost) * growth\n",
        "\n",
        "    return math.log(val_portfolio + 1e-6)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "def train_agent(episodes=150, \n",
        "                replay_capacity=10000, \n",
        "                batch_size=128, \n",
        "                gamma=0.99, \n",
        "                learning_rate=5e-4, \n",
        "                epsilon_start=1.0, \n",
        "                epsilon_min=0.2, \n",
        "                epsilon_decay=0.995, \n",
        "                target_update_freq=5, \n",
        "                n_step_return=5, \n",
        "                val_interval=10,\n",
        "                max_patience=5,\n",
        "                initial_state=initial_state):\n",
        "    \"\"\"\n",
        "    Runs the training loop for one experiment.\n",
        "    Returns:\n",
        "       - ensemble_nets: the trained ensemble networks (list)\n",
        "       - best_val_reward: best validation reward achieved (float)\n",
        "       - train_metrics: (optional) dictionary containing additional training metrics\n",
        "    \"\"\"\n",
        "\n",
        "    # Reset initial state and replay buffer\n",
        "    replay_buffer = ReplayBuffer(replay_capacity)\n",
        "    epsilon = epsilon_start\n",
        "    train_days = train_prices.shape[0]\n",
        "    \n",
        "    best_val_reward = -np.inf\n",
        "    patience = 0\n",
        "    best_ensemble_nets = None\n",
        "    temperature = 1.0\n",
        "    # initialize ensemble networks and target networks for each experiment\n",
        "    ensemble_size = 2\n",
        "    ensemble_nets = [DQNNetwork(state_dim, action_count) for _ in range(ensemble_size)]\n",
        "    ensemble_targets = [DQNNetwork(state_dim, action_count) for _ in range(ensemble_size)]\n",
        "    for net, target in zip(ensemble_nets, ensemble_targets):\n",
        "        target.load_state_dict(net.state_dict())\n",
        "        target.eval()\n",
        "    ensemble_optimizer = optim.Adam([p for net in ensemble_nets for p in net.parameters()], lr=learning_rate)\n",
        "\n",
        "    # Training loop\n",
        "    for ep in range(1, episodes + 1):\n",
        "        state = initial_state\n",
        "        portfolio_value = 1.0  # start with $1.0\n",
        "\n",
        "        # Iterate over each day (using train_prices, market indicators available from train_prices)\n",
        "        for t in range(0, train_days - 1 - n_step_return):\n",
        "            # Hybrid action selection: uses current day t and train_prices for market indicators\n",
        "            action_idx, action = select_action(state, t, train_prices, epsilon, ensemble_nets)\n",
        "            # temperature = max(0.1, temperature * 0.995)\n",
        "            new_state, transaction_cost = apply_action(state, action)\n",
        "\n",
        "            # Compute reward using n-step return\n",
        "            weights_new = [x/100.0 for x in new_state]\n",
        "            raw_reward = compute_reward(weights_new, train_prices, t, n_step=n_step_return,transaction_cost=transaction_cost)\n",
        "            portfolio_value *= math.exp(raw_reward)  # simulate n-step portfolio growth\n",
        "\n",
        "            # Shape reward using Sharpe reward shaper (with drawdown & volatility penalty)\n",
        "            reward = reward_shaper.shape(raw_reward, portfolio_value)\n",
        "            done = (t >= train_days - n_step_return - 1)\n",
        "\n",
        "            # Construct state representations (augmenting with market indicators)\n",
        "            state_repr = get_state(t, state, train_prices)\n",
        "            next_state_repr = get_state(t + n_step_return, new_state, train_prices)\n",
        "            replay_buffer.push(state_repr, action_idx, reward, next_state_repr, done)\n",
        "\n",
        "            state = new_state\n",
        "\n",
        "            # Learning step\n",
        "            if len(replay_buffer) >= batch_size:\n",
        "                state_reprs_batch, actions_batch, rewards_batch, next_state_reprs_batch, dones_batch = replay_buffer.sample(batch_size)\n",
        "                state_vec_tensor = torch.FloatTensor(np.array(state_reprs_batch))\n",
        "                next_state_vec_tensor = torch.FloatTensor(np.array(next_state_reprs_batch))\n",
        "                action_tensor = torch.LongTensor(actions_batch)\n",
        "                reward_tensor = torch.FloatTensor(rewards_batch)\n",
        "                done_tensor   = torch.BoolTensor(dones_batch)\n",
        "\n",
        "                # Compute current Q-values for each state in the batch using ensemble\n",
        "                q_values = ensemble_q_values(state_vec_tensor, ensemble_nets)\n",
        "                state_action_values = q_values.gather(1, action_tensor.unsqueeze(1)).squeeze(1)\n",
        "\n",
        "                # Compute target Q-values using target ensemble networks\n",
        "                with torch.no_grad():\n",
        "                    if use_double_dqn:\n",
        "                        online_next_q = ensemble_q_values(next_state_vec_tensor, ensemble_nets)\n",
        "                        best_actions = online_next_q.argmax(dim=1, keepdim=True)\n",
        "                        q_vals_targets_list = [target(next_state_vec_tensor) for target in ensemble_targets]\n",
        "                        target_next_q = torch.stack(q_vals_targets_list).mean(dim=0)\n",
        "                        selected_q = target_next_q.gather(1, best_actions).squeeze(1)\n",
        "                    else:\n",
        "                        # target_next_q = ensemble_q_values(next_state_vec_tensor, ensemble_targets)\n",
        "                        q_vals_targets_list = [target(next_state_vec_tensor) for target in ensemble_targets]\n",
        "                        target_next_q = torch.stack(q_vals_targets_list).mean(dim=0)\n",
        "                        selected_q = target_next_q.max(dim=1)[0]\n",
        "                selected_q = selected_q * (1 - done_tensor.float())\n",
        "                target_values = reward_tensor + gamma * selected_q\n",
        "\n",
        "                loss = F.mse_loss(state_action_values, target_values)\n",
        "                ensemble_optimizer.zero_grad()\n",
        "                loss.backward()\n",
        "                ensemble_optimizer.step()\n",
        "\n",
        "        # Decay epsilon and temprature after each episode\n",
        "        epsilon = max(epsilon * epsilon_decay, epsilon_min)\n",
        "        # temperature = max(0.1, temperature * 0.995)\n",
        "        # Update target networks periodically\n",
        "        if ep % target_update_freq == 0:\n",
        "            for net, target in zip(ensemble_nets, ensemble_targets):\n",
        "                target.load_state_dict(net.state_dict())\n",
        "\n",
        "        # Evaluate on the validation set every val_interval episodes for early stopping.\n",
        "        if ep % val_interval == 0:\n",
        "            val_reward = evaluate_on_validation(val_prices, ensemble_nets, initial_state)\n",
        "            print(f\"[Ep {ep}/{episodes}]  val={val_reward:.4f}  best={best_val_reward:.4f} epsilon={epsilon:.3f}\")\n",
        "\n",
        "            # 1) Early stopping: compare to best_val_reward\n",
        "            if val_reward > best_val_reward:\n",
        "                best_val_reward = val_reward\n",
        "                patience = 0\n",
        "                best_ensemble_nets = ensemble_nets\n",
        "                print(\"  ↳ new best! checkpointing ensemble\")\n",
        "            else:\n",
        "                patience += 1\n",
        "                print(f\"  ↳ no improvement over last ({patience}/{max_patience})\")\n",
        "                if patience >= max_patience:\n",
        "                    print(f\"Early stopping at episode {ep} (no improvement for {max_patience} checks)\")\n",
        "                    break\n",
        "    print(\"Training completed.\")\n",
        "    return ensemble_nets"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vzosJhCGr4NQ"
      },
      "source": [
        "### Policy Evaluation on Test Data"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 222,
      "metadata": {},
      "outputs": [],
      "source": [
        "def evaluate_policy(price_array, model, initial_state, runs=1):\n",
        "    \"\"\"\n",
        "    Evaluate the policy by simulating the portfolio over time using a greedy (deterministic) policy.\n",
        "    It runs the simulation multiple times (default 10), computes key metrics for each run, and returns\n",
        "    the best simulation (based on the final portfolio value). Key metrics include:\n",
        "      - Final portfolio value and return (%)\n",
        "      - Annualized Sharpe ratio (computed from daily returns)\n",
        "      - Maximum drawdown\n",
        "    Also computes baseline (buy-and-hold) performance for comparison.\n",
        "    \n",
        "    Parameters:\n",
        "      price_array: numpy array of asset prices.\n",
        "      model: the trained DQN model (or ensemble function) used for Q-value prediction.\n",
        "      initial_state: initial portfolio allocation (tuple of 6 integers).\n",
        "      runs: number of simulation runs.\n",
        "\n",
        "    Returns:\n",
        "      best_values: best portfolio value time series (list of portfolio values for each day).\n",
        "      best_metrics: dictionary of metrics for the best run.\n",
        "      baseline_value: final portfolio value using baseline (buy-and-hold equal weights).\n",
        "      baseline_return: baseline return in %.\n",
        "      all_metrics: list of metric dictionaries for each run.\n",
        "    \"\"\"\n",
        "    \n",
        "    best_values = None\n",
        "    best_final_value = -np.inf\n",
        "    best_metrics = {}\n",
        "    all_metrics = []\n",
        "    \n",
        "    # Run the simulation multiple times\n",
        "    for run in range(runs):\n",
        "        days = price_array.shape[0]\n",
        "        state = initial_state\n",
        "        portfolio_value = 1.0  # Start with $1.0\n",
        "        values = [portfolio_value]\n",
        "        daily_returns = []  # for Sharpe ratio computation\n",
        "        \n",
        "        for t in range(days - 1):\n",
        "            # Get the state vector (which includes allocations and market indicators)\n",
        "            state_input = get_state(t, state, price_array)\n",
        "            state_tensor = torch.FloatTensor(state_input).unsqueeze(0)\n",
        "            \n",
        "            # Predict Q-values for the current state\n",
        "            with torch.no_grad():\n",
        "                q_vals = ensemble_q_values(state_tensor, model).numpy().squeeze()\n",
        "            \n",
        "            # Mask out invalid actions\n",
        "            valid_acts = get_valid_actions(state)\n",
        "            invalid_acts = set(all_actions) - set(valid_acts)\n",
        "            for act in invalid_acts:\n",
        "                q_vals[action_to_index[act]] = -1e9\n",
        "                \n",
        "            best_act_idx = int(np.argmax(q_vals))\n",
        "            best_action = all_actions[best_act_idx]\n",
        "            \n",
        "            # Rebalance portfolio based on the chosen action\n",
        "            state, transaction_cost = apply_action(state, best_action)\n",
        "            \n",
        "            # Compute portfolio growth factor for day t to t+1\n",
        "            weights = [x / 100.0 for x in state]\n",
        "            growth_factor = sum([\n",
        "                weights[k] * (price_array[t+1][k] / price_array[t][k])\n",
        "                if k < num_assets else weights[cash_idx]\n",
        "                for k in range(len(assets_plus_cash))\n",
        "            ])\n",
        "            portfolio_value = (portfolio_value - transaction_cost) * growth_factor\n",
        "            values.append(portfolio_value)  # store portfolio value after transaction cost\n",
        "            daily_returns.append(growth_factor - 1)  # daily return (excess return if risk-free rate is 0)\n",
        "        \n",
        "        # Final return in percentage terms\n",
        "        final_return = (portfolio_value - 1.0) * 100.0\n",
        "        \n",
        "        # Compute Sharpe ratio: mean daily return / std daily return * sqrt(252)\n",
        "        mean_daily = np.mean(daily_returns)\n",
        "        std_daily = np.std(daily_returns) if np.std(daily_returns) > 0 else 1e-6\n",
        "        sharpe_ratio = mean_daily / std_daily * np.sqrt(price_array.shape[0])\n",
        "        # Compute maximum drawdown\n",
        "        values_array = np.array(values)\n",
        "        peak = np.maximum.accumulate(values_array)\n",
        "        drawdown = (peak - values_array) / peak\n",
        "        max_drawdown = np.max(drawdown)\n",
        "        \n",
        "        metrics = {\n",
        "            'final_portfolio_value': portfolio_value,\n",
        "            'final_return': final_return,\n",
        "            'sharpe_ratio': sharpe_ratio,\n",
        "            'max_drawdown': max_drawdown\n",
        "        }\n",
        "        all_metrics.append(metrics)\n",
        "        \n",
        "        # Update best run based on final portfolio value\n",
        "        if portfolio_value > best_final_value:\n",
        "            best_final_value = portfolio_value\n",
        "            best_values = values\n",
        "            best_metrics = metrics\n",
        "    \n",
        "    # Baseline evaluation: Buy-and-hold with equal weights for assets (cash remains unaltered)\n",
        "    # days = price_array.shape[0]\n",
        "    # baseline_value = 1.0\n",
        "    # baseline_weights = [1.0 / num_assets] * num_assets  # equal weights on assets\n",
        "    # baseline_shares = [baseline_weights[i] * baseline_value / price_array[0][i] for i in range(num_assets)]\n",
        "    # # Assuming no rebalancing, simulate value appreciation over time\n",
        "    # for t in range(days - 1):\n",
        "    #     baseline_portfolio_val = 0.0\n",
        "    #     for k in range(num_assets):\n",
        "    #         baseline_portfolio_val += baseline_shares[k] * price_array[t+1][k]\n",
        "    #     baseline_value = baseline_portfolio_val\n",
        "    # baseline_return = (baseline_value - 1.0) * 100.0\n",
        "    \n",
        "    print(\"Best simulation metrics:\")\n",
        "    print(f\"  Final portfolio value: {best_metrics['final_portfolio_value']:.4f}\")\n",
        "    print(f\"  Return: {best_metrics['final_return']:.2f}%\")\n",
        "    print(f\"  Annualized Sharpe ratio: {best_metrics['sharpe_ratio']:.4f}\")\n",
        "    print(f\"  Maximum drawdown: {best_metrics['max_drawdown']:.2%}\")\n",
        "    print()\n",
        "    # print(f\"Baseline portfolio value: {baseline_value:.4f}\")\n",
        "    # print(f\"  Return: {baseline_return:.2f}%\")\n",
        "    \n",
        "    return best_values, best_metrics, all_metrics"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 223,
      "metadata": {},
      "outputs": [],
      "source": [
        "def evaluate_baseline(price_array):\n",
        "    days = price_array.shape[0]\n",
        "    baseline_value = 1.0\n",
        "    baseline_weights = [1.0 / num_assets] * num_assets  # equal weights on assets\n",
        "    baseline_shares = [baseline_weights[i] * baseline_value / price_array[0][i] for i in range(num_assets)]\n",
        "    # Assuming no rebalancing, simulate value appreciation over time\n",
        "    for t in range(days - 1):\n",
        "        baseline_portfolio_val = 0.0\n",
        "        for k in range(num_assets):\n",
        "            baseline_portfolio_val += baseline_shares[k] * price_array[t+1][k]\n",
        "        baseline_value = baseline_portfolio_val\n",
        "    baseline_return = (baseline_value - 1.0) * 100.0\n",
        "    return baseline_value, baseline_return"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 224,
      "metadata": {
        "id": "a3zdHpEGGpHd"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Starting experiment 1\n",
            "[Ep 10/200]  val=-0.2346  best=-inf epsilon=0.951\n",
            "  ↳ new best! checkpointing ensemble\n",
            "[Ep 20/200]  val=-2.1333  best=-0.2346 epsilon=0.905\n",
            "  ↳ no improvement over last (1/5)\n",
            "[Ep 30/200]  val=-1.3214  best=-0.2346 epsilon=0.860\n",
            "  ↳ no improvement over last (2/5)\n",
            "[Ep 40/200]  val=-0.5679  best=-0.2346 epsilon=0.818\n",
            "  ↳ no improvement over last (3/5)\n",
            "[Ep 50/200]  val=-1.0133  best=-0.2346 epsilon=0.778\n",
            "  ↳ no improvement over last (4/5)\n",
            "[Ep 60/200]  val=-0.6228  best=-0.2346 epsilon=0.740\n",
            "  ↳ no improvement over last (5/5)\n",
            "Early stopping at episode 60 (no improvement for 5 checks)\n",
            "Training completed.\n",
            "Evaluating model...\n",
            "Best simulation metrics:\n",
            "  Final portfolio value: 0.6955\n",
            "  Return: -30.45%\n",
            "  Annualized Sharpe ratio: -0.8419\n",
            "  Maximum drawdown: 45.44%\n",
            "\n",
            "Evaluation completed.\n",
            "\n",
            "Starting experiment 2\n",
            "[Ep 10/200]  val=-0.3785  best=-inf epsilon=0.951\n",
            "  ↳ new best! checkpointing ensemble\n",
            "[Ep 20/200]  val=-1.7402  best=-0.3785 epsilon=0.905\n",
            "  ↳ no improvement over last (1/5)\n",
            "[Ep 30/200]  val=-1.7481  best=-0.3785 epsilon=0.860\n",
            "  ↳ no improvement over last (2/5)\n",
            "[Ep 40/200]  val=-1.4437  best=-0.3785 epsilon=0.818\n",
            "  ↳ no improvement over last (3/5)\n",
            "[Ep 50/200]  val=-1.5267  best=-0.3785 epsilon=0.778\n",
            "  ↳ no improvement over last (4/5)\n",
            "[Ep 60/200]  val=-1.7294  best=-0.3785 epsilon=0.740\n",
            "  ↳ no improvement over last (5/5)\n",
            "Early stopping at episode 60 (no improvement for 5 checks)\n",
            "Training completed.\n",
            "Evaluating model...\n",
            "Best simulation metrics:\n",
            "  Final portfolio value: 0.1669\n",
            "  Return: -83.31%\n",
            "  Annualized Sharpe ratio: -1.0852\n",
            "  Maximum drawdown: 89.71%\n",
            "\n",
            "Evaluation completed.\n",
            "\n",
            "Starting experiment 3\n",
            "[Ep 10/200]  val=-1.3441  best=-inf epsilon=0.951\n",
            "  ↳ new best! checkpointing ensemble\n"
          ]
        },
        {
          "ename": "KeyboardInterrupt",
          "evalue": "",
          "output_type": "error",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
            "Cell \u001b[0;32mIn[224], line 26\u001b[0m\n\u001b[1;32m     23\u001b[0m         \u001b[38;5;28mprint\u001b[39m() \n\u001b[1;32m     24\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m trained_models, results\n\u001b[0;32m---> 26\u001b[0m models, results \u001b[38;5;241m=\u001b[39m \u001b[43mrun_experiments\u001b[49m\u001b[43m(\u001b[49m\u001b[43mn_experiments\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m10\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mepisodes\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m200\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[1;32m     28\u001b[0m results[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mbaseline_value\u001b[39m\u001b[38;5;124m'\u001b[39m], results[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mbaseline_return\u001b[39m\u001b[38;5;124m'\u001b[39m] \u001b[38;5;241m=\u001b[39m evaluate_baseline(test_prices)\n\u001b[1;32m     30\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mFinal evaluation results:\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
            "Cell \u001b[0;32mIn[224], line 16\u001b[0m, in \u001b[0;36mrun_experiments\u001b[0;34m(n_experiments, **train_params)\u001b[0m\n\u001b[1;32m     14\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m i \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(n_experiments):\n\u001b[1;32m     15\u001b[0m     \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mStarting experiment\u001b[39m\u001b[38;5;124m\"\u001b[39m, i\u001b[38;5;241m+\u001b[39m\u001b[38;5;241m1\u001b[39m)\n\u001b[0;32m---> 16\u001b[0m     trained_ensemble \u001b[38;5;241m=\u001b[39m \u001b[43mtrain_agent\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mtrain_params\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     17\u001b[0m     trained_models\u001b[38;5;241m.\u001b[39mappend(trained_ensemble)\n\u001b[1;32m     18\u001b[0m     \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mEvaluating model...\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
            "Cell \u001b[0;32mIn[221], line 100\u001b[0m, in \u001b[0;36mtrain_agent\u001b[0;34m(episodes, replay_capacity, batch_size, gamma, learning_rate, epsilon_start, epsilon_min, epsilon_decay, target_update_freq, n_step_return, val_interval, max_patience, initial_state)\u001b[0m\n\u001b[1;32m     98\u001b[0m         ensemble_optimizer\u001b[38;5;241m.\u001b[39mzero_grad()\n\u001b[1;32m     99\u001b[0m         loss\u001b[38;5;241m.\u001b[39mbackward()\n\u001b[0;32m--> 100\u001b[0m         \u001b[43mensemble_optimizer\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mstep\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    102\u001b[0m \u001b[38;5;66;03m# Decay epsilon and temprature after each episode\u001b[39;00m\n\u001b[1;32m    103\u001b[0m epsilon \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mmax\u001b[39m(epsilon \u001b[38;5;241m*\u001b[39m epsilon_decay, epsilon_min)\n",
            "File \u001b[0;32m~/Library/Python/3.9/lib/python/site-packages/torch/optim/optimizer.py:493\u001b[0m, in \u001b[0;36mOptimizer.profile_hook_step.<locals>.wrapper\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    488\u001b[0m         \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    489\u001b[0m             \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mRuntimeError\u001b[39;00m(\n\u001b[1;32m    490\u001b[0m                 \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mfunc\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m must return None or a tuple of (new_args, new_kwargs), but got \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mresult\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    491\u001b[0m             )\n\u001b[0;32m--> 493\u001b[0m out \u001b[38;5;241m=\u001b[39m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    494\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_optimizer_step_code()\n\u001b[1;32m    496\u001b[0m \u001b[38;5;66;03m# call optimizer step post hooks\u001b[39;00m\n",
            "File \u001b[0;32m~/Library/Python/3.9/lib/python/site-packages/torch/optim/optimizer.py:91\u001b[0m, in \u001b[0;36m_use_grad_for_differentiable.<locals>._use_grad\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m     89\u001b[0m     torch\u001b[38;5;241m.\u001b[39mset_grad_enabled(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdefaults[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mdifferentiable\u001b[39m\u001b[38;5;124m\"\u001b[39m])\n\u001b[1;32m     90\u001b[0m     torch\u001b[38;5;241m.\u001b[39m_dynamo\u001b[38;5;241m.\u001b[39mgraph_break()\n\u001b[0;32m---> 91\u001b[0m     ret \u001b[38;5;241m=\u001b[39m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     92\u001b[0m \u001b[38;5;28;01mfinally\u001b[39;00m:\n\u001b[1;32m     93\u001b[0m     torch\u001b[38;5;241m.\u001b[39m_dynamo\u001b[38;5;241m.\u001b[39mgraph_break()\n",
            "File \u001b[0;32m~/Library/Python/3.9/lib/python/site-packages/torch/optim/adam.py:244\u001b[0m, in \u001b[0;36mAdam.step\u001b[0;34m(self, closure)\u001b[0m\n\u001b[1;32m    232\u001b[0m     beta1, beta2 \u001b[38;5;241m=\u001b[39m group[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mbetas\u001b[39m\u001b[38;5;124m\"\u001b[39m]\n\u001b[1;32m    234\u001b[0m     has_complex \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_init_group(\n\u001b[1;32m    235\u001b[0m         group,\n\u001b[1;32m    236\u001b[0m         params_with_grad,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    241\u001b[0m         state_steps,\n\u001b[1;32m    242\u001b[0m     )\n\u001b[0;32m--> 244\u001b[0m     \u001b[43madam\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    245\u001b[0m \u001b[43m        \u001b[49m\u001b[43mparams_with_grad\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    246\u001b[0m \u001b[43m        \u001b[49m\u001b[43mgrads\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    247\u001b[0m \u001b[43m        \u001b[49m\u001b[43mexp_avgs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    248\u001b[0m \u001b[43m        \u001b[49m\u001b[43mexp_avg_sqs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    249\u001b[0m \u001b[43m        \u001b[49m\u001b[43mmax_exp_avg_sqs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    250\u001b[0m \u001b[43m        \u001b[49m\u001b[43mstate_steps\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    251\u001b[0m \u001b[43m        \u001b[49m\u001b[43mamsgrad\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mgroup\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mamsgrad\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    252\u001b[0m \u001b[43m        \u001b[49m\u001b[43mhas_complex\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mhas_complex\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    253\u001b[0m \u001b[43m        \u001b[49m\u001b[43mbeta1\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mbeta1\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    254\u001b[0m \u001b[43m        \u001b[49m\u001b[43mbeta2\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mbeta2\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    255\u001b[0m \u001b[43m        \u001b[49m\u001b[43mlr\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mgroup\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mlr\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    256\u001b[0m \u001b[43m        \u001b[49m\u001b[43mweight_decay\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mgroup\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mweight_decay\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    257\u001b[0m \u001b[43m        \u001b[49m\u001b[43meps\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mgroup\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43meps\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    258\u001b[0m \u001b[43m        \u001b[49m\u001b[43mmaximize\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mgroup\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mmaximize\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    259\u001b[0m \u001b[43m        \u001b[49m\u001b[43mforeach\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mgroup\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mforeach\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    260\u001b[0m \u001b[43m        \u001b[49m\u001b[43mcapturable\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mgroup\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mcapturable\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    261\u001b[0m \u001b[43m        \u001b[49m\u001b[43mdifferentiable\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mgroup\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mdifferentiable\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    262\u001b[0m \u001b[43m        \u001b[49m\u001b[43mfused\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mgroup\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mfused\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    263\u001b[0m \u001b[43m        \u001b[49m\u001b[43mgrad_scale\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mgetattr\u001b[39;49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mgrad_scale\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    264\u001b[0m \u001b[43m        \u001b[49m\u001b[43mfound_inf\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mgetattr\u001b[39;49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mfound_inf\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    265\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    267\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m loss\n",
            "File \u001b[0;32m~/Library/Python/3.9/lib/python/site-packages/torch/optim/optimizer.py:154\u001b[0m, in \u001b[0;36m_disable_dynamo_if_unsupported.<locals>.wrapper.<locals>.maybe_fallback\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    152\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m disabled_func(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[1;32m    153\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m--> 154\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
            "File \u001b[0;32m~/Library/Python/3.9/lib/python/site-packages/torch/optim/adam.py:876\u001b[0m, in \u001b[0;36madam\u001b[0;34m(params, grads, exp_avgs, exp_avg_sqs, max_exp_avg_sqs, state_steps, foreach, capturable, differentiable, fused, grad_scale, found_inf, has_complex, amsgrad, beta1, beta2, lr, weight_decay, eps, maximize)\u001b[0m\n\u001b[1;32m    873\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    874\u001b[0m     func \u001b[38;5;241m=\u001b[39m _single_tensor_adam\n\u001b[0;32m--> 876\u001b[0m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    877\u001b[0m \u001b[43m    \u001b[49m\u001b[43mparams\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    878\u001b[0m \u001b[43m    \u001b[49m\u001b[43mgrads\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    879\u001b[0m \u001b[43m    \u001b[49m\u001b[43mexp_avgs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    880\u001b[0m \u001b[43m    \u001b[49m\u001b[43mexp_avg_sqs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    881\u001b[0m \u001b[43m    \u001b[49m\u001b[43mmax_exp_avg_sqs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    882\u001b[0m \u001b[43m    \u001b[49m\u001b[43mstate_steps\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    883\u001b[0m \u001b[43m    \u001b[49m\u001b[43mamsgrad\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mamsgrad\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    884\u001b[0m \u001b[43m    \u001b[49m\u001b[43mhas_complex\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mhas_complex\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    885\u001b[0m \u001b[43m    \u001b[49m\u001b[43mbeta1\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mbeta1\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    886\u001b[0m \u001b[43m    \u001b[49m\u001b[43mbeta2\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mbeta2\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    887\u001b[0m \u001b[43m    \u001b[49m\u001b[43mlr\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mlr\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    888\u001b[0m \u001b[43m    \u001b[49m\u001b[43mweight_decay\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mweight_decay\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    889\u001b[0m \u001b[43m    \u001b[49m\u001b[43meps\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43meps\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    890\u001b[0m \u001b[43m    \u001b[49m\u001b[43mmaximize\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mmaximize\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    891\u001b[0m \u001b[43m    \u001b[49m\u001b[43mcapturable\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcapturable\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    892\u001b[0m \u001b[43m    \u001b[49m\u001b[43mdifferentiable\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdifferentiable\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    893\u001b[0m \u001b[43m    \u001b[49m\u001b[43mgrad_scale\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mgrad_scale\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    894\u001b[0m \u001b[43m    \u001b[49m\u001b[43mfound_inf\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mfound_inf\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    895\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
            "File \u001b[0;32m~/Library/Python/3.9/lib/python/site-packages/torch/optim/adam.py:478\u001b[0m, in \u001b[0;36m_single_tensor_adam\u001b[0;34m(params, grads, exp_avgs, exp_avg_sqs, max_exp_avg_sqs, state_steps, grad_scale, found_inf, amsgrad, has_complex, beta1, beta2, lr, weight_decay, eps, maximize, capturable, differentiable)\u001b[0m\n\u001b[1;32m    475\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    476\u001b[0m         denom \u001b[38;5;241m=\u001b[39m (exp_avg_sq\u001b[38;5;241m.\u001b[39msqrt() \u001b[38;5;241m/\u001b[39m bias_correction2_sqrt)\u001b[38;5;241m.\u001b[39madd_(eps)\n\u001b[0;32m--> 478\u001b[0m     \u001b[43mparam\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43maddcdiv_\u001b[49m\u001b[43m(\u001b[49m\u001b[43mexp_avg\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdenom\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mvalue\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m-\u001b[39;49m\u001b[43mstep_size\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    480\u001b[0m \u001b[38;5;66;03m# Lastly, switch back to complex view\u001b[39;00m\n\u001b[1;32m    481\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m amsgrad \u001b[38;5;129;01mand\u001b[39;00m torch\u001b[38;5;241m.\u001b[39mis_complex(params[i]):\n",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
          ]
        }
      ],
      "source": [
        "# Run evaluation on the test prices using the ensemble Q-value network function (or your model)\n",
        "def run_experiments(n_experiments=5, **train_params):\n",
        "    \"\"\"\n",
        "    Runs the entire training process for n experiments and prints a summary.\n",
        "    Returns a list of (trained_ensemble, best_val_reward) tuples from each experiment.\n",
        "    \"\"\"\n",
        "    trained_models = []\n",
        "    results = {\n",
        "        'final_portfolio_value': -np.inf,\n",
        "        'final_return': 0.0,\n",
        "        'sharpe_ratio': 0.0,\n",
        "        'max_drawdown': 0.0,\n",
        "    }\n",
        "    for i in range(n_experiments):\n",
        "        print(\"Starting experiment\", i+1)\n",
        "        trained_ensemble = train_agent(**train_params)\n",
        "        trained_models.append(trained_ensemble)\n",
        "        print(\"Evaluating model...\")\n",
        "        agent_values, agent_metrics, all_run_metrics = evaluate_policy(test_prices, trained_ensemble, initial_state, 1)\n",
        "        if results['final_portfolio_value'] < agent_metrics['final_portfolio_value']:\n",
        "            results = agent_metrics\n",
        "        print(\"Evaluation completed.\")\n",
        "        print() \n",
        "    return trained_models, results\n",
        "\n",
        "models, results = run_experiments(n_experiments=10, episodes=200)\n",
        "    \n",
        "results['baseline_value'], results['baseline_return'] = evaluate_baseline(test_prices)\n",
        "\n",
        "print(\"Final evaluation results:\")\n",
        "print(f\"    Final portfolio value: {results['final_portfolio_value']:.4f}\")\n",
        "print(f\"    Final return: {results['final_return']:.2f}%\")\n",
        "print(f\"    Annualized Sharpe ratio: {results['sharpe_ratio']:.4f}\")\n",
        "print(f\"    Maximum drawdown: {results['max_drawdown']:.2%}\")\n",
        "print()\n",
        "print(f\"Baseline portfolio value: {results['baseline_value']:.4f}\")\n",
        "print(f\"  Return: {results['baseline_return']:.2f}%\")"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "T4",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.9.6"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
