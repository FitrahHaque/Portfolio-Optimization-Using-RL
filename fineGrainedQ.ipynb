{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 210,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "_Y77BeglBh0B",
        "outputId": "e96b675b-fabe-4d82-947e-bccea7072beb"
      },
      "outputs": [],
      "source": [
        "# from google.colab import drive\n",
        "# drive.mount('/content/drive')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 211,
      "metadata": {
        "id": "ygKC_nPqrODL"
      },
      "outputs": [],
      "source": [
        "import math\n",
        "import random\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "import torch.optim as optim\n",
        "import yfinance as yf\n",
        "from collections import deque\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "WaT-xj4XrdOK"
      },
      "source": [
        "### Data Preparation"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 212,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "9ILxQXy4rY-R",
        "outputId": "77615a20-66fc-412f-dfc7-bed4f770d3c6"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Loaded data from prices.csv\n",
            "Training days: 1323, Testing days: 250\n"
          ]
        }
      ],
      "source": [
        "# Define the 10 assets (tickers) for the portfolio\n",
        "# tickers = [\"AAPL\", \"MSFT\", \"GOOGL\", \"AMZN\", \"TSLA\", \"BA\", \"NFLX\", \"NVDA\", \"META\", \"SBUX\"]\n",
        "\n",
        "# tickers = [\"AAPL\", \"MSFT\", \"GOOGL\", \"SBUX\", \"TSLA\"]\n",
        "tickers = [\"GME\", \"AMC\", \"SPCE\", \"NVAX\", \"NOK\"]\n",
        "# tickers = [\"GME\", \"AMC\", \"BB\", \"NVAX\", \"NOK\"]\n",
        "# tickers = [\"GME\", \"AMC\", \"HMC\", \"NVAX\", \"NOK\"]\n",
        "# Date range for historical data\n",
        "start_date = \"2017-01-01\"\n",
        "end_date   = \"2023-12-31\"\n",
        "\n",
        "# Try to load price data from a local CSV, otherwise download using yfinance\n",
        "data_file = \"prices.csv\"\n",
        "try:\n",
        "    prices_df = pd.read_csv(data_file, index_col=0, parse_dates=True)\n",
        "    print(\"Loaded data from\", data_file)\n",
        "except FileNotFoundError:\n",
        "    print(\"Downloading price data for tickers:\", tickers)\n",
        "    df = yf.download(tickers, start=start_date, end=end_date, interval=\"1d\")\n",
        "    # Extract the 'Close' prices from the MultiIndex DataFrame\n",
        "    prices_df = df.xs('Close', axis=1, level='Price')\n",
        "    prices_df.dropna(inplace=True)\n",
        "    prices_df.to_csv(data_file)\n",
        "    print(\"Data downloaded and saved to\", data_file)\n",
        "\n",
        "# Split data into training (first 4 years) and testing (last year)\n",
        "full_train_df = prices_df[prices_df.index < \"2023-01-01\"]\n",
        "test_df  = prices_df[prices_df.index >= \"2023-01-01\"]\n",
        "full_train_prices = full_train_df.values  # shape: [train_days, 5]\n",
        "test_prices  = test_df.values   # shape: [test_days, 5]\n",
        "num_assets = full_train_prices.shape[1]\n",
        "print(f\"Training days: {full_train_prices.shape[0]}, Testing days: {test_prices.shape[0]}\")\n",
        "\n",
        "# Further split full training into training and validation (80/20)\n",
        "split_index = int(0.8 * full_train_prices.shape[0])\n",
        "train_prices = full_train_prices[:split_index]\n",
        "val_prices   = full_train_prices[split_index:]"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "###  Technical Indicators & State Representation"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 213,
      "metadata": {},
      "outputs": [],
      "source": [
        "def get_market_indicators(price_array, t, window=10):\n",
        "    \"\"\"\n",
        "    Compute technical indicators for each asset at time t:\n",
        "      - Moving Average Ratio: current price / moving average over window\n",
        "      - Volatility: std dev of daily returns over window (normalized by current price)\n",
        "    If not enough history exists, use current price and low volatility.\n",
        "    Returns a list of 2 values per asset.\n",
        "    \"\"\"\n",
        "    indicators = []\n",
        "    for asset in range(num_assets):\n",
        "        if t < window:\n",
        "            # Not enough history: use current price and set volatility to 0\n",
        "            moving_avg_ratio = 1.0\n",
        "            vol = 0.0\n",
        "        else:\n",
        "            window_prices = price_array[t-window+1:t+1, asset]\n",
        "            moving_avg = np.mean(window_prices)\n",
        "            moving_avg_ratio = price_array[t, asset] / (moving_avg + 1e-6)\n",
        "            returns = np.diff(window_prices) / (window_prices[:-1] + 1e-6)\n",
        "            vol = np.std(returns) / (price_array[t, asset] + 1e-6)\n",
        "        indicators.extend([moving_avg_ratio, vol])\n",
        "    return indicators\n",
        "\n",
        "def get_state(t, alloc_state, price_array):\n",
        "    \"\"\"\n",
        "    Constructs the state vector at time t by combining:\n",
        "      - Portfolio allocations (6 numbers: 5 assets + cash) normalized to [0,1]\n",
        "      - Market indicators for assets (2 per asset)\n",
        "    Returns a numpy array of length 6 + 10 = 16.\n",
        "    \"\"\"\n",
        "    # Normalize allocation percentages (they sum to 100)\n",
        "    alloc_norm = [a / 100.0 for a in alloc_state]\n",
        "    indicators = get_market_indicators(price_array, t)\n",
        "    return np.array(alloc_norm + indicators)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "uEJnt93hrlGz"
      },
      "source": [
        "### State Encoding/Decoding and Actions"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 214,
      "metadata": {
        "id": "0191obe4rd9I"
      },
      "outputs": [],
      "source": [
        "# Construct the new action space.\n",
        "# Each action is a tuple (src, dst, amt) where:\n",
        "# - src and dst are indices in assets_plus_cash (0..5)\n",
        "# - amt is in [1, 2, 3, 4, 5] representing a 1%-5% transfer.\n",
        "# We allow transferring funds between any two different assets.\n",
        "\n",
        "# Define all possible actions (including no-action) as tuples\n",
        "all_actions = []\n",
        "cash_idx = num_assets            # index 5 is for cash\n",
        "assets_plus_cash = list(range(num_assets)) + [cash_idx]  # [0,1,2,3,4,5]\n",
        "percents = [1,2,3,4,5]\n",
        "for pct in percents:\n",
        "    for src in assets_plus_cash:\n",
        "        for dst in assets_plus_cash:\n",
        "            if src != dst:\n",
        "                all_actions.append((src, dst, pct))\n",
        "all_actions.append((None, None, 0))  # No action\n",
        "action_count = len(all_actions) # 151\n",
        "# print(\"Number of actions:\", action_count)\n",
        "# Create a mapping from action tuple to index in all_actions list\n",
        "action_to_index = {act: idx for idx, act in enumerate(all_actions)}\n",
        "\n",
        "def get_valid_actions(state):\n",
        "    \"\"\"\n",
        "    Given a state (tuple of 6 integers summing to 100 representing allocations for 5 stocks and cash),\n",
        "    return a list of valid actions (from all_actions_new).\n",
        "    \n",
        "    Rules:\n",
        "      - If src is not cash, state[src] must be >= amt (i.e. you must have enough allocation to sell).\n",
        "      - Optionally, we can restrict that if dst is not cash, the allocation after transfer should not exceed 100.\n",
        "        (Here we assume no single asset can have more than 100%).\n",
        "      - The no-action (None, None, 0) is always valid.\n",
        "    \"\"\"\n",
        "    valid = []\n",
        "    for act in all_actions:\n",
        "        if act == (None, None, 0):\n",
        "            valid.append(act)\n",
        "        else:\n",
        "            src, dst, amt = act\n",
        "            # Check: if src is not cash, then it must have at least 'amt'\n",
        "            if src != cash_idx and state[src] < amt:\n",
        "                continue\n",
        "            # Check: if dst is not cash, ensure it does not exceed 100.\n",
        "            if dst != cash_idx and state[dst] + amt > 100:\n",
        "                continue\n",
        "            valid.append(act)\n",
        "    return valid\n",
        "\n",
        "\n",
        "def apply_action(state, action):\n",
        "    \"\"\"\n",
        "    Apply an action (src, dst, amt) to the state.\n",
        "    - state: tuple of 6 integers (allocations in % for 5 stocks and cash; sum to 100)\n",
        "    - action: (src, dst, amt). For example, (0,5,3) means transfer 3% from asset 0 to cash.\n",
        "      (None, None, 0) means no action.\n",
        "    Returns the new state as a tuple of 6 integers.\n",
        "    \"\"\"\n",
        "    state = list(state)\n",
        "    if action == (None, None, 0):\n",
        "        return tuple(state)\n",
        "    src, dst, amt = action\n",
        "    # If src is not cash, reduce its allocation by amt (clip at 0)\n",
        "    if src is not None and state[src] >= amt:\n",
        "        state[src] = state[src] - amt\n",
        "    # If dst is not cash, increase its allocation by amt (clip at 100)\n",
        "    if dst is not None and dst != cash_idx and state[dst] + amt <= 100:\n",
        "        state[dst] = state[dst] + amt\n",
        "    # If dst is cash, add amt to cash.\n",
        "    if dst == cash_idx:\n",
        "        state[cash_idx] = state[cash_idx] + amt\n",
        "    # Optionally, enforce that the new state's sum remains 100. Here, if clipping occurred,\n",
        "    # you might choose to normalize or simply allow slight deviations.\n",
        "    # For now, assume actions are valid and state remains valid.\n",
        "    return tuple(state)\n",
        "\n",
        "# def compute_reward(weights_frac, price_today, price_next):\n",
        "#     \"\"\"\n",
        "#     Compute the log return of the portfolio for one time step.\n",
        "#     - weights_frac: list of 5 asset weight fractions after rebalancing on day t (sum=1).\n",
        "#     - price_today: prices of the 5 assets at day t.\n",
        "#     - price_next: prices of the 5 assets at day t+1.\n",
        "#     Returns: log(portfolio return) from day t to t+1.\n",
        "#     \"\"\"\n",
        "#     # Portfolio value growth factor = sum_k w_k * (price_next_k / price_today_k)\n",
        "#     growth_factor = 0.0\n",
        "#     for k in range(len(assets_plus_cash)):\n",
        "#         if k == cash_idx:\n",
        "#             ratio = 1.0\n",
        "#         else:\n",
        "#             ratio = price_next[k] / price_today[k]\n",
        "#         growth_factor += weights_frac[k] * ratio\n",
        "#     # Reward is the log of the growth factor\n",
        "#     return math.log(growth_factor)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qMipXOQPiNWW"
      },
      "source": [
        "### Reward Shaping using a Rolling Sharpe Ratio"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 215,
      "metadata": {
        "id": "7WYWx-jViOO0"
      },
      "outputs": [],
      "source": [
        "class SharpeRewardShaper:\n",
        "    def __init__(self, window=30, epsilon=1e-6):\n",
        "        self.window = window\n",
        "        self.rewards_history = []\n",
        "        self.epsilon = epsilon\n",
        "        self.portfolio_max = -np.inf\n",
        "\n",
        "    def shape(self, raw_reward, current_portfolio):\n",
        "        \"\"\"\n",
        "        Enhances reward by:\n",
        "          - Computing rolling Sharpe ratio,\n",
        "          - Penalizing drawdowns (if current_portfolio is lower than past peak),\n",
        "          - Penalizing high volatility.\n",
        "        \"\"\"\n",
        "        self.rewards_history.append(raw_reward)\n",
        "        if len(self.rewards_history) > self.window:\n",
        "            self.rewards_history.pop(0)\n",
        "        avg_reward = np.mean(self.rewards_history)\n",
        "        std_reward = np.std(self.rewards_history) + self.epsilon\n",
        "        sharpe = avg_reward / std_reward\n",
        "\n",
        "        # Update running maximum portfolio value\n",
        "        self.portfolio_max = max(self.portfolio_max, current_portfolio)\n",
        "        drawdown = (self.portfolio_max - current_portfolio) / (self.portfolio_max + self.epsilon)\n",
        "        drawdown_penalty = drawdown * 0.5  # factor can be tuned\n",
        "\n",
        "        # Volatility penalty: higher volatility (std reward) reduces reward\n",
        "        vol_penalty = std_reward * 0.5  # factor can be tuned\n",
        "\n",
        "        # Final shaped reward: encourage long term gain via n-step (raw_reward already multi-step) + penalize risks\n",
        "        shaped_reward = sharpe - drawdown_penalty - vol_penalty\n",
        "\n",
        "        return shaped_reward\n",
        "\n",
        "def compute_reward(weights_frac, price_array, t, n_step=3):\n",
        "    \"\"\"\n",
        "    Compute the reward using n-step return from time t to t+n_step.\n",
        "    Growth is computed by taking the weighted sum of asset returns over n_step days.\n",
        "    If t+n_step is beyond available data, use last available day.\n",
        "    \"\"\"\n",
        "    last_index = min(t + n_step, price_array.shape[0] - 1)\n",
        "    growth_factor = 0.0\n",
        "    for k in range(len(assets_plus_cash)):\n",
        "        # For cash, return is 1.0; for assets use price ratio over n steps.\n",
        "        if k == cash_idx:\n",
        "            ratio = 1.0\n",
        "        else:\n",
        "            ratio = price_array[last_index, k] / price_array[t, k]\n",
        "        growth_factor += weights_frac[k] * ratio\n",
        "    # raw reward is log of multi-step growth factor\n",
        "    return math.log(growth_factor+1e-6)\n",
        "\n",
        "reward_shaper = SharpeRewardShaper(window=30)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Uqa6xKTSrqzp"
      },
      "source": [
        "### Replay Buffer for Experience Replay"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 216,
      "metadata": {
        "id": "88rVfnT-rnrp"
      },
      "outputs": [],
      "source": [
        "class ReplayBuffer:\n",
        "    def __init__(self, capacity):\n",
        "        \"\"\"\n",
        "        Replay buffer to store past transitions for experience replay.\n",
        "        Stores tuples of (state, action_index, reward, next_state, done).\n",
        "        \"\"\"\n",
        "        self.capacity = capacity\n",
        "        self.buffer = deque(maxlen=capacity)\n",
        "\n",
        "    def push(self, state, action_idx, reward, next_state, done):\n",
        "        \"\"\"Save a transition to the buffer.\"\"\"\n",
        "        self.buffer.append((state, action_idx, reward, next_state, done))\n",
        "\n",
        "    def sample(self, batch_size):\n",
        "        \"\"\"\n",
        "        Sample a random batch of transitions from the buffer.\n",
        "        Returns: tuples (states, action_idxs, rewards, next_states, dones) for the batch.\n",
        "        \"\"\"\n",
        "        batch = random.sample(self.buffer, batch_size)\n",
        "        # Extract each component into separate lists\n",
        "        states, action_idxs, rewards, next_states, dones = zip(*batch)\n",
        "        return list(states), list(action_idxs), list(rewards), list(next_states), list(dones)\n",
        "\n",
        "    def __len__(self):\n",
        "        \"\"\"Current size of the buffer.\"\"\"\n",
        "        return len(self.buffer)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-z0AwXHTrwmz"
      },
      "source": [
        "### Neural Network for Q-value Function"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 217,
      "metadata": {
        "id": "j6RJgJrfrtDJ"
      },
      "outputs": [],
      "source": [
        "class DQNNetwork(nn.Module):\n",
        "    def __init__(self, state_dim, action_dim):\n",
        "        \"\"\"\n",
        "        Neural network that approximates Q(s,a) for all actions a given state s.\n",
        "        state_dim: dimensionality of state input (e.g. 5)\n",
        "        action_dim: number of possible actions (e.g. 21)\n",
        "        \"\"\"\n",
        "        super(DQNNetwork, self).__init__()\n",
        "\n",
        "        # self.fc1 = nn.Linear(state_dim, 128)\n",
        "        # self.bn1 = nn.BatchNorm1d(128)\n",
        "        # self.dropout1 = nn.Dropout(0.3)\n",
        "        # self.fc2 = nn.Linear(128, 256)\n",
        "        # self.bn2 = nn.BatchNorm1d(256)\n",
        "        # self.dropout2 = nn.Dropout(0.3)\n",
        "        # self.fc3 = nn.Linear(256, 128)\n",
        "        # self.fc4 = nn.Linear(128, action_dim)\n",
        "\n",
        "        # hidden1 = 128\n",
        "        # hidden2 = 128\n",
        "        # self.fc1 = nn.Linear(state_dim, hidden1)\n",
        "        # self.fc2 = nn.Linear(hidden1, hidden2)\n",
        "        # self.fc3 = nn.Linear(hidden2, action_dim)\n",
        "\n",
        "        self.fc1 = nn.Linear(state_dim, 128)\n",
        "        self.ln1 = nn.LayerNorm(128)\n",
        "        self.dropout1 = nn.Dropout(0.3)\n",
        "        self.fc2 = nn.Linear(128, 256)\n",
        "        self.ln2 = nn.LayerNorm(256)\n",
        "        self.dropout2 = nn.Dropout(0.3)\n",
        "        self.fc3 = nn.Linear(256, 128)\n",
        "        self.fc4 = nn.Linear(128, action_dim)\n",
        "\n",
        "    def forward(self, x):\n",
        "        # x is a tensor of shape [batch_size, state_dim]\n",
        "\n",
        "        # x = F.relu(self.bn1(self.fc1(x)))\n",
        "        # x = self.dropout1(x)\n",
        "        # x = F.relu(self.bn2(self.fc2(x)))\n",
        "        # x = self.dropout2(x)\n",
        "        # x = F.relu(self.fc3(x))\n",
        "        # q_vals = self.fc4(x)\n",
        "\n",
        "        # x = F.relu(self.fc1(x))\n",
        "        # x = F.relu(self.fc2(x))\n",
        "        # q_values = self.fc3(x)  # outputs Q-values for each action\n",
        "\n",
        "        x = F.relu(self.ln1(self.fc1(x)))\n",
        "        x = self.dropout1(x)\n",
        "        x = F.relu(self.ln2(self.fc2(x)))\n",
        "        x = self.dropout2(x)\n",
        "        x = F.relu(self.fc3(x))\n",
        "        q_vals = self.fc4(x)\n",
        "\n",
        "        return q_vals"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8DkUKJ6zr0X7"
      },
      "source": [
        "### DQN Agent Training Setup"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "4FEtbjn1r0v7",
        "outputId": "aeb4afb6-650e-4ba8-903b-54dec9397ab2"
      },
      "outputs": [],
      "source": [
        "# Hyperparameters\n",
        "# gamma = 0.99            # discount factor for future rewards\n",
        "# learning_rate = 5e-4  # learning rate for optimizer\n",
        "# epsilon_start = 1.0     # initial exploration rate\n",
        "# epsilon_min   = 0.2     # minimum exploration rate\n",
        "# epsilon_decay = 0.995    # multiplicative decay factor per episode\n",
        "# episodes = 150          # number of training episodes\n",
        "# batch_size = 128         # mini-batch size for replay updates\n",
        "# target_update_freq = 5 # how often (in episodes) to update the target network\n",
        "# replay_capacity = 10000 # capacity of the replay buffer\n",
        "use_double_dqn = True  # use Double DQN for      \n",
        "state_dim = num_assets + 1 + num_assets * 2  # 6 (allocations) + 10 (2 indicators per each of the 5 assets) = 16\n",
        "# ensemble_size = 2\n",
        "# temperature = 1.0\n",
        "initial_state=(15, 15, 15, 15, 15, 25)\n",
        "# Initialize replay memory, policy network, target network, optimizer\n",
        "# prioritized_replay_buffer = PrioritizedReplayBuffer(replay_capacity)\n",
        "\n",
        "# policy_net = DQNNetwork(state_dim, action_dim)\n",
        "# target_net = DQNNetwork(state_dim, action_dim)\n",
        "# Copy initial weights to target network\n",
        "# target_net.load_state_dict(policy_net.state_dict())\n",
        "# target_net.eval()  # target network in evaluation mode (not strictly necessary)\n",
        "# optimizer = optim.Adam(policy_net.parameters(), lr=learning_rate)\n",
        "\n",
        "\n",
        "# # Create ensemble of networks and their corresponding target networks\n",
        "# ensemble_nets = [DQNNetwork(state_dim, action_count) for _ in range(ensemble_size)]\n",
        "# ensemble_targets = [DQNNetwork(state_dim, action_count) for _ in range(ensemble_size)]\n",
        "# for net, target in zip(ensemble_nets, ensemble_targets):\n",
        "#     target.load_state_dict(net.state_dict())\n",
        "#     target.eval()\n",
        "# # Combine parameters of all ensemble networks in one optimizer\n",
        "# ensemble_optimizer = optim.Adam([p for net in ensemble_nets for p in net.parameters()], lr=learning_rate)\n",
        "\n",
        "# For action selection and training, we take the average Q-values across ensemble members.\n",
        "def ensemble_q_values(state_input, nets):\n",
        "    # Temporarily store the training state of each network.\n",
        "    original_modes = [net.training for net in nets]\n",
        "\n",
        "    # Switch networks to eval mode for inference (to avoid BN issues with batch size 1)\n",
        "    for net in nets:\n",
        "        net.eval()\n",
        "\n",
        "    # Compute Q-values from each network and average them\n",
        "    q_vals_list = [net(state_input) for net in nets]  # shape: [ensemble_size, batch, action_dim]\n",
        "    avg_q_vals = torch.stack(q_vals_list, dim=0).mean(dim=0)\n",
        "\n",
        "    # Restore the original training mode of each network\n",
        "    for net, mode in zip(nets, original_modes):\n",
        "        if mode:\n",
        "            net.train()\n",
        "        else:\n",
        "            net.eval()\n",
        "\n",
        "    return avg_q_vals\n",
        "\n",
        "# Helper function: select action\n",
        "def select_action(state, t, train_prices, epsilon, nets, temperature = 1.0):\n",
        "    \"\"\"\n",
        "    Hybrid exploration:\n",
        "      - With probability epsilon, choose a random valid action.\n",
        "      - Otherwise, use Boltzmann sampling (softmax over Q-values for valid actions).\n",
        "    The state passed in is the portfolio allocation (tuple of 6 ints).\n",
        "    We combine it with market indicators computed from price_array at time t.\n",
        "    Returns: (action_idx, action_tuple)\n",
        "    \"\"\"\n",
        "    valid_actions = get_valid_actions(state)\n",
        "    if random.random() < epsilon:\n",
        "        # Exploration: random valid action\n",
        "        action = random.choice(valid_actions)\n",
        "    else:\n",
        "        # Exploitation: choose best action according to Q-network\n",
        "        # Normalize state: now state values are percentages out of 100\n",
        "        state_vec = get_state(t, state, train_prices)\n",
        "        state_tensor = torch.FloatTensor(state_vec).unsqueeze(0)\n",
        "        with torch.no_grad():\n",
        "          q_values = ensemble_q_values(state_tensor, nets)  # shape [1, action_count]\n",
        "          q_values = q_values.numpy().squeeze()  # shape [action_count]\n",
        "        # Mask out invalid actions by setting their Q-value very low\n",
        "        # (So they won't be chosen as max)\n",
        "        invalid_actions = set(all_actions) - set(valid_actions)\n",
        "        for act in invalid_actions:\n",
        "            # if act in action_to_index:\n",
        "            q_values[action_to_index[act]] = -1e9  # large negative to disable\n",
        "        # Boltzmann (softmax) sampling over valid Q-values\n",
        "        # exp_q = np.exp(q_values / temperature)\n",
        "        q_values = np.nan_to_num(q_values)  # replace NaN with 0\n",
        "        q_values -= q_values.max()  # for numerical stability\n",
        "        exp_q = np.exp(q_values / temperature)\n",
        "        # Zero out probabilities for invalid actions\n",
        "        for act in invalid_actions:\n",
        "            exp_q[action_to_index[act]] = 0.0\n",
        "        total = exp_q.sum()\n",
        "        if total <= 0 or np.isnan(total):\n",
        "            best_idx = int(np.nnanargmax(q_values))\n",
        "            return best_idx, all_actions[best_idx]\n",
        "        probs = exp_q / total\n",
        "        action_idx = np.random.choice(len(probs), p=probs)\n",
        "        action = all_actions[action_idx]\n",
        "\n",
        "    # Return both the index and the tuple representation\n",
        "    return action_to_index[action], action\n",
        "\n",
        "def evaluate_on_validation(val_prices, ensemble_nets, initial_state):\n",
        "    \"\"\"\n",
        "    Runs the greedy policy defined by ensemble_nets on val_prices,\n",
        "    returns the log of the final portfolio value (i.e. validation 'reward').\n",
        "    \"\"\"\n",
        "    val_portfolio = 1.0\n",
        "    state = initial_state\n",
        "\n",
        "    for t in range(val_prices.shape[0] - 1):\n",
        "        # build state + indicators\n",
        "        inp = get_state(t, state, val_prices)\n",
        "        inp_t = torch.FloatTensor(inp).unsqueeze(0)\n",
        "        with torch.no_grad():\n",
        "            q_vals = ensemble_q_values(inp_t, ensemble_nets).numpy().squeeze()\n",
        "\n",
        "        # mask invalid actions\n",
        "        valid = set(get_valid_actions(state))\n",
        "        for act in set(all_actions) - valid:\n",
        "            q_vals[action_to_index[act]] = -1e9\n",
        "\n",
        "        # take the greedy action\n",
        "        best_idx = int(np.argmax(q_vals))\n",
        "        state = apply_action(state, all_actions[best_idx])\n",
        "\n",
        "        # update portfolio\n",
        "        w = [x/100 for x in state]\n",
        "        growth = sum(\n",
        "            w[k] * (val_prices[t+1][k] / val_prices[t][k])\n",
        "            if k < num_assets else w[cash_idx]\n",
        "            for k in range(len(assets_plus_cash))\n",
        "        )\n",
        "        val_portfolio *= growth\n",
        "\n",
        "    return math.log(val_portfolio + 1e-6)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 219,
      "metadata": {},
      "outputs": [],
      "source": [
        "def train_agent(episodes=150, \n",
        "                replay_capacity=10000, \n",
        "                batch_size=128, \n",
        "                gamma=0.99, \n",
        "                learning_rate=5e-4, \n",
        "                epsilon_start=1.0, \n",
        "                epsilon_min=0.2, \n",
        "                epsilon_decay=0.995, \n",
        "                target_update_freq=5, \n",
        "                n_step_return=5, \n",
        "                val_interval=10,\n",
        "                max_patience=5,\n",
        "                initial_state=initial_state):\n",
        "    \"\"\"\n",
        "    Runs the training loop for one experiment.\n",
        "    Returns:\n",
        "       - ensemble_nets: the trained ensemble networks (list)\n",
        "       - best_val_reward: best validation reward achieved (float)\n",
        "       - train_metrics: (optional) dictionary containing additional training metrics\n",
        "    \"\"\"\n",
        "\n",
        "    # Reset initial state and replay buffer\n",
        "    replay_buffer = ReplayBuffer(replay_capacity)\n",
        "    epsilon = epsilon_start\n",
        "    train_days = train_prices.shape[0]\n",
        "    \n",
        "    best_val_reward = -np.inf\n",
        "    patience = 0\n",
        "    best_ensemble_nets = None\n",
        "    \n",
        "    # initialize ensemble networks and target networks for each experiment\n",
        "    ensemble_size = 2\n",
        "    ensemble_nets = [DQNNetwork(state_dim, action_count) for _ in range(ensemble_size)]\n",
        "    ensemble_targets = [DQNNetwork(state_dim, action_count) for _ in range(ensemble_size)]\n",
        "    for net, target in zip(ensemble_nets, ensemble_targets):\n",
        "        target.load_state_dict(net.state_dict())\n",
        "        target.eval()\n",
        "    ensemble_optimizer = optim.Adam([p for net in ensemble_nets for p in net.parameters()], lr=learning_rate)\n",
        "\n",
        "    # Training loop\n",
        "    for ep in range(1, episodes + 1):\n",
        "        state = initial_state\n",
        "        portfolio_value = 1.0  # start with $1.0\n",
        "\n",
        "        # Iterate over each day (using train_prices, market indicators available from train_prices)\n",
        "        for t in range(0, train_days - 1 - n_step_return):\n",
        "            # Hybrid action selection: uses current day t and train_prices for market indicators\n",
        "            action_idx, action = select_action(state, t, train_prices, epsilon, ensemble_nets)\n",
        "            new_state = apply_action(state, action)\n",
        "\n",
        "            # Compute reward using n-step return\n",
        "            weights_new = [x/100.0 for x in new_state]\n",
        "            raw_reward = compute_reward(weights_new, train_prices, t, n_step=n_step_return)\n",
        "            portfolio_value *= math.exp(raw_reward)  # simulate n-step portfolio growth\n",
        "\n",
        "            # Shape reward using Sharpe reward shaper (with drawdown & volatility penalty)\n",
        "            reward = reward_shaper.shape(raw_reward, portfolio_value)\n",
        "            done = (t >= train_days - n_step_return - 1)\n",
        "\n",
        "            # Construct state representations (augmenting with market indicators)\n",
        "            state_repr = get_state(t, state, train_prices)\n",
        "            next_state_repr = get_state(t + n_step_return, new_state, train_prices)\n",
        "            replay_buffer.push(state_repr, action_idx, reward, next_state_repr, done)\n",
        "\n",
        "            state = new_state\n",
        "\n",
        "            # Learning step\n",
        "            if len(replay_buffer) >= batch_size:\n",
        "                state_reprs_batch, actions_batch, rewards_batch, next_state_reprs_batch, dones_batch = replay_buffer.sample(batch_size)\n",
        "                state_vec_tensor = torch.FloatTensor(np.array(state_reprs_batch))\n",
        "                next_state_vec_tensor = torch.FloatTensor(np.array(next_state_reprs_batch))\n",
        "                action_tensor = torch.LongTensor(actions_batch)\n",
        "                reward_tensor = torch.FloatTensor(rewards_batch)\n",
        "                done_tensor   = torch.BoolTensor(dones_batch)\n",
        "\n",
        "                # Compute current Q-values for each state in the batch using ensemble\n",
        "                q_values = ensemble_q_values(state_vec_tensor, ensemble_nets)\n",
        "                state_action_values = q_values.gather(1, action_tensor.unsqueeze(1)).squeeze(1)\n",
        "\n",
        "                # Compute target Q-values using target ensemble networks\n",
        "                with torch.no_grad():\n",
        "                    if use_double_dqn:\n",
        "                        online_next_q = ensemble_q_values(next_state_vec_tensor, ensemble_nets)\n",
        "                        best_actions = online_next_q.argmax(dim=1, keepdim=True)\n",
        "                        q_vals_targets_list = [target(next_state_vec_tensor) for target in ensemble_targets]\n",
        "                        target_next_q = torch.stack(q_vals_targets_list).mean(dim=0)\n",
        "                        selected_q = target_next_q.gather(1, best_actions).squeeze(1)\n",
        "                    else:\n",
        "                        # target_next_q = ensemble_q_values(next_state_vec_tensor, ensemble_targets)\n",
        "                        q_vals_targets_list = [target(next_state_vec_tensor) for target in ensemble_targets]\n",
        "                        target_next_q = torch.stack(q_vals_targets_list).mean(dim=0)\n",
        "                        selected_q = target_next_q.max(dim=1)[0]\n",
        "                selected_q = selected_q * (1 - done_tensor.float())\n",
        "                target_values = reward_tensor + gamma * selected_q\n",
        "\n",
        "                loss = F.mse_loss(state_action_values, target_values)\n",
        "                ensemble_optimizer.zero_grad()\n",
        "                loss.backward()\n",
        "                ensemble_optimizer.step()\n",
        "\n",
        "        # Decay epsilon after each episode\n",
        "        epsilon = max(epsilon * epsilon_decay, epsilon_min)\n",
        "        # Update target networks periodically\n",
        "        if ep % target_update_freq == 0:\n",
        "            for net, target in zip(ensemble_nets, ensemble_targets):\n",
        "                target.load_state_dict(net.state_dict())\n",
        "\n",
        "        # Evaluate on the validation set every val_interval episodes for early stopping.\n",
        "        if ep % val_interval == 0:\n",
        "            val_reward = evaluate_on_validation(val_prices, ensemble_nets, initial_state)\n",
        "            print(f\"[Ep {ep}/{episodes}]  val={val_reward:.4f}  best={best_val_reward:.4f} epsilon={epsilon:.3f}\")\n",
        "\n",
        "            # 1) Early stopping: compare to best_val_reward\n",
        "            if val_reward > best_val_reward:\n",
        "                best_val_reward = val_reward\n",
        "                patience = 0\n",
        "                best_ensemble_nets = ensemble_nets\n",
        "                print(\"  ↳ new best! checkpointing ensemble\")\n",
        "            else:\n",
        "                patience += 1\n",
        "                print(f\"  ↳ no improvement over last ({patience}/{max_patience})\")\n",
        "                if patience >= max_patience:\n",
        "                    print(f\"Early stopping at episode {ep} (no improvement for {max_patience} checks)\")\n",
        "                    break\n",
        "        \n",
        "    print(\"Training completed.\")\n",
        "    return ensemble_nets"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 220,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Starting experiment 1\n",
            "[Ep 10/200]  val=-0.5236  best=-inf epsilon=0.951\n",
            "  ↳ new best! checkpointing ensemble\n",
            "[Ep 20/200]  val=39.4067  best=-0.5236 epsilon=0.905\n",
            "  ↳ new best! checkpointing ensemble\n",
            "[Ep 30/200]  val=109.8481  best=39.4067 epsilon=0.860\n",
            "  ↳ new best! checkpointing ensemble\n",
            "[Ep 40/200]  val=90.4305  best=109.8481 epsilon=0.818\n",
            "  ↳ no improvement over last (1/5)\n",
            "[Ep 50/200]  val=64.8335  best=109.8481 epsilon=0.778\n",
            "  ↳ no improvement over last (2/5)\n",
            "[Ep 60/200]  val=72.1770  best=109.8481 epsilon=0.740\n",
            "  ↳ no improvement over last (3/5)\n",
            "[Ep 70/200]  val=43.4126  best=109.8481 epsilon=0.704\n",
            "  ↳ no improvement over last (4/5)\n",
            "[Ep 80/200]  val=50.9663  best=109.8481 epsilon=0.670\n",
            "  ↳ no improvement over last (5/5)\n",
            "Early stopping at episode 80 (no improvement for 5 checks)\n",
            "Training completed.\n",
            "Starting experiment 2\n",
            "[Ep 10/200]  val=123.2404  best=-inf epsilon=0.951\n",
            "  ↳ new best! checkpointing ensemble\n",
            "[Ep 20/200]  val=84.2221  best=123.2404 epsilon=0.905\n",
            "  ↳ no improvement over last (1/5)\n",
            "[Ep 30/200]  val=69.2116  best=123.2404 epsilon=0.860\n",
            "  ↳ no improvement over last (2/5)\n",
            "[Ep 40/200]  val=180.3798  best=123.2404 epsilon=0.818\n",
            "  ↳ new best! checkpointing ensemble\n",
            "[Ep 50/200]  val=128.1989  best=180.3798 epsilon=0.778\n",
            "  ↳ no improvement over last (1/5)\n",
            "[Ep 60/200]  val=109.2820  best=180.3798 epsilon=0.740\n",
            "  ↳ no improvement over last (2/5)\n",
            "[Ep 70/200]  val=176.4970  best=180.3798 epsilon=0.704\n",
            "  ↳ no improvement over last (3/5)\n",
            "[Ep 80/200]  val=79.2423  best=180.3798 epsilon=0.670\n",
            "  ↳ no improvement over last (4/5)\n",
            "[Ep 90/200]  val=30.9612  best=180.3798 epsilon=0.637\n",
            "  ↳ no improvement over last (5/5)\n",
            "Early stopping at episode 90 (no improvement for 5 checks)\n",
            "Training completed.\n",
            "Starting experiment 3\n",
            "[Ep 10/200]  val=148.4167  best=-inf epsilon=0.951\n",
            "  ↳ new best! checkpointing ensemble\n",
            "[Ep 20/200]  val=149.6166  best=148.4167 epsilon=0.905\n",
            "  ↳ new best! checkpointing ensemble\n",
            "[Ep 30/200]  val=87.9829  best=149.6166 epsilon=0.860\n",
            "  ↳ no improvement over last (1/5)\n",
            "[Ep 40/200]  val=-0.0669  best=149.6166 epsilon=0.818\n",
            "  ↳ no improvement over last (2/5)\n",
            "[Ep 50/200]  val=61.8933  best=149.6166 epsilon=0.778\n",
            "  ↳ no improvement over last (3/5)\n",
            "[Ep 60/200]  val=0.0400  best=149.6166 epsilon=0.740\n",
            "  ↳ no improvement over last (4/5)\n",
            "[Ep 70/200]  val=52.8902  best=149.6166 epsilon=0.704\n",
            "  ↳ no improvement over last (5/5)\n",
            "Early stopping at episode 70 (no improvement for 5 checks)\n",
            "Training completed.\n",
            "Starting experiment 4\n",
            "[Ep 10/200]  val=80.1326  best=-inf epsilon=0.951\n",
            "  ↳ new best! checkpointing ensemble\n",
            "[Ep 20/200]  val=53.0983  best=80.1326 epsilon=0.905\n",
            "  ↳ no improvement over last (1/5)\n",
            "[Ep 30/200]  val=56.6759  best=80.1326 epsilon=0.860\n",
            "  ↳ no improvement over last (2/5)\n",
            "[Ep 40/200]  val=82.9168  best=80.1326 epsilon=0.818\n",
            "  ↳ new best! checkpointing ensemble\n",
            "[Ep 50/200]  val=173.8812  best=82.9168 epsilon=0.778\n",
            "  ↳ new best! checkpointing ensemble\n",
            "[Ep 60/200]  val=-0.6616  best=173.8812 epsilon=0.740\n",
            "  ↳ no improvement over last (1/5)\n",
            "[Ep 70/200]  val=-0.6616  best=173.8812 epsilon=0.704\n",
            "  ↳ no improvement over last (2/5)\n",
            "[Ep 80/200]  val=-0.6616  best=173.8812 epsilon=0.670\n",
            "  ↳ no improvement over last (3/5)\n",
            "[Ep 90/200]  val=-0.6616  best=173.8812 epsilon=0.637\n",
            "  ↳ no improvement over last (4/5)\n",
            "[Ep 100/200]  val=-0.6616  best=173.8812 epsilon=0.606\n",
            "  ↳ no improvement over last (5/5)\n",
            "Early stopping at episode 100 (no improvement for 5 checks)\n",
            "Training completed.\n",
            "Starting experiment 5\n",
            "[Ep 10/200]  val=-0.6616  best=-inf epsilon=0.951\n",
            "  ↳ new best! checkpointing ensemble\n",
            "[Ep 20/200]  val=-0.6616  best=-0.6616 epsilon=0.905\n",
            "  ↳ no improvement over last (1/5)\n",
            "[Ep 30/200]  val=-0.6616  best=-0.6616 epsilon=0.860\n",
            "  ↳ no improvement over last (2/5)\n",
            "[Ep 40/200]  val=-0.6616  best=-0.6616 epsilon=0.818\n",
            "  ↳ no improvement over last (3/5)\n",
            "[Ep 50/200]  val=-0.6616  best=-0.6616 epsilon=0.778\n",
            "  ↳ no improvement over last (4/5)\n",
            "[Ep 60/200]  val=-0.6616  best=-0.6616 epsilon=0.740\n",
            "  ↳ no improvement over last (5/5)\n",
            "Early stopping at episode 60 (no improvement for 5 checks)\n",
            "Training completed.\n",
            "Starting experiment 6\n",
            "[Ep 10/200]  val=-0.6616  best=-inf epsilon=0.951\n",
            "  ↳ new best! checkpointing ensemble\n",
            "[Ep 20/200]  val=-0.6616  best=-0.6616 epsilon=0.905\n",
            "  ↳ no improvement over last (1/5)\n",
            "[Ep 30/200]  val=-0.6616  best=-0.6616 epsilon=0.860\n",
            "  ↳ no improvement over last (2/5)\n",
            "[Ep 40/200]  val=-0.6616  best=-0.6616 epsilon=0.818\n",
            "  ↳ no improvement over last (3/5)\n",
            "[Ep 50/200]  val=-0.6616  best=-0.6616 epsilon=0.778\n",
            "  ↳ no improvement over last (4/5)\n",
            "[Ep 60/200]  val=-0.6616  best=-0.6616 epsilon=0.740\n",
            "  ↳ no improvement over last (5/5)\n",
            "Early stopping at episode 60 (no improvement for 5 checks)\n",
            "Training completed.\n",
            "Starting experiment 7\n",
            "[Ep 10/200]  val=-0.6616  best=-inf epsilon=0.951\n",
            "  ↳ new best! checkpointing ensemble\n",
            "[Ep 20/200]  val=-0.6616  best=-0.6616 epsilon=0.905\n",
            "  ↳ no improvement over last (1/5)\n",
            "[Ep 30/200]  val=-0.6616  best=-0.6616 epsilon=0.860\n",
            "  ↳ no improvement over last (2/5)\n",
            "[Ep 40/200]  val=-0.6616  best=-0.6616 epsilon=0.818\n",
            "  ↳ no improvement over last (3/5)\n",
            "[Ep 50/200]  val=-0.6616  best=-0.6616 epsilon=0.778\n",
            "  ↳ no improvement over last (4/5)\n",
            "[Ep 60/200]  val=-0.6616  best=-0.6616 epsilon=0.740\n",
            "  ↳ no improvement over last (5/5)\n",
            "Early stopping at episode 60 (no improvement for 5 checks)\n",
            "Training completed.\n",
            "Starting experiment 8\n",
            "[Ep 10/200]  val=-0.6616  best=-inf epsilon=0.951\n",
            "  ↳ new best! checkpointing ensemble\n",
            "[Ep 20/200]  val=-0.6616  best=-0.6616 epsilon=0.905\n",
            "  ↳ no improvement over last (1/5)\n",
            "[Ep 30/200]  val=-0.6616  best=-0.6616 epsilon=0.860\n",
            "  ↳ no improvement over last (2/5)\n",
            "[Ep 40/200]  val=-0.6616  best=-0.6616 epsilon=0.818\n",
            "  ↳ no improvement over last (3/5)\n",
            "[Ep 50/200]  val=-0.6616  best=-0.6616 epsilon=0.778\n",
            "  ↳ no improvement over last (4/5)\n",
            "[Ep 60/200]  val=-0.6616  best=-0.6616 epsilon=0.740\n",
            "  ↳ no improvement over last (5/5)\n",
            "Early stopping at episode 60 (no improvement for 5 checks)\n",
            "Training completed.\n",
            "Starting experiment 9\n",
            "[Ep 10/200]  val=-0.6616  best=-inf epsilon=0.951\n",
            "  ↳ new best! checkpointing ensemble\n",
            "[Ep 20/200]  val=-0.6616  best=-0.6616 epsilon=0.905\n",
            "  ↳ no improvement over last (1/5)\n",
            "[Ep 30/200]  val=-0.6616  best=-0.6616 epsilon=0.860\n",
            "  ↳ no improvement over last (2/5)\n",
            "[Ep 40/200]  val=-0.6616  best=-0.6616 epsilon=0.818\n",
            "  ↳ no improvement over last (3/5)\n",
            "[Ep 50/200]  val=-0.6616  best=-0.6616 epsilon=0.778\n",
            "  ↳ no improvement over last (4/5)\n",
            "[Ep 60/200]  val=-0.6616  best=-0.6616 epsilon=0.740\n",
            "  ↳ no improvement over last (5/5)\n",
            "Early stopping at episode 60 (no improvement for 5 checks)\n",
            "Training completed.\n",
            "Starting experiment 10\n",
            "[Ep 10/200]  val=-0.6616  best=-inf epsilon=0.951\n",
            "  ↳ new best! checkpointing ensemble\n",
            "[Ep 20/200]  val=-0.6616  best=-0.6616 epsilon=0.905\n",
            "  ↳ no improvement over last (1/5)\n",
            "[Ep 30/200]  val=-0.6616  best=-0.6616 epsilon=0.860\n",
            "  ↳ no improvement over last (2/5)\n",
            "[Ep 40/200]  val=-0.6616  best=-0.6616 epsilon=0.818\n",
            "  ↳ no improvement over last (3/5)\n",
            "[Ep 50/200]  val=-0.6616  best=-0.6616 epsilon=0.778\n",
            "  ↳ no improvement over last (4/5)\n",
            "[Ep 60/200]  val=-0.6616  best=-0.6616 epsilon=0.740\n",
            "  ↳ no improvement over last (5/5)\n",
            "Early stopping at episode 60 (no improvement for 5 checks)\n",
            "Training completed.\n"
          ]
        }
      ],
      "source": [
        "def run_experiments(n_experiments=5, **train_params):\n",
        "    \"\"\"\n",
        "    Runs the entire training process for n experiments and prints a summary.\n",
        "    Returns a list of (trained_ensemble, best_val_reward) tuples from each experiment.\n",
        "    \"\"\"\n",
        "    trained_models = []\n",
        "    for i in range(n_experiments):\n",
        "        print(\"Starting experiment\", i+1)\n",
        "        trained_ensemble = train_agent(**train_params)\n",
        "        trained_models.append(trained_ensemble)\n",
        "    return trained_models\n",
        "\n",
        "models = run_experiments(n_experiments=10, episodes=200)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vzosJhCGr4NQ"
      },
      "source": [
        "### Policy Evaluation on Test Data"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "def evaluate_policy(price_array, model, initial_state, runs=1):\n",
        "    \"\"\"\n",
        "    Evaluate the policy by simulating the portfolio over time using a greedy (deterministic) policy.\n",
        "    It runs the simulation multiple times (default 10), computes key metrics for each run, and returns\n",
        "    the best simulation (based on the final portfolio value). Key metrics include:\n",
        "      - Final portfolio value and return (%)\n",
        "      - Annualized Sharpe ratio (computed from daily returns)\n",
        "      - Maximum drawdown\n",
        "    Also computes baseline (buy-and-hold) performance for comparison.\n",
        "    \n",
        "    Parameters:\n",
        "      price_array: numpy array of asset prices.\n",
        "      model: the trained DQN model (or ensemble function) used for Q-value prediction.\n",
        "      initial_state: initial portfolio allocation (tuple of 6 integers).\n",
        "      runs: number of simulation runs.\n",
        "\n",
        "    Returns:\n",
        "      best_values: best portfolio value time series (list of portfolio values for each day).\n",
        "      best_metrics: dictionary of metrics for the best run.\n",
        "      baseline_value: final portfolio value using baseline (buy-and-hold equal weights).\n",
        "      baseline_return: baseline return in %.\n",
        "      all_metrics: list of metric dictionaries for each run.\n",
        "    \"\"\"\n",
        "    \n",
        "    best_values = None\n",
        "    best_final_value = -np.inf\n",
        "    best_metrics = {}\n",
        "    all_metrics = []\n",
        "    \n",
        "    # Run the simulation multiple times\n",
        "    for run in range(runs):\n",
        "        days = price_array.shape[0]\n",
        "        state = initial_state\n",
        "        portfolio_value = 1.0  # Start with $1.0\n",
        "        values = [portfolio_value]\n",
        "        daily_returns = []  # for Sharpe ratio computation\n",
        "        \n",
        "        for t in range(days - 1):\n",
        "            # Get the state vector (which includes allocations and market indicators)\n",
        "            state_input = get_state(t, state, price_array)\n",
        "            state_tensor = torch.FloatTensor(state_input).unsqueeze(0)\n",
        "            \n",
        "            # Predict Q-values for the current state\n",
        "            with torch.no_grad():\n",
        "                q_vals = ensemble_q_values(state_tensor, model).numpy().squeeze()\n",
        "            \n",
        "            # Mask out invalid actions\n",
        "            valid_acts = get_valid_actions(state)\n",
        "            invalid_acts = set(all_actions) - set(valid_acts)\n",
        "            for act in invalid_acts:\n",
        "                q_vals[action_to_index[act]] = -1e9\n",
        "                \n",
        "            best_act_idx = int(np.argmax(q_vals))\n",
        "            best_action = all_actions[best_act_idx]\n",
        "            \n",
        "            # Rebalance portfolio based on the chosen action\n",
        "            state = apply_action(state, best_action)\n",
        "            \n",
        "            # Compute portfolio growth factor for day t to t+1\n",
        "            weights = [x / 100.0 for x in state]\n",
        "            growth_factor = sum([\n",
        "                weights[k] * (price_array[t+1][k] / price_array[t][k])\n",
        "                if k < num_assets else weights[cash_idx]\n",
        "                for k in range(len(assets_plus_cash))\n",
        "            ])\n",
        "            portfolio_value *= growth_factor\n",
        "            values.append(portfolio_value)\n",
        "            daily_returns.append(growth_factor - 1)  # daily return (excess return if risk-free rate is 0)\n",
        "        \n",
        "        # Final return in percentage terms\n",
        "        final_return = (portfolio_value - 1.0) * 100.0\n",
        "        \n",
        "        # Compute Sharpe ratio: mean daily return / std daily return * sqrt(252)\n",
        "        mean_daily = np.mean(daily_returns)\n",
        "        std_daily = np.std(daily_returns) if np.std(daily_returns) > 0 else 1e-6\n",
        "        sharpe_ratio = mean_daily / std_daily * np.sqrt(price_array.shape[0])\n",
        "        # Compute maximum drawdown\n",
        "        values_array = np.array(values)\n",
        "        peak = np.maximum.accumulate(values_array)\n",
        "        drawdown = (peak - values_array) / peak\n",
        "        max_drawdown = np.max(drawdown)\n",
        "        \n",
        "        metrics = {\n",
        "            'final_portfolio_value': portfolio_value,\n",
        "            'final_return': final_return,\n",
        "            'sharpe_ratio': sharpe_ratio,\n",
        "            'max_drawdown': max_drawdown\n",
        "        }\n",
        "        all_metrics.append(metrics)\n",
        "        \n",
        "        # Update best run based on final portfolio value\n",
        "        if portfolio_value > best_final_value:\n",
        "            best_final_value = portfolio_value\n",
        "            best_values = values\n",
        "            best_metrics = metrics\n",
        "    \n",
        "    # Baseline evaluation: Buy-and-hold with equal weights for assets (cash remains unaltered)\n",
        "    # days = price_array.shape[0]\n",
        "    # baseline_value = 1.0\n",
        "    # baseline_weights = [1.0 / num_assets] * num_assets  # equal weights on assets\n",
        "    # baseline_shares = [baseline_weights[i] * baseline_value / price_array[0][i] for i in range(num_assets)]\n",
        "    # # Assuming no rebalancing, simulate value appreciation over time\n",
        "    # for t in range(days - 1):\n",
        "    #     baseline_portfolio_val = 0.0\n",
        "    #     for k in range(num_assets):\n",
        "    #         baseline_portfolio_val += baseline_shares[k] * price_array[t+1][k]\n",
        "    #     baseline_value = baseline_portfolio_val\n",
        "    # baseline_return = (baseline_value - 1.0) * 100.0\n",
        "    \n",
        "    print(\"Best simulation metrics:\")\n",
        "    print(f\"  Final portfolio value: {best_metrics['final_portfolio_value']:.4f}\")\n",
        "    print(f\"  Return: {best_metrics['final_return']:.2f}%\")\n",
        "    print(f\"  Annualized Sharpe ratio: {best_metrics['sharpe_ratio']:.4f}\")\n",
        "    print(f\"  Maximum drawdown: {best_metrics['max_drawdown']:.2%}\")\n",
        "    print()\n",
        "    # print(f\"Baseline portfolio value: {baseline_value:.4f}\")\n",
        "    # print(f\"  Return: {baseline_return:.2f}%\")\n",
        "    \n",
        "    return best_values, best_metrics, all_metrics"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "def evaluate_baseline(price_array):\n",
        "    days = price_array.shape[0]\n",
        "    baseline_value = 1.0\n",
        "    baseline_weights = [1.0 / num_assets] * num_assets  # equal weights on assets\n",
        "    baseline_shares = [baseline_weights[i] * baseline_value / price_array[0][i] for i in range(num_assets)]\n",
        "    # Assuming no rebalancing, simulate value appreciation over time\n",
        "    for t in range(days - 1):\n",
        "        baseline_portfolio_val = 0.0\n",
        "        for k in range(num_assets):\n",
        "            baseline_portfolio_val += baseline_shares[k] * price_array[t+1][k]\n",
        "        baseline_value = baseline_portfolio_val\n",
        "    baseline_return = (baseline_value - 1.0) * 100.0\n",
        "    return baseline_value, baseline_return"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "a3zdHpEGGpHd"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Evaluating model...\n",
            "Best simulation metrics:\n",
            "  Final portfolio value: 1.2724\n",
            "  Return: 27.24%\n",
            "  Annualized Sharpe ratio: 1.0501\n",
            "  Maximum drawdown: 24.07%\n",
            "\n",
            "Evaluation completed.\n",
            "\n",
            "Evaluating model...\n",
            "Best simulation metrics:\n",
            "  Final portfolio value: 29517456.8283\n",
            "  Return: 2951745582.83%\n",
            "  Annualized Sharpe ratio: 25.2207\n",
            "  Maximum drawdown: 6.09%\n",
            "\n",
            "Evaluation completed.\n",
            "\n",
            "Evaluating model...\n",
            "Best simulation metrics:\n",
            "  Final portfolio value: 1416967557894460982047269954613254052887331189078539563433984.0000\n",
            "  Return: 141696755789446102486470073579205048463591027256339365491638272.00%\n",
            "  Annualized Sharpe ratio: 31.2688\n",
            "  Maximum drawdown: 1.20%\n",
            "\n",
            "Evaluation completed.\n",
            "\n",
            "Evaluating model...\n",
            "Best simulation metrics:\n",
            "  Final portfolio value: 0.9643\n",
            "  Return: -3.57%\n",
            "  Annualized Sharpe ratio: 0.0932\n",
            "  Maximum drawdown: 35.87%\n",
            "\n",
            "Evaluation completed.\n",
            "\n",
            "Evaluating model...\n",
            "Best simulation metrics:\n",
            "  Final portfolio value: 0.9643\n",
            "  Return: -3.57%\n",
            "  Annualized Sharpe ratio: 0.0932\n",
            "  Maximum drawdown: 35.87%\n",
            "\n",
            "Evaluation completed.\n",
            "\n",
            "Evaluating model...\n",
            "Best simulation metrics:\n",
            "  Final portfolio value: 0.9643\n",
            "  Return: -3.57%\n",
            "  Annualized Sharpe ratio: 0.0932\n",
            "  Maximum drawdown: 35.87%\n",
            "\n",
            "Evaluation completed.\n",
            "\n",
            "Evaluating model...\n",
            "Best simulation metrics:\n",
            "  Final portfolio value: 0.9643\n",
            "  Return: -3.57%\n",
            "  Annualized Sharpe ratio: 0.0932\n",
            "  Maximum drawdown: 35.87%\n",
            "\n",
            "Evaluation completed.\n",
            "\n",
            "Evaluating model...\n",
            "Best simulation metrics:\n",
            "  Final portfolio value: 0.9643\n",
            "  Return: -3.57%\n",
            "  Annualized Sharpe ratio: 0.0932\n",
            "  Maximum drawdown: 35.87%\n",
            "\n",
            "Evaluation completed.\n",
            "\n",
            "Evaluating model...\n",
            "Best simulation metrics:\n",
            "  Final portfolio value: 0.9643\n",
            "  Return: -3.57%\n",
            "  Annualized Sharpe ratio: 0.0932\n",
            "  Maximum drawdown: 35.87%\n",
            "\n",
            "Evaluation completed.\n",
            "\n",
            "Evaluating model...\n",
            "Best simulation metrics:\n",
            "  Final portfolio value: 0.9643\n",
            "  Return: -3.57%\n",
            "  Annualized Sharpe ratio: 0.0932\n",
            "  Maximum drawdown: 35.87%\n",
            "\n",
            "Evaluation completed.\n",
            "\n",
            "Final evaluation results:\n",
            "    Final portfolio value: 1416967557894460982047269954613254052887331189078539563433984.0000\n",
            "    Final return: 141696755789446102486470073579205048463591027256339365491638272.00%\n",
            "    Annualized Sharpe ratio: 31.2688\n",
            "    Maximum drawdown: 1.20%\n",
            "\n",
            "Baseline portfolio value: 0.6263\n",
            "  Return: -37.37%\n"
          ]
        }
      ],
      "source": [
        "# Run evaluation on the test prices using the ensemble Q-value network function (or your model)\n",
        "results = {\n",
        "    'final_portfolio_value': -np.inf,\n",
        "    'final_return': 0.0,\n",
        "    'sharpe_ratio': 0.0,\n",
        "    'max_drawdown': 0.0,\n",
        "}\n",
        "for model in models:\n",
        "    print(\"Evaluating model...\")\n",
        "    agent_values, agent_metrics, all_run_metrics = evaluate_policy(test_prices, model, initial_state, 10)\n",
        "    if results['final_portfolio_value'] < agent_metrics['final_portfolio_value']:\n",
        "        results = agent_metrics\n",
        "    print(\"Evaluation completed.\")\n",
        "    print()\n",
        "\n",
        "results['baseline_value'], results['baseline_return'] = evaluate_baseline(test_prices)\n",
        "\n",
        "print(\"Final evaluation results:\")\n",
        "print(f\"    Final portfolio value: {results['final_portfolio_value']:.4f}\")\n",
        "print(f\"    Final return: {results['final_return']:.2f}%\")\n",
        "print(f\"    Annualized Sharpe ratio: {results['sharpe_ratio']:.4f}\")\n",
        "print(f\"    Maximum drawdown: {results['max_drawdown']:.2%}\")\n",
        "print()\n",
        "print(f\"Baseline portfolio value: {results['baseline_value']:.4f}\")\n",
        "print(f\"  Return: {results['baseline_return']:.2f}%\")"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "T4",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.9.6"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
