{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 86,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "_Y77BeglBh0B",
        "outputId": "e96b675b-fabe-4d82-947e-bccea7072beb"
      },
      "outputs": [],
      "source": [
        "# from google.colab import drive\n",
        "# drive.mount('/content/drive')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 87,
      "metadata": {
        "id": "ygKC_nPqrODL"
      },
      "outputs": [],
      "source": [
        "import math\n",
        "import random\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "import torch.optim as optim\n",
        "import yfinance as yf\n",
        "from collections import deque\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "WaT-xj4XrdOK"
      },
      "source": [
        "### Data Preparation"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 88,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "9ILxQXy4rY-R",
        "outputId": "77615a20-66fc-412f-dfc7-bed4f770d3c6"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Loaded data from prices.csv\n",
            "Training days: 1323, Testing days: 250\n"
          ]
        }
      ],
      "source": [
        "# Define the 10 assets (tickers) for the portfolio\n",
        "# tickers = [\"AAPL\", \"MSFT\", \"GOOGL\", \"AMZN\", \"TSLA\", \"BA\", \"NFLX\", \"NVDA\", \"META\", \"SBUX\"]\n",
        "# tickers = [\"AAPL\", \"MSFT\", \"GOOGL\", \"SBUX\", \"TSLA\"]\n",
        "tickers = [\"GME\", \"AMC\", \"SPCE\", \"NVAX\", \"NOK\"]\n",
        "\n",
        "# Date range for historical data\n",
        "start_date = \"2015-01-01\"\n",
        "end_date   = \"2023-12-31\"\n",
        "\n",
        "# Try to load price data from a local CSV, otherwise download using yfinance\n",
        "data_file = \"prices.csv\"\n",
        "try:\n",
        "    prices_df = pd.read_csv(data_file, index_col=0, parse_dates=True)\n",
        "    print(\"Loaded data from\", data_file)\n",
        "except FileNotFoundError:\n",
        "    print(\"Downloading price data for tickers:\", tickers)\n",
        "    df = yf.download(tickers, start=start_date, end=end_date, interval=\"1d\")\n",
        "    # Extract the 'Close' prices from the MultiIndex DataFrame\n",
        "    prices_df = df.xs('Close', axis=1, level='Price')\n",
        "    prices_df.dropna(inplace=True)\n",
        "    prices_df.to_csv(data_file)\n",
        "    print(\"Data downloaded and saved to\", data_file)\n",
        "\n",
        "# Split data into training (first 4 years) and testing (last year)\n",
        "full_train_df = prices_df[prices_df.index < \"2023-01-01\"]\n",
        "test_df  = prices_df[prices_df.index >= \"2023-01-01\"]\n",
        "full_train_prices = full_train_df.values  # shape: [train_days, 5]\n",
        "test_prices  = test_df.values   # shape: [test_days, 5]\n",
        "num_assets = full_train_prices.shape[1]\n",
        "print(f\"Training days: {full_train_prices.shape[0]}, Testing days: {test_prices.shape[0]}\")\n",
        "\n",
        "# Further split full training into training and validation (80/20)\n",
        "split_index = int(0.8 * full_train_prices.shape[0])\n",
        "train_prices = full_train_prices[:split_index]\n",
        "val_prices   = full_train_prices[split_index:]"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "###  Technical Indicators & State Representation"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 89,
      "metadata": {},
      "outputs": [],
      "source": [
        "def get_market_indicators(price_array, t, window=10):\n",
        "    \"\"\"\n",
        "    Compute technical indicators for each asset at time t:\n",
        "      - Moving Average Ratio: current price / moving average over window\n",
        "      - Volatility: std dev of daily returns over window (normalized by current price)\n",
        "    If not enough history exists, use current price and low volatility.\n",
        "    Returns a list of 2 values per asset.\n",
        "    \"\"\"\n",
        "    indicators = []\n",
        "    for asset in range(num_assets):\n",
        "        if t < window:\n",
        "            # Not enough history: use current price and set volatility to 0\n",
        "            moving_avg_ratio = 1.0\n",
        "            vol = 0.0\n",
        "        else:\n",
        "            window_prices = price_array[t-window+1:t+1, asset]\n",
        "            moving_avg = np.mean(window_prices)\n",
        "            moving_avg_ratio = price_array[t, asset] / (moving_avg + 1e-6)\n",
        "            returns = np.diff(window_prices) / (window_prices[:-1] + 1e-6)\n",
        "            vol = np.std(returns) / (price_array[t, asset] + 1e-6)\n",
        "        indicators.extend([moving_avg_ratio, vol])\n",
        "    return indicators\n",
        "\n",
        "def get_state(t, alloc_state, price_array):\n",
        "    \"\"\"\n",
        "    Constructs the state vector at time t by combining:\n",
        "      - Portfolio allocations (6 numbers: 5 assets + cash) normalized to [0,1]\n",
        "      - Market indicators for assets (2 per asset)\n",
        "    Returns a numpy array of length 6 + 10 = 16.\n",
        "    \"\"\"\n",
        "    # Normalize allocation percentages (they sum to 100)\n",
        "    alloc_norm = [a / 100.0 for a in alloc_state]\n",
        "    indicators = get_market_indicators(price_array, t)\n",
        "    return np.array(alloc_norm + indicators)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "uEJnt93hrlGz"
      },
      "source": [
        "### State Encoding/Decoding and Actions"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 90,
      "metadata": {
        "id": "0191obe4rd9I"
      },
      "outputs": [],
      "source": [
        "# Construct the new action space.\n",
        "# Each action is a tuple (src, dst, amt) where:\n",
        "# - src and dst are indices in assets_plus_cash (0..5)\n",
        "# - amt is in [1, 2, 3, 4, 5] representing a 1%-5% transfer.\n",
        "# We allow transferring funds between any two different assets.\n",
        "\n",
        "# Define all possible actions (including no-action) as tuples\n",
        "all_actions = []\n",
        "cash_idx = num_assets            # index 5 is for cash\n",
        "assets_plus_cash = list(range(num_assets)) + [cash_idx]  # [0,1,2,3,4,5]\n",
        "percents = [1,2,3,4,5]\n",
        "for pct in percents:\n",
        "    for src in assets_plus_cash:\n",
        "        for dst in assets_plus_cash:\n",
        "            if src != dst:\n",
        "                all_actions.append((src, dst, pct))\n",
        "all_actions.append((None, None, 0))  # No action\n",
        "action_count = len(all_actions) # 151\n",
        "# print(\"Number of actions:\", action_count)\n",
        "# Create a mapping from action tuple to index in all_actions list\n",
        "action_to_index = {act: idx for idx, act in enumerate(all_actions)}\n",
        "\n",
        "def get_valid_actions(state):\n",
        "    \"\"\"\n",
        "    Given a state (tuple of 6 integers summing to 100 representing allocations for 5 stocks and cash),\n",
        "    return a list of valid actions (from all_actions_new).\n",
        "    \n",
        "    Rules:\n",
        "      - If src is not cash, state[src] must be >= amt (i.e. you must have enough allocation to sell).\n",
        "      - Optionally, we can restrict that if dst is not cash, the allocation after transfer should not exceed 100.\n",
        "        (Here we assume no single asset can have more than 100%).\n",
        "      - The no-action (None, None, 0) is always valid.\n",
        "    \"\"\"\n",
        "    valid = []\n",
        "    for act in all_actions:\n",
        "        if act == (None, None, 0):\n",
        "            valid.append(act)\n",
        "        else:\n",
        "            src, dst, amt = act\n",
        "            # Check: if src is not cash, then it must have at least 'amt'\n",
        "            if src != cash_idx and state[src] < amt:\n",
        "                continue\n",
        "            # Check: if dst is not cash, ensure it does not exceed 100.\n",
        "            if dst != cash_idx and state[dst] + amt > 100:\n",
        "                continue\n",
        "            valid.append(act)\n",
        "    return valid\n",
        "\n",
        "\n",
        "def apply_action(state, action):\n",
        "    \"\"\"\n",
        "    Apply an action (src, dst, amt) to the state.\n",
        "    - state: tuple of 6 integers (allocations in % for 5 stocks and cash; sum to 100)\n",
        "    - action: (src, dst, amt). For example, (0,5,3) means transfer 3% from asset 0 to cash.\n",
        "      (None, None, 0) means no action.\n",
        "    Returns the new state as a tuple of 6 integers.\n",
        "    \"\"\"\n",
        "    state = list(state)\n",
        "    if action == (None, None, 0):\n",
        "        return tuple(state)\n",
        "    src, dst, amt = action\n",
        "    # If src is not cash, reduce its allocation by amt (clip at 0)\n",
        "    if src is not None and src != cash_idx and state[src] >= amt:\n",
        "        state[src] = state[src] - amt\n",
        "    # If dst is not cash, increase its allocation by amt (clip at 100)\n",
        "    if dst is not None and dst != cash_idx and state[dst] + amt <= 100:\n",
        "        state[dst] = state[dst] + amt\n",
        "    # If src is cash, subtract amt from cash, if dst is cash, add amt to cash.\n",
        "    if src == cash_idx:\n",
        "        state[cash_idx] = state[cash_idx] - amt\n",
        "    if dst == cash_idx:\n",
        "        state[cash_idx] = state[cash_idx] + amt\n",
        "    # Optionally, enforce that the new state's sum remains 100. Here, if clipping occurred,\n",
        "    # you might choose to normalize or simply allow slight deviations.\n",
        "    # For now, assume actions are valid and state remains valid.\n",
        "    return tuple(state)\n",
        "\n",
        "# def compute_reward(weights_frac, price_today, price_next):\n",
        "#     \"\"\"\n",
        "#     Compute the log return of the portfolio for one time step.\n",
        "#     - weights_frac: list of 5 asset weight fractions after rebalancing on day t (sum=1).\n",
        "#     - price_today: prices of the 5 assets at day t.\n",
        "#     - price_next: prices of the 5 assets at day t+1.\n",
        "#     Returns: log(portfolio return) from day t to t+1.\n",
        "#     \"\"\"\n",
        "#     # Portfolio value growth factor = sum_k w_k * (price_next_k / price_today_k)\n",
        "#     growth_factor = 0.0\n",
        "#     for k in range(len(assets_plus_cash)):\n",
        "#         if k == cash_idx:\n",
        "#             ratio = 1.0\n",
        "#         else:\n",
        "#             ratio = price_next[k] / price_today[k]\n",
        "#         growth_factor += weights_frac[k] * ratio\n",
        "#     # Reward is the log of the growth factor\n",
        "#     return math.log(growth_factor)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qMipXOQPiNWW"
      },
      "source": [
        "### Reward Shaping using a Rolling Sharpe Ratio"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 91,
      "metadata": {
        "id": "7WYWx-jViOO0"
      },
      "outputs": [],
      "source": [
        "class SharpeRewardShaper:\n",
        "    def __init__(self, window=30, epsilon=1e-6):\n",
        "        self.window = window\n",
        "        self.rewards_history = []\n",
        "        self.epsilon = epsilon\n",
        "        self.portfolio_max = -np.inf\n",
        "\n",
        "    def shape(self, raw_reward, current_portfolio):\n",
        "        \"\"\"\n",
        "        Enhances reward by:\n",
        "          - Computing rolling Sharpe ratio,\n",
        "          - Penalizing drawdowns (if current_portfolio is lower than past peak),\n",
        "          - Penalizing high volatility.\n",
        "        \"\"\"\n",
        "        self.rewards_history.append(raw_reward)\n",
        "        if len(self.rewards_history) > self.window:\n",
        "            self.rewards_history.pop(0)\n",
        "        avg_reward = np.mean(self.rewards_history)\n",
        "        std_reward = np.std(self.rewards_history) + self.epsilon\n",
        "        sharpe = avg_reward / std_reward\n",
        "\n",
        "        # Update running maximum portfolio value\n",
        "        self.portfolio_max = max(self.portfolio_max, current_portfolio)\n",
        "        drawdown = (self.portfolio_max - current_portfolio) / (self.portfolio_max + self.epsilon)\n",
        "        drawdown_penalty = drawdown * 0.5  # factor can be tuned\n",
        "\n",
        "        # Volatility penalty: higher volatility (std reward) reduces reward\n",
        "        vol_penalty = std_reward * 0.5  # factor can be tuned\n",
        "\n",
        "        # Final shaped reward: encourage long term gain via n-step (raw_reward already multi-step) + penalize risks\n",
        "        shaped_reward = sharpe - drawdown_penalty - vol_penalty\n",
        "\n",
        "        return shaped_reward\n",
        "\n",
        "def compute_reward(weights_frac, price_array, t, n_step=3):\n",
        "    \"\"\"\n",
        "    Compute the reward using n-step return from time t to t+n_step.\n",
        "    Growth is computed by taking the weighted sum of asset returns over n_step days.\n",
        "    If t+n_step is beyond available data, use last available day.\n",
        "    \"\"\"\n",
        "    last_index = min(t + n_step, price_array.shape[0] - 1)\n",
        "    growth_factor = 0.0\n",
        "    for k in range(len(assets_plus_cash)):\n",
        "        # For cash, return is 1.0; for assets use price ratio over n steps.\n",
        "        if k == cash_idx:\n",
        "            ratio = 1.0\n",
        "        else:\n",
        "            ratio = price_array[last_index, k] / price_array[t, k]\n",
        "        growth_factor += weights_frac[k] * ratio\n",
        "    # raw reward is log of multi-step growth factor\n",
        "    return math.log(growth_factor + 1e-6)\n",
        "\n",
        "reward_shaper = SharpeRewardShaper(window=30)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Uqa6xKTSrqzp"
      },
      "source": [
        "### Replay Buffer for Experience Replay"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 92,
      "metadata": {
        "id": "88rVfnT-rnrp"
      },
      "outputs": [],
      "source": [
        "class ReplayBuffer:\n",
        "    def __init__(self, capacity):\n",
        "        \"\"\"\n",
        "        Replay buffer to store past transitions for experience replay.\n",
        "        Stores tuples of (state, action_index, reward, next_state, done).\n",
        "        \"\"\"\n",
        "        self.capacity = capacity\n",
        "        self.buffer = deque(maxlen=capacity)\n",
        "\n",
        "    def push(self, state, action_idx, reward, next_state, done):\n",
        "        \"\"\"Save a transition to the buffer.\"\"\"\n",
        "        self.buffer.append((state, action_idx, reward, next_state, done))\n",
        "\n",
        "    def sample(self, batch_size):\n",
        "        \"\"\"\n",
        "        Sample a random batch of transitions from the buffer.\n",
        "        Returns: tuples (states, action_idxs, rewards, next_states, dones) for the batch.\n",
        "        \"\"\"\n",
        "        batch = random.sample(self.buffer, batch_size)\n",
        "        # Extract each component into separate lists\n",
        "        states, action_idxs, rewards, next_states, dones = zip(*batch)\n",
        "        return list(states), list(action_idxs), list(rewards), list(next_states), list(dones)\n",
        "\n",
        "    def __len__(self):\n",
        "        \"\"\"Current size of the buffer.\"\"\"\n",
        "        return len(self.buffer)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-z0AwXHTrwmz"
      },
      "source": [
        "### Neural Network for Q-value Function"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 93,
      "metadata": {
        "id": "j6RJgJrfrtDJ"
      },
      "outputs": [],
      "source": [
        "class DQNNetwork(nn.Module):\n",
        "    def __init__(self, state_dim, action_dim):\n",
        "        \"\"\"\n",
        "        Neural network that approximates Q(s,a) for all actions a given state s.\n",
        "        state_dim: dimensionality of state input (e.g. 5)\n",
        "        action_dim: number of possible actions (e.g. 21)\n",
        "        \"\"\"\n",
        "        super(DQNNetwork, self).__init__()\n",
        "\n",
        "        # self.fc1 = nn.Linear(state_dim, 128)\n",
        "        # self.bn1 = nn.BatchNorm1d(128)\n",
        "        # self.dropout1 = nn.Dropout(0.3)\n",
        "        # self.fc2 = nn.Linear(128, 256)\n",
        "        # self.bn2 = nn.BatchNorm1d(256)\n",
        "        # self.dropout2 = nn.Dropout(0.3)\n",
        "        # self.fc3 = nn.Linear(256, 128)\n",
        "        # self.fc4 = nn.Linear(128, action_dim)\n",
        "\n",
        "        # hidden1 = 128\n",
        "        # hidden2 = 128\n",
        "        # self.fc1 = nn.Linear(state_dim, hidden1)\n",
        "        # self.fc2 = nn.Linear(hidden1, hidden2)\n",
        "        # self.fc3 = nn.Linear(hidden2, action_dim)\n",
        "\n",
        "        self.fc1 = nn.Linear(state_dim, 128)\n",
        "        self.ln1 = nn.LayerNorm(128)\n",
        "        self.dropout1 = nn.Dropout(0.3)\n",
        "        self.fc2 = nn.Linear(128, 256)\n",
        "        self.ln2 = nn.LayerNorm(256)\n",
        "        self.dropout2 = nn.Dropout(0.3)\n",
        "        self.fc3 = nn.Linear(256, 128)\n",
        "        self.fc4 = nn.Linear(128, action_dim)\n",
        "\n",
        "    def forward(self, x):\n",
        "        # x is a tensor of shape [batch_size, state_dim]\n",
        "\n",
        "        # x = F.relu(self.bn1(self.fc1(x)))\n",
        "        # x = self.dropout1(x)\n",
        "        # x = F.relu(self.bn2(self.fc2(x)))\n",
        "        # x = self.dropout2(x)\n",
        "        # x = F.relu(self.fc3(x))\n",
        "        # q_vals = self.fc4(x)\n",
        "\n",
        "        # x = F.relu(self.fc1(x))\n",
        "        # x = F.relu(self.fc2(x))\n",
        "        # q_values = self.fc3(x)  # outputs Q-values for each action\n",
        "\n",
        "        x = F.relu(self.ln1(self.fc1(x)))\n",
        "        x = self.dropout1(x)\n",
        "        x = F.relu(self.ln2(self.fc2(x)))\n",
        "        x = self.dropout2(x)\n",
        "        x = F.relu(self.fc3(x))\n",
        "        q_vals = self.fc4(x)\n",
        "\n",
        "        return q_vals"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8DkUKJ6zr0X7"
      },
      "source": [
        "### DQN Agent Training Setup"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "4FEtbjn1r0v7",
        "outputId": "aeb4afb6-650e-4ba8-903b-54dec9397ab2"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Episode 10/150 completed, epsilon=0.951\n",
            "Episode 20/150 completed, epsilon=0.905\n",
            "Episode 30/150 completed, epsilon=0.860\n",
            "Episode 40/150 completed, epsilon=0.818\n",
            "Episode 50/150 completed, epsilon=0.778\n",
            "Episode 60/150 completed, epsilon=0.740\n",
            "Episode 70/150 completed, epsilon=0.704\n",
            "Episode 80/150 completed, epsilon=0.670\n",
            "Episode 90/150 completed, epsilon=0.637\n",
            "Episode 100/150 completed, epsilon=0.606\n",
            "Episode 110/150 completed, epsilon=0.576\n",
            "Episode 120/150 completed, epsilon=0.548\n",
            "Episode 130/150 completed, epsilon=0.521\n",
            "Episode 140/150 completed, epsilon=0.496\n",
            "Episode 150/150 completed, epsilon=0.471\n",
            "Training completed.\n"
          ]
        }
      ],
      "source": [
        "# Hyperparameters\n",
        "gamma = 0.99            # discount factor for future rewards\n",
        "learning_rate = 5e-4  # learning rate for optimizer\n",
        "epsilon_start = 1.0     # initial exploration rate\n",
        "epsilon_min   = 0.2     # minimum exploration rate\n",
        "epsilon_decay = 0.995    # multiplicative decay factor per episode\n",
        "episodes = 150          # number of training episodes\n",
        "batch_size = 128         # mini-batch size for replay updates\n",
        "target_update_freq = 5 # how often (in episodes) to update the target network\n",
        "replay_capacity = 10000 # capacity of the replay buffer\n",
        "use_double_dqn = True  # use Double DQN for      \n",
        "state_dim = num_assets + 1 + num_assets * 2  # 6 (allocations) + 10 (2 indicators per each of the 5 assets) = 16\n",
        "ensemble_size = 2\n",
        "temperature = 1.0\n",
        "\n",
        "# Initialize replay memory, policy network, target network, optimizer\n",
        "# prioritized_replay_buffer = PrioritizedReplayBuffer(replay_capacity)\n",
        "\n",
        "# policy_net = DQNNetwork(state_dim, action_dim)\n",
        "# target_net = DQNNetwork(state_dim, action_dim)\n",
        "# Copy initial weights to target network\n",
        "# target_net.load_state_dict(policy_net.state_dict())\n",
        "# target_net.eval()  # target network in evaluation mode (not strictly necessary)\n",
        "# optimizer = optim.Adam(policy_net.parameters(), lr=learning_rate)\n",
        "\n",
        "\n",
        "# Create ensemble of networks and their corresponding target networks\n",
        "ensemble_nets = [DQNNetwork(state_dim, action_count) for _ in range(ensemble_size)]\n",
        "ensemble_targets = [DQNNetwork(state_dim, action_count) for _ in range(ensemble_size)]\n",
        "for net, target in zip(ensemble_nets, ensemble_targets):\n",
        "    target.load_state_dict(net.state_dict())\n",
        "    target.eval()\n",
        "# Combine parameters of all ensemble networks in one optimizer\n",
        "ensemble_optimizer = optim.Adam([p for net in ensemble_nets for p in net.parameters()], lr=learning_rate)\n",
        "\n",
        "# For action selection and training, we take the average Q-values across ensemble members.\n",
        "def ensemble_q_values(state_input):\n",
        "    # Temporarily store the training state of each network.\n",
        "    original_modes = [net.training for net in ensemble_nets]\n",
        "\n",
        "    # Switch networks to eval mode for inference (to avoid BN issues with batch size 1)\n",
        "    for net in ensemble_nets:\n",
        "        net.eval()\n",
        "\n",
        "    # Compute Q-values from each network and average them\n",
        "    q_vals_list = [net(state_input) for net in ensemble_nets]  # shape: [ensemble_size, batch, action_dim]\n",
        "    avg_q_vals = torch.stack(q_vals_list, dim=0).mean(dim=0)\n",
        "\n",
        "    # Restore the original training mode of each network\n",
        "    for net, mode in zip(ensemble_nets, original_modes):\n",
        "        if mode:\n",
        "            net.train()\n",
        "        else:\n",
        "            net.eval()\n",
        "\n",
        "    return avg_q_vals\n",
        "\n",
        "# Helper function: select action using epsilon-greedy policy\n",
        "def select_action(state, t, train_prices, epsilon):\n",
        "    \"\"\"\n",
        "    Hybrid exploration:\n",
        "      - With probability epsilon, choose a random valid action.\n",
        "      - Otherwise, use Boltzmann sampling (softmax over Q-values for valid actions).\n",
        "    The state passed in is the portfolio allocation (tuple of 6 ints).\n",
        "    We combine it with market indicators computed from price_array at time t.\n",
        "    Returns: (action_idx, action_tuple)\n",
        "    \"\"\"\n",
        "    valid_actions = get_valid_actions(state)\n",
        "    if random.random() < epsilon:\n",
        "        # Exploration: random valid action\n",
        "        action = random.choice(valid_actions)\n",
        "    else:\n",
        "        # Exploitation: choose best action according to Q-network\n",
        "        # Normalize state: now state values are percentages out of 100\n",
        "        state_vec = get_state(t, state, train_prices)\n",
        "        state_input = torch.FloatTensor(state_vec).unsqueeze(0)\n",
        "        with torch.no_grad():\n",
        "          q_values = ensemble_q_values(state_input)  # shape [1, action_count]\n",
        "          q_values = q_values.numpy().squeeze()  # shape [action_count]\n",
        "        # Mask out invalid actions by setting their Q-value very low\n",
        "        # (So they won't be chosen as max)\n",
        "        invalid_actions = set(all_actions) - set(valid_actions)\n",
        "        for act in invalid_actions:\n",
        "            # if act in action_to_index:\n",
        "            q_values[action_to_index[act]] = -1e9  # large negative to disable\n",
        "        # Boltzmann (softmax) sampling over valid Q-values\n",
        "        # exp_q = np.exp(q_values / temperature)\n",
        "        q_values -= np.max(q_values)\n",
        "        exp_q = np.exp(q_values / temperature)\n",
        "        # Zero out probabilities for invalid actions\n",
        "        for act in invalid_actions:\n",
        "            exp_q[action_to_index[act]] = 0.0\n",
        "        probs = exp_q / (np.sum(exp_q) + 1e-9)\n",
        "        action_idx = np.random.choice(len(probs), p=probs)\n",
        "        action = all_actions[action_idx]\n",
        "\n",
        "    # Return both the index and the tuple representation\n",
        "    return action_to_index[action], action\n",
        "\n",
        "# Training loop\n",
        "# Use the new state representation: (stock1, stock2, stock3, stock4, stock5, cash)\n",
        "# For instance, an equal weight in stocks (15% each) and 25% cash:\n",
        "initial_state = (15, 15, 15, 15, 15, 25)\n",
        "replay_buffer = ReplayBuffer(replay_capacity)\n",
        "epsilon = epsilon_start\n",
        "train_days = train_prices.shape[0]\n",
        "val_interval = 10  # evaluate on validation set every 10 episodes\n",
        "best_val_reward = -np.inf\n",
        "patience = 0\n",
        "max_patience = 5  # stop training if no improvement in 5 consecutive evaluations\n",
        "n_step_return = 3\n",
        "\n",
        "for ep in range(1, episodes+1):\n",
        "    state = initial_state\n",
        "    # For each episode, reset running portfolio value (start with $1.0)\n",
        "    portfolio_value = 1.0\n",
        "    # Iterate over each day in training data (except last, as we look ahead one day for reward)\n",
        "    # For simplicity, use training price data; note we use market indicators from train_prices\n",
        "    for t in range(0, train_days - 1 - n_step_return):\n",
        "        # # Hybrid action selection using current day t and train_prices for indicators\n",
        "        action_idx, action = select_action(state, t, train_prices, epsilon)\n",
        "        # Apply action to get new state\n",
        "        new_state = apply_action(state, action)\n",
        "        # Compute reward from day t to t+1\n",
        "        weights_new = [x/100.0 for x in new_state]  # convert increments to fractions\n",
        "        raw_reward = compute_reward(weights_new, train_prices,t, n_step_return)\n",
        "        # Update portfolio value (simulate n-step growth)\n",
        "        portfolio_value *= math.exp(raw_reward)\n",
        "        # Shape reward using Sharpe reward shaper (with drawdown & volatility penalty)\n",
        "        reward = reward_shaper.shape(raw_reward, portfolio_value)\n",
        "        # Check if we've reached the end of an episode (done flag)\n",
        "        done = (t >= train_days - n_step_return - 1)  # True if next_state will be the last state of episode\n",
        "        # Append market indicators to the state representation\n",
        "        state_repr = get_state(t, state, train_prices)\n",
        "        next_state_repr = get_state(t + n_step_return, new_state, train_prices)\n",
        "        # Store the transition in replay memory\n",
        "        replay_buffer.push(state_repr, action_idx, reward, next_state_repr, done)\n",
        "        # Update state\n",
        "        state = new_state\n",
        "        # # Perform a learning step if we have enough samples\n",
        "        if len(replay_buffer) >= batch_size:\n",
        "            # Sample a batch of transitions\n",
        "            state_reprs_batch, actions_batch, rewards_batch, next_state_reprs_batch, dones_batch = replay_buffer.sample(batch_size)\n",
        "\n",
        "            # Convert to tensors\n",
        "            # State and next state inputs as batch_size x 5 tensors (normalize allocations to [0,1])\n",
        "            state_vec_tensor = torch.FloatTensor(state_reprs_batch)\n",
        "            next_state_vec_tensor = torch.FloatTensor(next_state_reprs_batch)\n",
        "            action_tensor = torch.LongTensor(actions_batch)\n",
        "            reward_tensor = torch.FloatTensor(rewards_batch)\n",
        "            done_tensor   = torch.BoolTensor(dones_batch)\n",
        "\n",
        "            # Compute current Q values for each state-action in the batch\n",
        "            # policy_net(state_tensor) has shape [batch, action_dim]; gather along actions\n",
        "            q_values = ensemble_q_values(state_vec_tensor)  # [batch, action_count]\n",
        "            state_action_values = q_values.gather(1, action_tensor.unsqueeze(1)).squeeze(1)\n",
        "            # Compute target Q values using target network\n",
        "            with torch.no_grad():\n",
        "                # next_q_values = target_net(next_state_tensor)  # [batch, action_count]\n",
        "                if use_double_dqn:\n",
        "                    # Step 1: For each next_state, select the best action using the online network\n",
        "                    # Online ensemble selects best action:\n",
        "                    online_next_q = ensemble_q_values(next_state_vec_tensor)  # avg Q from ensemble networks\n",
        "                    best_actions = online_next_q.argmax(dim=1, keepdim=True)  # best action indices from online net\n",
        "                    # Step 2: Evaluate these actions using the target network\n",
        "                    # For evaluation, take average target Q from target ensemble\n",
        "                    q_vals_targets_list = [target(next_state_vec_tensor) for target in ensemble_targets]\n",
        "                    target_next_q = torch.stack(q_vals_targets_list).mean(dim=0)\n",
        "                    selected_q = target_next_q.gather(1, best_actions).squeeze(1)\n",
        "                else:\n",
        "                    # Standard DQN target: use the max Q-value from the target network directly\n",
        "                    q_vals_targets_list = [target(next_state_vec_tensor) for target in ensemble_targets]\n",
        "                    target_next_q = torch.stack(q_vals_targets_list).mean(dim=0)\n",
        "                    selected_q = target_next_q.max(dim=1)[0]\n",
        "\n",
        "            selected_q = selected_q * (1 - done_tensor.float())\n",
        "            target_values = reward_tensor + gamma * selected_q\n",
        "\n",
        "            # target_tensor = torch.FloatTensor(target_values)\n",
        "            # losses = F.smooth_l1_loss(state_action_values, target_values, reduction='none')\n",
        "            # loss = (losses * torch.FloatTensor(weights)).mean() if 'weights' in locals() else losses.mean()\n",
        "            # loss = losses.mean()\n",
        "            # Optimize the model: MSE loss between state_action_values and target_values\n",
        "            loss = F.mse_loss(state_action_values, target_values)\n",
        "\n",
        "            ensemble_optimizer.zero_grad()\n",
        "            loss.backward()\n",
        "            ensemble_optimizer.step()\n",
        "\n",
        "    # Decay epsilon after each episode\n",
        "    epsilon = max(epsilon * epsilon_decay, epsilon_min)\n",
        "    # Update target network periodically\n",
        "    if ep % target_update_freq == 0:\n",
        "        for net, target in zip(ensemble_nets, ensemble_targets):\n",
        "            target.load_state_dict(net.state_dict())\n",
        "\n",
        "    # Every val_interval episodes, evaluate on the validation set for early stopping\n",
        "    if ep % val_interval == 0:\n",
        "        val_portfolio = 1.0\n",
        "        val_state = initial_state\n",
        "        # Evaluate over validation period\n",
        "        for t in range(0, val_prices.shape[0] - 1):\n",
        "            state_input = get_state(t, val_state, val_prices)\n",
        "            state_tensor = torch.FloatTensor(state_input).unsqueeze(0)\n",
        "            with torch.no_grad():\n",
        "                q_vals = ensemble_q_values(state_tensor).numpy().squeeze()\n",
        "            valid_acts = get_valid_actions(val_state)\n",
        "            invalid_acts = set(all_actions) - set(valid_acts)\n",
        "            for act in invalid_acts:\n",
        "                q_vals[action_to_index[act]] = -1e9\n",
        "            best_act_idx = int(np.argmax(q_vals))\n",
        "            best_action = all_actions[best_act_idx]\n",
        "            val_state = apply_action(val_state, best_action)\n",
        "            weights = [x/100.0 for x in val_state]\n",
        "            growth_factor = sum([weights[k] * (val_prices[t+1][k] / val_prices[t][k]) if k < num_assets else weights[cash_idx] for k in range(len(assets_plus_cash))])\n",
        "            val_portfolio *= growth_factor\n",
        "        # Use portfolio value change as validation reward metric\n",
        "        val_reward = math.log(val_portfolio)\n",
        "        print(f\"Episode {ep}: Validation reward = {val_reward:.4f}, epsilon={epsilon:.3f}\")\n",
        "        if val_reward > best_val_reward:\n",
        "            best_val_reward = val_reward\n",
        "            patience = 0  # reset patience if improvement observed\n",
        "        else:\n",
        "            patience += 1\n",
        "            if patience >= max_patience:\n",
        "                print(\"Early stopping triggered at episode\", ep)\n",
        "                break\n",
        "\n",
        "    if ep % 10 == 0 or ep == episodes:\n",
        "        print(f\"Episode {ep}/{episodes} completed, epsilon={epsilon:.3f}\")\n",
        "print(\"Training completed.\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vzosJhCGr4NQ"
      },
      "source": [
        "### Policy Evaluation on Test Data"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 95,
      "metadata": {
        "id": "j8hnwCBjr15n"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Test period: Agent final portfolio value = 0.7512 (Return = -24.88%)\n",
            "Test period: Baseline final value = 0.6263 (Return = -37.37%)\n"
          ]
        }
      ],
      "source": [
        "def evaluate_policy(price_array, model, initial_state):\n",
        "    \"\"\"\n",
        "    Simulate the portfolio value over time on given price data using the provided model (greedy policy).\n",
        "    Returns a list of portfolio values for each day in the price data.\n",
        "    \"\"\"\n",
        "    days = price_array.shape[0]\n",
        "    state = initial_state\n",
        "    portfolio_value = 1.0  # start with $1.0\n",
        "    values = [portfolio_value]\n",
        "    \n",
        "    for t in range(days - 1):\n",
        "        # Agent action: choose greedy (highest Q) action for current state\n",
        "        state_input = get_state(t, state, price_array)\n",
        "        state_tensor = torch.FloatTensor(state_input).unsqueeze(0)\n",
        "        with torch.no_grad():\n",
        "            ensemble_qs = model(state_tensor)\n",
        "            q_vals = ensemble_qs.numpy().squeeze()\n",
        "        # Mask invalid actions for current state\n",
        "        valid_acts = get_valid_actions(state)\n",
        "        invalid_acts = set(all_actions) - set(valid_acts)\n",
        "        for act in invalid_acts:\n",
        "            # if act in action_to_index:\n",
        "            q_vals[action_to_index[act]] = -1e9\n",
        "        best_act_idx = int(np.argmax(q_vals))\n",
        "        best_action = all_actions[best_act_idx]\n",
        "        # Rebalance portfolio according to best action\n",
        "        state = apply_action(state, best_action)\n",
        "        # Compute portfolio growth factor from day t to t+1 for agent\n",
        "        weights = [x / 100.0 for x in state]\n",
        "        # growth_factor = 0.0\n",
        "        # for k in range(num_assets):\n",
        "        #     growth_factor += weights[k] * (price_array[t+1][k] / price_array[t][k])\n",
        "        growth_factor = sum([weights[k] * (price_array[t+1][k] / price_array[t][k]) if k < num_assets else weights[cash_idx] for k in range(len(assets_plus_cash))])\n",
        "        portfolio_value *= growth_factor\n",
        "        values.append(portfolio_value)\n",
        "        # Update baseline value (its shares just appreciate with market, no rebalance)\n",
        "        # Baseline (buy-and-hold equal weights) for comparison\n",
        "        baseline_value = 1.0\n",
        "        # Compute initial shares for baseline (with equal weights)\n",
        "        baseline_weights = [0.2] * num_assets  # 20% each\n",
        "        baseline_shares = [baseline_weights[i] * baseline_value / price_array[0][i] for i in range(num_assets)]\n",
        "        baseline_portfolio_val = 0.0\n",
        "        for k in range(num_assets):\n",
        "            baseline_portfolio_val += baseline_shares[k] * price_array[t+1][k]\n",
        "        baseline_value = baseline_portfolio_val\n",
        "    final_return = (portfolio_value - 1.0) * 100.0 # in %\n",
        "    baseline_return = (baseline_value - 1.0) * 100.0\n",
        "    # After iterating, 'values' list contains portfolio value from start to end of period\n",
        "    print(f\"Test period: Agent final portfolio value = {portfolio_value:.4f} (Return = {final_return:.2f}%)\")\n",
        "    print(f\"Test period: Baseline final value = {baseline_value:.4f} (Return = {baseline_return:.2f}%)\")\n",
        "    return values, baseline_weights\n",
        "\n",
        "# Evaluate the trained model on test data\n",
        "agent_values, baseline_weights = evaluate_policy(test_prices, ensemble_q_values, initial_state)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "a3zdHpEGGpHd"
      },
      "outputs": [],
      "source": []
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "T4",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.9.6"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
